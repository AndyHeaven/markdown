## 计算机基础

1. ### OSI七层网络

   > 1. 物理层（Physical Layer）：该层处理物理传输介质和数据的传输方式，如电缆、光纤、无线信号等。它负责传输比特流，并定义了电压、电流和物理连接等物理特性。
   > 2. 数据链路层（Data Link Layer）：该层负责在直接相连的节点之间可靠地传输数据帧。它通过物理地址（MAC地址）标识网络设备，并处理错误检测和纠正，以确保数据的完整性和可靠性。
   > 3. 网络层（Network Layer）：该层处理数据包在网络中的路由和转发。它使用逻辑地址（IP地址）来标识不同的网络设备，并决定数据包的路由路径，以实现跨网络的通信。
   > 4. 传输层（Transport Layer）：该层提供端到端的可靠数据传输。它负责分割和重组数据，以及提供错误检测和恢复机制。常见的传输层协议包括TCP（传输控制协议）和UDP（用户数据报协议）。
   > 5. 会话层（Session Layer）：该层建立、管理和终止网络中的会话连接。它负责在通信节点之间建立通信会话，并处理会话的控制和同步。
   > 6. 表示层（Presentation Layer）：该层处理数据的表示和转换，以确保不同设备上的应用程序能够正确解释和使用数据。它处理数据的加密、压缩、格式转换等操作。
   > 7. 应用层（Application Layer）：该层为用户提供网络服务和应用程序接口。它包含各种应用程序，如电子邮件、文件传输协议（FTP）、域名系统（DNS）等，以满足用户的特定需求。

   ![OSI 七层模型](https://oss.javaguide.cn/github/javaguide/cs-basics/network/osi-7-model.png)

2. ### OSI7层网络，每一层具体做什么的

   > 如上所述

3. ### 应用层包括的协议

   > **HTTP（Hypertext Transfer Protocol，超文本传输协议）**：基于 TCP 协议，是一种用于传输超文本和多媒体内容的协议，主要是为 Web 浏览器与 Web 服务器之间的通信而设计的。当我们使用浏览器浏览网页的时候，我们网页就是通过 HTTP 请求进行加载的。
   >
   > **SMTP（Simple Mail Transfer Protocol，简单邮件发送协议）**：基于 TCP 协议，是一种用于发送电子邮件的协议。注意 ⚠️：SMTP 协议只负责邮件的发送，而不是接收。要从邮件服务器接收邮件，需要使用 POP3 或 IMAP 协议。
   >
   > **POP3/IMAP（邮件接收协议）**：基于 TCP 协议，两者都是负责邮件接收的协议。IMAP 协议是比 POP3 更新的协议，它在功能和性能上都更加强大。IMAP 支持邮件搜索、标记、分类、归档等高级功能，而且可以在多个设备之间同步邮件状态。几乎所有现代电子邮件客户端和服务器都支持 IMAP。
   >
   > **FTP（File Transfer Protocol，文件传输协议）** : 基于 TCP 协议，是一种用于在计算机之间传输文件的协议，可以屏蔽操作系统和文件存储方式。注意 ⚠️：FTP 是一种不安全的协议，因为它在传输过程中不会对数据进行加密。建议在传输敏感数据时使用更安全的协议，如 SFTP。
   >
   > **Telnet（远程登陆协议）**：基于 TCP 协议，用于通过一个终端登陆到其他服务器。Telnet 协议的最大缺点之一是所有数据（包括用户名和密码）均以明文形式发送，这有潜在的安全风险。这就是为什么如今很少使用 Telnet，而是使用一种称为 SSH 的非常安全的网络传输协议的主要原因。
   >
   > **SSH（Secure Shell Protocol，安全的网络传输协议）**：基于 TCP 协议，通过加密和认证机制实现安全的访问和文件传输等业务
   >
   > **RTP（Real-time Transport Protocol，实时传输协议）**：通常基于 UDP 协议，但也支持 TCP 协议。它提供了端到端的实时传输数据的功能，但不包含资源预留存、不保证实时传输质量，这些功能由 WebRTC 实现。
   >
   > **DNS（Domain Name System，域名管理系统）**: 基于 UDP 协议，用于解决域名和 IP 地址的映射问题。

   ![应用层常见协议](https://oss.javaguide.cn/github/javaguide/cs-basics/network/application-layer-protocol.png)

4. ### HTTP和HTTPS的区别

   > |              | HTTP               | HTTPS                                   |
   > | ------------ | ------------------ | --------------------------------------- |
   > | 端口         | 80                 | 443                                     |
   > | 安全性       | 无加密，安全性较差 | 有加密机制，安全性较高                  |
   > | 资源消耗     | 较少               | 由于加密处理，资源消耗更多              |
   > | 是否需要证书 | 不需要             | 需要                                    |
   > | 协议         | 运行在TCP协议之上  | 运行在SSL协议之上，SSL运行在TCP协议之上 |

5. ### HTTPS加密

   > 在建立连接时，HTTPS使用非对称加密（公钥加密）来进行身份验证和会话密钥的安全交换。具体步骤如下：
   >
   > 1. 服务器端生成公钥和私钥对，并将公钥放在SSL证书中。服务器的私钥保密保存。
   > 2. 客户端向服务器发送连接请求，并要求建立安全连接。
   > 3. 服务器将自己的SSL证书（包含公钥）发送给客户端。
   > 4. 客户端收到证书后，验证证书的有效性、签名和颁发机构。如果证书有效，客户端会生成一个随机的会话密钥，并使用服务器的公钥（非对称加密）对该会话密钥进行加密。
   > 5. 客户端将加密后的会话密钥发送给服务器。
   > 6. 服务器使用自己的私钥对收到的会话密钥进行解密，从而获取会话密钥。
   >
   > 一旦会话密钥被客户端和服务器双方获取，后续的通信将使用对称加密（会话密钥加密）进行加密和解密。对称加密算法更高效，因此使用会话密钥来加密实际的数据传输。
   >
   > 综上所述，HTTPS使用非对称加密来安全地交换对称加密所需的会话密钥，从而结合了两者的优势，既保证了安全性又提供了高效的数据传输。

   总结：SSL/TSL是采用非对称加密和对称加密组合的，HTTPS使用非对称加密（公钥加密）来进行身份验证和会话密钥的安全交换，在交换成功后，后续通信使用对称加密进行。

6. ### 如果HTTP的数据会被拦截，对称加密的密钥为什么不会被拦截

   > 当使用HTTPS进行通信时，HTTP的数据会被加密，并且对称加密的密钥在建立连接时使用非对称加密进行安全交换。这确保了对称加密密钥在传输过程中不会被拦截。

   总结：对称加密的密钥是非对称加密算法（服务器公钥）加密的，而解密的私钥存在于服务器

7. ### HTTP请求的过程，重定向和DNS解析

   > 1. DNS解析：客户端首先需要将URL中的域名解析为对应的IP地址。它会向本地DNS解析器发送域名查询请求，解析器会查找缓存中是否有对应的IP地址。如果没有缓存或者过期，解析器会向DNS服务器发送查询请求以获取IP地址。
   > 2. TCP连接建立：客户端使用解析得到的IP地址，通过TCP/IP协议与服务器建立连接。这个过程涉及到客户端和服务器之间的三次握手，确保双方都准备好进行通信。
   > 3. 发送HTTP请求：一旦TCP连接建立，客户端会发送HTTP请求到服务器。请求包括请求方法（GET、POST等）、URL路径、请求头（包含一些元数据信息如User-Agent、Cookie等）以及请求体（对于POST请求）。
   > 4. 服务器处理请求：服务器接收到客户端的请求后，会根据请求的URL路径和方法执行相应的处理。这可能涉及到读取数据库、执行业务逻辑等操作。
   > 5. 服务器响应：服务器根据处理结果生成HTTP响应，并将其发送回客户端。响应包括状态码（表示请求的处理结果，如200表示成功，404表示未找到等）、响应头（包含一些元数据信息如Content-Type、Cache-Control等）以及响应体（包含实际的响应内容）。
   > 6. 数据传输和关闭连接：一旦响应开始发送，服务器和客户端之间开始进行数据传输。服务器按照响应的内容长度和分块传输等方式发送数据，客户端接收响应并处理。完成后，连接可以被关闭，或者保持长连接以便进行后续请求。
   >
   > 在这个过程中，可能会出现重定向。重定向是服务器返回一个特殊的响应状态码（如301或302），指示客户端重新发送请求到另一个URL。客户端会根据重定向响应中的新URL重新发送请求，并继续后续的处理流程。
   >
   > 总结起来，HTTP请求的过程包括DNS解析、TCP连接建立、发送请求、服务器处理、服务器响应和数据传输。重定向是一种特殊的响应状态码，用于指示客户端重新发送请求到另一个URL。

   **递归查询**是指客户端向本地DNS服务器发送查询请求，本地DNS服务器负责完成整个查询过程，包括向其他DNS服务器发送查询请求，并等待最终的解析结果。而**迭代查询**是指本地DNS服务器向其他DNS服务器发送查询请求，并根据每个DNS服务器的回答继续查询，直到获取最终的解析结果。

   总结：客户端向本地DNS服务器请求是递归的；本地DNS服务器向其他DNS服务器是迭代的

   本地DNS查询是从上到下：本地DNS服务器-根DNS服务器-顶级DNS服务器-权威DNS服务器

   ![img](https://pic1.zhimg.com/80/v2-d4ebb1c03acc11c40aa2d0624e98f9f8_720w.webp)

   如上图所示，域名结构是树状结构，树的最顶端代表根服务器，根的下一层就是由我们所熟知的.com、.net、.cn等通用域和.cn、.uk等国家域组成，称为顶级域。网上注册的域名基本都是二级域名，比如[http://baidu.com](https://link.zhihu.com/?target=http%3A//baidu.com)、[http://taobao.com](https://link.zhihu.com/?target=http%3A//taobao.com)等等二级域名（权威域），它们基本上是归企业和运维人员管理。接下来是三级或者四级域名。

   访问网站www.google.com实质上浏览器会自动加上`.`，访问的是www.google.com.

   网址的解析是一个从右向左的过程：. （根域）-> .com （顶级域）-> google.com（权威域） -> www.google.com（目标域名）

8. ### 操作系统级别怎么做到阻塞

   > 1. **系统调用**：操作系统提供了系统调用，允许应用程序请求操作系统执行某些特定操作。在阻塞场景中，应用程序可以通过调用适当的系统调用来触发阻塞。例如，`sleep`或`wait`系统调用会让进程或线程暂停一段时间或等待特定事件发生。
   > 2. **进程/线程调度**：操作系统通过进程/线程调度来决定哪个进程或线程在某一时刻运行。如果一个进程或线程被置于阻塞状态，操作系统可能会将CPU资源分配给其他处于就绪状态的进程/线程。
   > 3. **阻塞状态**：操作系统维护进程或线程的状态信息，其中包括运行、就绪和阻塞等状态。当一个进程或线程需要等待某个事件发生时，操作系统将其状态设置为阻塞，并将其从CPU调度队列中移除。
   > 4. **等待队列**：操作系统通常会维护等待队列，用于存放被阻塞的进程或线程。当某个条件得到满足，例如I/O操作完成，操作系统会将等待队列中的进程/线程唤醒，并将其状态设置为就绪，以便在合适的时候再次被调度执行。
   > 5. **信号量和锁**：在多线程环境中，操作系统可以使用信号量、互斥锁等同步原语来实现阻塞。当线程尝试获取一个被锁定的资源时，它可能会被阻塞，直到资源可用或锁被释放。
   > 6. **事件对象**：操作系统可能提供事件对象，允许进程或线程等待某个事件的发生。当事件发生时，操作系统会通知阻塞的进程/线程，并将其从阻塞状态转换为就绪状态。
   > 7. **中断处理**：某些情况下，硬件中断也可以引起阻塞。例如，一个设备需要完成数据传输，而CPU可能会被暂时挂起，等待设备完成操作。

9. ### wait方法操作系统里是如何实现的？unknown

   > 信号量、条件变量、事件对象

10. ### TCP协议和UDP协议

    > |                        |                      TCP                       |               UDP                |
    > | :--------------------: | :--------------------------------------------: | :------------------------------: |
    > |      是否面向连接      |                       是                       |                否                |
    > |        是否可靠        |                       是                       |                否                |
    > |       是否有状态       |                       是                       |                否                |
    > |        传输效率        |                      较慢                      |               较快               |
    > |        传输形式        |                     字节流                     |            数据报文段            |
    > |        首部开销        |                 20 ～ 60 bytes                 |             8 bytes              |
    > | 是否提供广播或多播服务 |                       否                       |                是                |
    > |        应用场景        |             对传输准确性要求特别高             | 对传输数据的准确性要求不是特别高 |
    > |      基于此的协议      | HTTP、HTTPS、FTP、SSH、POP3/IMAP、SMTP、Telnet |            DHCP、DNS             |

    

11. ### 为什么UDP传输快

    > 1. **无连接性**：UDP是无连接的协议，这意味着在数据传输之前不需要建立和维护连接状态。相比之下，TCP需要在通信开始前进行握手和连接管理。因此，在某些场景下，UDP可以跳过这些连接管理步骤，从而提高传输速度。
    > 2. **少量的协议开销**：与TCP相比，UDP的头部较小，占用的网络带宽较少。TCP的头部包含更多的控制信息，例如序列号、确认号等，这些信息在一些情况下可以导致较大的协议开销。在需要高效传输而不需要复杂的连接管理和可靠性保证的情况下，UDP的较小开销可以提高传输速度。
    > 3. **不进行拥塞控制**：TCP使用拥塞控制算法来防止网络拥塞，这可能会导致发送速率的限制，特别是在网络条件不佳时。而UDP不执行拥塞控制，因此它允许发送者以更高的速率发送数据。这在实时应用，如音视频传输中，可能会更有优势。

    ---

    尽管UDP在某些情况下可以提供更快的传输速度，但它也有一些限制：

    > 1. **不可靠性**：UDP不提供可靠性保证，数据包可能会在传输过程中丢失、重复或顺序错乱。这使得在需要确保数据完整性和可靠性的场景下，UDP并不适合使用。
    > 2. **无流量控制**：UDP没有内置的流量控制机制，这意味着如果发送数据速率过快，接收方可能会不堪重负，从而导致丢失数据。
    > 3. **应用层负责处理**：由于UDP不处理传输层上的复杂逻辑，应用程序必须自行处理丢包、重复等问题。这需要更多的开发工作。

    总之，UDP传输快主要是因为它的简单性和少量的协议开销，适用于一些不需要可靠性保证和连接管理的场景。但在许多情况下，需要权衡传输速度和数据完整性，选择合适的传输协议。

12. ### TCP怎么确保可靠，除了序列号还有什么方法

    TCP主要提供了检验和、序列号/确认应答、超时重传、滑动窗口、拥塞控制和 流量控制等方法实现了可靠性传输。

    > 1. 检验和：通过检验和的方式，接收端可以检测出来数据是否有差错和异常，假如有差错就会直接丢弃TCP段，重新发送。
    >
    > 2. 序列号/确认应答：
    >
    >    序列号的作用不仅仅是应答的作用，有了序列号能够将接收到的数据根据序列号排序，并且去掉重复序列号的数据。
    >
    >    TCP传输的过程中，每次接收方收到数据后，都会对传输方进行确认应答。也就是发送ACK报文，这个ACK报文当中带有对应的确认序列号，告诉发送方，接收到了哪些数据，下一次的数据从哪里发。
    >
    > 3. 滑动窗口：滑动窗口既提高了报文传输的效率，也避免了发送方发送过多的数据而导致接收方无法正常处理的异常。
    >
    > 4. 超时重传：超时重传是指发送出去的数据包到接收到确认包之间的时间，如果超过了这个时间会被认为是丢包了，需要重传。最大超时时间是动态计算的。
    >
    > 5. 拥塞控制：在数据传输过程中，可能由于网络状态的问题，造成网络拥堵，此时引入拥塞控制机制，在保证TCP可靠性的同时，提高性能。
    >
    > 6. 流量控制：如果主机A 一直向主机B发送数据，不考虑主机B的接受能力，则可能导致主机B的接受缓冲区满了而无法再接受数据，从而会导致大量的数据丢包，引发重传机制。而在重传的过程中，若主机B的接收缓冲区情况仍未好转，则会将大量的时间浪费在重传数据上，降低传送数据的效率。所以引入流量控制机制，主机B通过告诉主机A自己接收缓冲区的大小，来使主机A控制发送的数据量。流量控制与TCP协议报头中的窗口大小有关。

13. ### TCP客户端挂了服务端怎么办

    （保活机制？）

    > 简而言之，通过**定时器 + 超时重试机制**，尝试获取确认，直到最后会自动断开连接。
    >
    > 具体而言，TCP 设有一个保活计时器。服务器每收到一次客户端的数据，都会重新复位这个计时器，以Linux服务器为例，时间通常是设置为 2 小时。若 2 小时还没有收到客户端的任何数据，服务器就开始重试：每隔 75 秒（默认）发送一个探测报文段，若一共发送 10 个探测报文后客户端依然没有回应，那么服务器就认为连接已经断开了。

14. ### web的会话保持方式有哪些，cookie里面放点啥

    > Web的会话保持方式有以下几种常见的方式：
    >
    > 1. Cookie：Cookie是服务器在HTTP响应中设置的一小段文本信息，存储在客户端浏览器中。浏览器在后续的请求中会自动将Cookie信息包含在HTTP头中发送给服务器，从而实现会话保持。
    > 2. URL重写：服务器可以通过在URL中包含会话标识符（Session ID）来进行会话保持。每次客户端发送请求时，服务器根据URL中的会话标识符来识别和关联对应的会话数据。
    > 3. 隐藏表单字段：在HTML表单中添加隐藏字段，用于存储会话标识符。客户端提交表单时，会话标识符也会一同提交，服务器根据该标识符来关联会话。
    > 4. HTTP认证：使用HTTP认证机制，例如基本认证（Basic Authentication）或摘要认证（Digest Authentication），在每个请求中包含用户凭证，以验证和保持会话。
    >
    > ---
    >
    > 关于Cookie中存放的内容，可以根据实际需求存放不同的数据，例如：
    >
    > - 会话标识符（Session ID）：用于标识会话，关联服务器端存储的会话数据。
    > - 用户身份信息：例如用户ID、用户名等，用于标识用户身份。
    > - 个性化设置：例如用户的主题偏好、语言偏好等。
    > - 购物车信息：在电子商务网站中，可以将用户的购物车内容存储在Cookie中。
    > - 追踪信息：用于分析用户行为，例如广告点击、访问次数等。

15. ### linux一些命令

    > 当涉及到Linux操作系统时，下面是一些常用的命令：
    >
    > 1. ls：列出目录中的文件和子目录。
    > 2. cd：改变当前工作目录。
    > 3. pwd：显示当前工作目录的路径。
    > 4. mkdir：创建新目录。
    > 5. rm：删除文件或目录。
    > 6. cp：复制文件和目录。
    > 7. mv：移动文件和目录，或者重命名文件和目录。
    > 8. touch：创建空文件或更新文件的访问时间。
    > 9. cat：显示文件内容。
    > 10. more：逐页显示文件内容。
    > 11. less：以交互方式逐页显示文件内容。
    > 12. head：显示文件的开头几行。
    > 13. tail：显示文件的末尾几行。
    > 14. grep：在文件中搜索指定模式。
    > 15. find：根据特定条件查找文件。
    > 16. chmod：修改文件或目录的权限。
    > 17. chown：修改文件或目录的所有者。
    > 18. chgrp：修改文件或目录的所属组。
    > 19. tar：打包和解压文件。
    > 20. gzip：压缩文件。
    > 21. unzip：解压缩文件。
    > 22. ssh：通过安全的Shell连接远程服务器。
    > 23. scp：在本地系统和远程系统之间复制文件。
    > 24. top：显示系统中运行的进程和系统资源的使用情况。
    > 25. ps：显示运行中的进程。
    > 26. kill：终止正在运行的进程。
    > 27. ifconfig：显示和配置网络接口信息。
    > 28. ping：测试与另一台计算机的连接。
    > 29. wget：从网络上下载文件。
    > 30. man：查看命令的帮助文档。
    >
    > 这只是一小部分常用的Linux命令，Linux拥有众多强大的命令和工具，能够满足各种系统管理和开发需求。如果您想了解更多命令的详细使用方法，可以使用命令的man页（例如，man ls）或在终端中使用命令的--help选项（例如，ls --help）来获取相关帮助信息。

## Java基础

1. ### c++和Java的区别

   > 1. **编程范式**：C++是一种多范式编程语言，支持面向对象编程（OOP）、过程式编程和泛型编程。Java也是一种多范式语言，但主要以面向对象编程为主，也支持部分泛型编程。c++支持多继承，有指针，java不支持多继承，无指针
   > 2. **内存管理**：c++手动管理内存，java垃圾回收机制自动管理内存
   > 3. 平台独立性：Java是一种平台无关的语言，通过Java虚拟机（Java Virtual Machine，JVM）在不同的操作系统上运行。Java代码在编译为字节码后，可以在任何支持相应JVM的操作系统上运行。而C++是一种编译型语言，需要将源代码直接编译为特定平台的机器码。
   > 4. **异常处理**：C++和Java都支持异常处理机制，但在语法和使用方式上有所不同。C++使用`try-catch`块来捕获和处理异常，而Java要求开发人员在方法签名中声明可能抛出的异常，并要求在代码中显式处理或传递异常。
   > 5. **应用领域**：C++通常用于系统级编程、游戏开发、嵌入式系统和对性能要求较高的应用。Java更多地用于企业级应用开发、Web应用程序、移动应用开发和大规模分布式系统。

2. ### 如何理解Java面向对象设计，面向对象有什么好处，面向对象比面向过程的优势，如何理解多态

   > 1. 面向对象设计（Object-Oriented Design）：面向对象设计是一种以对象为中心的设计思想，它将现实世界中的事物抽象为对象，通过对象之间的交互和消息传递来实现程序的逻辑。面向对象设计的关键概念包括封装（Encapsulation）、继承（Inheritance）和多态（Polymorphism）。
   > 2. 面向对象的好处：
   >    - **模块化和可重用性**：面向对象的设计使得代码可以按照对象的方式进行组织，通过封装和抽象可以将复杂系统分解为独立的模块，提高代码的可重用性和可维护性。
   >    - **易于扩展和修改**：由于封装的特性，对于一个对象的修改不会对其他对象产生影响，从而使得系统更易于扩展和修改，减少了代码的耦合度。
   >    - **可靠性和灵活性**：面向对象的设计强调对象之间的消息传递和交互，通过定义良好的接口和封装数据，可以提高代码的可靠性和灵活性。
   > 3. 面向对象与面向过程的优势：
   >    - 面向对象的编程范式更符合现实世界的建模方式，使代码更易于理解和维护。
   >    - 面向对象的设计具有更好的模块化和可重用性，减少了代码的重复和冗余。
   >    - 面向对象的设计使得系统更易于扩展和修改，降低了代码的耦合度。
   > 4. 多态（Polymorphism）的理解：多态是面向对象编程的一个重要特性，它允许不同类型的对象对同一个消息做出不同的响应。在Java中，多态可以通过继承和接口实现。多态的好处在于提高了代码的灵活性和可扩展性，使得代码可以更容易适应变化和扩展。通过多态，可以编写出更通用和可复用的代码，实现了面向对象设计中的"开闭原则"（Open-Closed Principle）。

   总结起来：面向对象设计以对象为中心，通过封装、继承和多态等特性来组织和设计代码。面向对象的设计具有模块化、可重用性、易于扩展和修改等优势，与面向过程编程相比，更符合现实世界的建模方式，使代码更易于理解和维护。多态是面向对象编程的重要特性，通过多态可以实现代码的灵活性和可扩展性，使得代码更容易适应变化和扩展。

3. ### 面向对象编程，讲讲三个特点。追问多态，用父类声明，编译时是父类还是子类，运行时是父类还是子类

   > 面向对象编程（Object-Oriented Programming，OOP）是一种编程范式，它基于对象的概念，通过封装、继承和多态等特性来组织和设计代码。以下是面向对象编程的三个主要特点：
   >
   > 1. 封装（Encapsulation）：封装是面向对象编程的核心原则之一。它指的是将数据和操作封装在一个对象中，对象对外部提供访问和操作的接口，而隐藏了内部的实现细节。封装可以通过访问修饰符（如私有、公共、受保护）来控制对对象成员的访问权限。封装提供了数据的安全性和代码的模块化，使得对象的实现细节可以独立地进行修改，而不会影响使用该对象的其他部分。
   >
   > 2. 继承（Inheritance）：继承是面向对象编程中的一种机制，它允许一个类（子类）继承另一个类（父类）的属性和方法。子类可以继承父类的特性，并可以在此基础上进行扩展或修改。继承提供了代码的重用性和层次化的组织结构，通过继承可以建立类之间的关系，形成类的继承体系。
   >
   > 3. 多态（Polymorphism）：多态是面向对象编程中的一个重要特性，它允许不同类型的对象对同一个消息作出不同的响应。多态实现了“一个接口，多种实现”的概念，通过多态，可以编写出更通用和可复用的代码。多态可以通过继承和接口实现，通过方法的重写、接口的实现和方法的重载等机制来实现多态。多态提高了代码的灵活性和可扩展性，使得代码可以更容易适应变化和扩展。
   >
   > 这些特点共同构成了面向对象编程的基础，使得代码更易于理解、扩展和维护。封装保护了对象的内部状态，继承提供了代码的重用性和层次化的组织结构，多态提供了灵活性和可扩展性，使得代码能够更好地应对需求的变化。
   >
   > ---
   >
   > 使用父类声明时，编译时会将对象视为父类对象，而运行时则会根据实际创建的对象类型来确定是父类还是子类。
   >
   > 编译时（Compile-time）类型是指在编译阶段编译器根据声明的类型来进行类型检查和绑定。当使用父类声明时，编译器只关注声明的类型，即父类类型。它会检查代码是否符合父类的接口和规范，并进行相应的编译时检查。
   >
   > 运行时（Runtime）类型是指在程序运行时实际创建的对象的类型。当使用父类声明并创建了子类对象时，实际上创建了一个子类的实例，该实例在内存中的类型是子类类型。在运行时，程序会根据实际创建的对象类型来确定使用的方法和属性，即根据子类类型进行动态绑定。
   >
   > 例如，假设有一个父类`Animal`和一个子类`Cat`，代码如下：
   >
   > ```java
   > Animal animal = new Cat();
   > ```
   >
   > 在这种情况下，编译器会将对象`animal`视为`Animal`类型，因为声明时使用的是父类类型。编译器将检查代码是否符合`Animal`类的接口和规范。
   >
   > 然而，实际上在运行时创建了一个`Cat`对象。因此，在运行时，程序会根据对象的实际类型`Cat`来确定调用的方法和属性。如果`Cat`类中有重写了父类`Animal`的方法，那么运行时会调用`Cat`类中的方法。
   >
   > 总结起来，**使用父类声明时，编译时会将对象视为父类类型进行编译时检查，而在运行时会根据实际创建的对象类型来确定调用的方法和属性，即根据子类类型进行动态绑定。**

4. ### 列举一下常见的排序算法以及对应的时间复杂度，空间复杂度，比快排更快的排序算法，堆排序中构建堆的时间复杂度

   > 1. 冒泡排序（Bubble Sort）：
   >    - 时间复杂度：最好情况下O(n)，平均和最坏情况下O(n^2)
   >    - 空间复杂度：O(1)
   >    
   > 2. 选择排序（Selection Sort）：
   >    - 时间复杂度：最好、平均和最坏情况下都是O(n^2)
   >    - 空间复杂度：O(1)
   > 3. 插入排序（Insertion Sort）：
   >    - 时间复杂度：最好情况下O(n)，平均和最坏情况下O(n^2)
   >    - 空间复杂度：O(1)
   > 4. 归并排序（Merge Sort）：
   >    - 时间复杂度：最好、平均和最坏情况下都是O(n log n)
   >    - 空间复杂度：O(n)
   > 5. 快速排序（Quick Sort）：
   >    - 时间复杂度：最好和平均情况下O(n log n)，最坏情况下O(n^2)
   >    - 空间复杂度：最好和平均情况下O(log n)，最坏情况下O(n)
   > 6. 堆排序（Heap Sort）：
   >    - 时间复杂度：最好、平均和最坏情况下都是O(n log n)
   >    - 空间复杂度：O(1)
   > 7. 希尔排序（Shell Sort）：
   >    - 时间复杂度：取决于增量序列的选择，最好的已知序列最坏情况下为O(n^2)
   >    - 空间复杂度：O(1)
   > 8. 计数排序（Counting Sort）：
   >    - 时间复杂度：最好、平均和最坏情况下都是O(n + k)，其中n是元素个数，k是数据范围
   >    - 空间复杂度：O(n + k)
   > 9. 桶排序（Bucket Sort）：
   >    - 时间复杂度：最好、平均情况下O(n + k)，其中n是元素个数，k是桶的数量
   >    - 空间复杂度：O(n + k)
   > 10. 基数排序（Radix Sort）：
   >     - 时间复杂度：最好、平均和最坏情况下都是O(d * (n + k))，其中d是最大数字的位数，n是元素个数，k是基数
   >     - 空间复杂度：O(n + k)
   >
   > **比快速排序更快的排序算法包括：**
   >
   > - 在最坏情况下，快速排序的时间复杂度为O(n^2)，这是由于选择的基准元素不合适导致的。因此，存在一些排序算法在最坏情况下比快速排序更快。
   >   
   >   以下是一些在最坏情况下比快速排序更快的排序算法：
   >   
   >   1. 归并排序（Merge Sort）：归并排序的时间复杂度在最好、平均和最坏情况下都是O(n log n)。它使用了分治的思想，将数组逐步分解为较小的子数组并进行排序，然后再将排序好的子数组合并起来。归并排序具有稳定性和可预测的性能，在最坏情况下也能保持较好的时间复杂度。
   >   2. 堆排序（Heap Sort）：堆排序的时间复杂度在最好、平均和最坏情况下都是O(n log n)。堆排序使用了堆这种数据结构来进行排序，通过构建最大堆或最小堆来选择最大或最小元素，并逐步将元素放置在正确的位置上。堆排序的特点是不需要额外的空间，并且在最坏情况下仍能保持O(n log n)的时间复杂度。
   >   3. 基数排序（Radix Sort）：基数排序的时间复杂度在最好、平均和最坏情况下都是O(d * (n + k))，其中d是最大数字的位数，n是元素个数，k是基数。基数排序按照每个数字的位数进行排序，从最低位到最高位依次进行，通过稳定的排序算法（如计数排序或桶排序）对每一位进行排序。基数排序适用于对整数或字符串等具有固定位数的元素进行排序。
   >   
   >

5. ### 在常用的队列基本上是使用在哪一种？哪些场景？

   > Java中常用的队列包括以下几种：
   >
   > 1. LinkedList：链表队列
   >    - 应用场景：适用于需要在队列的两端进行添加和移除操作的场景，例如实现栈、回文判断等。
   >
   > 2. ArrayDeque：数组双端队列
   >    - 应用场景：适用于双端队列的场景，可以在队列的两端进行添加和移除操作。
   >
   > 3. PriorityQueue：优先队列
   >    - 应用场景：适用于需要按照元素的优先级进行排序的场景，例如任务调度、事件处理等。
   >
   > 4. ArrayBlockingQueue：数组阻塞队列
   >    - 应用场景：适用于多线程环境下的生产者-消费者模型，提供线程安全的队列操作。
   >
   > 5. LinkedBlockingQueue：链表阻塞队列
   >    - 应用场景：适用于多线程环境下的生产者-消费者模型，提供线程安全的队列操作。
   >
   > 6. ConcurrentLinkedQueue：并发链表队列
   >    - 应用场景：适用于高并发环境下的无界非阻塞队列，提供高效的并发读写操作。
   >
   > 7. SynchronousQueue：同步队列
   >    - 应用场景：适用于实现生产者-消费者模型，要求生产者和消费者同步进行的场景。
   >
   > 8. DelayQueue：延迟队列
   >    - 应用场景：适用于延迟任务的调度和执行，可用于定时任务处理。
   >
   > 9. LinkedTransferQueue：链表传输队列
   >    - 应用场景：适用于高并发环境下的无界阻塞队列，提供高效的并发读写操作。
   >
   > 10. PriorityBlockingQueue：优先阻塞队列
   >     - 应用场景：适用于需要按照元素的优先级进行排序的场景，支持并发读写操作。
   >
   > 这些队列具有不同的特点和适用场景。根据具体的需求，可以选择合适的队列实现来满足业务需求。

6. ### 数据结构中有哪些树，都是用来做什么的。

   > 数据结构中常见的树包括以下几种：
   >
   > 1. 二叉树（Binary Tree）：每个节点最多有两个子节点，左子节点和右子节点。二叉树常用于实现搜索树和排序树等数据结构。
   >
   > 2. 二叉搜索树（Binary Search Tree）：一种特殊的二叉树结构，左子树的节点值小于等于根节点的值，右子树的节点值大于等于根节点的值。二叉搜索树常用于实现快速的插入、删除和查找操作。
   >
   > 3. 平衡二叉树（Balanced Binary Tree）：一种高度平衡的二叉树，保持左子树和右子树的高度差不超过一个固定值。常见的平衡二叉树包括红黑树和AVL树，用于提高二叉树的搜索效率。
   >
   > 4. B树（B-Tree）：一种自平衡的树结构，用于存储大量的数据并支持高效的插入、删除和查找操作。B树常用于文件系统和数据库索引等场景。
   >
   > 5. B+树（B+ Tree）：在B树的基础上进行了优化，将数据都存储在叶子节点中，而非内部节点，提高了范围查询的效率。B+树常用于数据库索引和文件系统等。
   >
   > 6. Trie树（字典树、前缀树）：一种多叉树结构，用于高效地存储和查找字符串集合。Trie树常用于字符串匹配、自动完成和单词搜索等应用。
   >
   > 7. 堆（Heap）：一种特殊的树结构，根节点的值大于等于（或小于等于）子节点的值。堆常用于实现优先队列和堆排序等算法。
   >
   > 这些树结构在数据结构中具有不同的特点和应用。它们可以用于解决各种问题，包括数据的存储、搜索、排序、范围查询等。根据具体的需求和问题，选择合适的树结构可以提高算法和数据操作的效率。

7. ### 看过Java jdk的那些源码，jdk19 有那些更新

   > 1. 记录模式
   > 2. 虚拟线程
   > 3. Vector API
   > 4. Swicth匹配模式，使用when 子句增强保护
   > 5. 外部函数与内存API
   > 6. 结构化并发
   > 7. Linux/RISC-V移植

## Java集合

1. ### Java里常用的数据结构有哪些

   > Java中常用的数据结构包括以下几种：
   >
   > 1. 数组（Array）：是一种线性数据结构，用于存储一组相同类型的元素。数组具有固定长度，在内存中是连续存储的。
   >
   > 2. 链表（Linked List）：是一种由节点组成的数据结构，每个节点包含数据和指向下一个节点的引用。链表可以分为单链表、双向链表和循环链表等类型。
   >
   > 3. 栈（Stack）：是一种后进先出（LIFO）的数据结构，只能在一端进行插入和删除操作。常用的实现有基于数组和链表的栈。
   >
   > 4. 队列（Queue）：是一种先进先出（FIFO）的数据结构，允许在一端插入元素，在另一端删除元素。常用的实现有基于数组和链表的队列。
   >
   > 5. 哈希表（Hash Table）：是一种根据键（Key）快速查找值（Value）的数据结构。Java中的HashMap和Hashtable就是基于哈希表实现的。
   >
   > 6. 树（Tree）：是一种非线性数据结构，由节点和边组成。常见的树结构包括二叉树、二叉搜索树、平衡二叉树（如AVL树和红黑树）、堆等。
   >
   > 7. 图（Graph）：是一种由节点和边组成的数据结构，用于表示各种关系和连接。常见的图结构包括有向图和无向图，可以用邻接矩阵或邻接表等方式实现。
   >
   > 8. 集合（Set）和列表（List）：Java提供了各种集合和列表的接口和实现类，如HashSet、TreeSet、ArrayList、LinkedList等，用于存储和操作一组元素。
   >
   > 9. 栈（Stack）和队列（Queue）的变体：Java提供了一些栈和队列的变体，如Deque（双端队列）、PriorityQueue（优先级队列）等，具有更多的操作和特性。
   >
   > 除了以上常见的数据结构，Java还提供了其他数据结构和集合框架，如BitSet（位集合）、Vector（向量）、Enumeration（枚举）等，以满足不同的需求和场景。

2. ### Hashmap和Concurrenthashmap

   > HashMap和ConcurrentHashMap都是Java中常用的哈希表（Hash Table）实现，用于存储键值对数据。
   >
   > HashMap是非线程安全的哈希表实现。它基于哈希算法，将键映射到对应的桶（Bucket），每个桶中可以存储一个或多个键值对。HashMap允许空键和空值，并且支持快速的插入、删除和查找操作。通过调整初始容量和负载因子，可以控制HashMap的性能和空间占用。
   >
   > ConcurrentHashMap是线程安全的哈希表实现，它是通过使用锁的方式来保证线程安全性。与HashMap相比，ConcurrentHashMap在并发环境下提供更好的性能和可靠性。它将数据分割为多个段（Segment），每个段都有自己的锁，不同的线程可以同时访问不同的段，从而提高并发性能。ConcurrentHashMap的读操作不需要加锁，因此可以实现高效的并发读取。
   >
   > 在Java 8及以后的版本中，ConcurrentHashMap进行了一些改进。它引入了一种新的数据结构，称为分段锁（Segmented Locking），用于替代之前版本中的ReentrantLock。这种改进进一步提高了并发性能，并减少了对锁的争用。
   >
   > 需要注意的是，虽然ConcurrentHashMap是线程安全的，但在某些特定情况下，仍需要额外的同步措施来保证原子性操作。例如，如果需要对键值对进行原子性的条件更新，可以使用ConcurrentHashMap的一些特殊方法，如`putIfAbsent()`、`replace()`等。
   >
   > 总结起来，HashMap适用于单线程环境或者在多线程环境下只有读操作的情况，而ConcurrentHashMap适用于并发环境下的读写操作，提供更好的线程安全性和性能。选择使用哪种哈希表实现取决于具体的需求和并发情况。

3. ### 稳定的排序方式，collections类，sort原理

   > 稳定的排序方式是指在排序过程中具有相同键值的元素，排序后仍然保持它们在原序列中的相对顺序不变。Java中的`Collections`类提供了对集合进行排序的方法，其中最常用的是`Collections.sort()`方法。
   >
   > `Collections.sort()`方法使用的排序算法是基于归并排序（Merge Sort）和插入排序（Insertion Sort）的混合排序算法。它首先将待排序的集合拆分成较小的子集，然后对每个子集进行排序，最后将排序好的子集合并成一个有序的集合。
   >
   > 排序过程中，对于具有相同键值的元素，`Collections.sort()`方法会保持它们在原序列中的相对顺序不变，即排序后它们的相对位置仍然保持一致，这就是稳定排序的特性。
   >
   > 在进行排序时，`Collections.sort()`方法会根据元素的自然顺序（如果元素实现了`Comparable`接口）或者根据自定义的比较器（如果提供了比较器）来确定元素的顺序。
   >
   > 例如，对一个整数列表进行排序可以这样使用`Collections.sort()`方法：
   >
   > ```java
   > List<Integer> numbers = new ArrayList<>();
   > numbers.add(5);
   > numbers.add(2);
   > numbers.add(7);
   > numbers.add(2);
   > numbers.add(1);
   > 
   > Collections.sort(numbers);
   > 
   > System.out.println(numbers); // 输出：[1, 2, 2, 5, 7]
   > ```
   >
   > 在以上示例中，`Collections.sort()`方法对整数列表进行排序后，保持了相同的元素顺序，即两个值为2的元素仍然保持在原来的相对位置。
   >
   > 需要注意的是，`Collections.sort()`方法对于可变的列表进行原地排序，会改变原列表的顺序。如果需要保留原列表不变并返回一个新的排序后的列表，可以使用`List`接口的`sort()`方法，或者创建一个新的列表并将原列表的元素复制到新列表中进行排序。

4. ### Java集合的数据结构

   > 以下是Java中常见的集合类及其对应的数据结构：
   >
   > 1. ArrayList：基于动态数组实现，可以动态增长和缩小。适用于需要频繁访问元素的场景。
   >
   > 2. LinkedList：基于双向链表实现，每个元素都包含前一个和后一个节点的引用。适用于频繁插入、删除操作的场景。
   >
   > 3. HashSet：基于哈希表实现，使用哈希函数将元素映射到桶（Bucket）。不保证元素的顺序，不允许重复元素。
   >
   > 4. TreeSet：基于红黑树（自平衡二叉搜索树）实现，元素按照自然顺序或自定义比较器进行排序。
   >
   > 5. LinkedHashSet：基于哈希表和双向链表实现，保留了元素插入的顺序。
   >
   > 6. HashMap：基于哈希表实现，使用键值对存储数据。不保证键值对的顺序，不允许重复键。
   >
   > 7. TreeMap：基于红黑树实现，键按照自然顺序或自定义比较器进行排序。
   >
   > 8. LinkedHashMap：基于哈希表和双向链表实现，保留了键值对的插入顺序。
   >
   > 9. PriorityQueue：基于堆实现，元素按照优先级进行排序。
   >
   > 10. Stack：基于数组或链表实现的堆栈（后进先出）数据结构。
   >
   > 11. Queue：基于数组或链表实现的队列（先进先出）数据结构。
   >
   > 12. Deque：双端队列，既可以作为队列使用，也可以作为栈使用。
   >
   > 这些集合类都实现了Java集合框架的接口，如List、Set、Map、Queue等。选择合适的集合类取决于具体的需求，包括对元素的访问方式、插入删除的频率、是否允许重复元素、是否需要保持顺序等因素。

5. ### hashmap怎么实现的

   > HashMap是Java中常用的哈希表（Hash Table）实现，用于存储键值对数据。它是基于哈希算法和数组实现的。
   >
   > HashMap的内部数据结构由数组和链表（或红黑树）组成。它的核心思想是通过哈希函数将键映射到数组的索引位置上，从而实现快速的插入、查找和删除操作。
   >
   > 下面是HashMap的实现原理的概述：
   >
   > 1. 初始化：创建一个初始容量（initial capacity）的数组，用于存储键值对。初始容量是指数组的长度，通常是一个较大的素数。
   >
   > 2. 存储键值对：当需要存储一个键值对时，先对键进行哈希计算，得到哈希码（hash code）。哈希码通过一个哈希函数转换为数组的索引位置。如果该索引位置上已经存在其他键值对，可能发生哈希碰撞（hash collision），即多个键计算得到相同的索引位置。
   >
   > 3. 处理哈希碰撞：当发生哈希碰撞时，HashMap使用链表（Java 8之前）或红黑树（Java 8及以后）来解决冲突。如果多个键值对的哈希码相同，它们会组成一个链表或红黑树，存储在同一个索引位置上。
   >
   > 4. 扩容：当元素数量达到数组容量的一定比例（负载因子 load factor）时，HashMap会进行扩容操作，重新调整数组的大小，以便提高性能。扩容涉及重新计算元素的哈希码，并重新分配到新的数组位置上。
   >
   > 5. 查找和删除：通过键的哈希码找到对应的索引位置，然后在链表或红黑树中进行查找或删除操作。由于链表或红黑树的复杂度较低，这些操作都可以在常数时间内完成。
   >
   > 需要注意的是，HashMap允许使用null作为键和值，但在多线程环境下，需要额外的同步措施来保证线程安全性。如果需要在并发环境下使用哈希表，可以考虑使用线程安全的ConcurrentHashMap。
   >
   > 总结起来，HashMap通过哈希算法和数组实现了高效的插入、查找和删除操作。它利用键的哈希码将键值对映射到数组的索引位置，并通过链表或红黑树来解决哈希碰撞。这使得HashMap成为Java中常用的数据结构，用于存储和操作大量的键值对数据。

6. ### hashmap的扩容算法

   > HashMap在扩容时，会重新调整内部数组的大小，以提高性能和保持负载因子在可接受范围内。下面是HashMap的扩容算法的概述：
   >
   > 1. 初始化：HashMap在创建时会设置一个初始容量（initial capacity），通常是一个较大的素数。同时，还会设置一个负载因子（load factor）的阈值，表示数组中元素的平均数量。默认负载因子是0.75。
   >
   > 2. 扩容条件：HashMap在插入元素时，会检查当前元素数量是否达到负载因子与数组容量的乘积。如果达到或超过这个值，就需要进行扩容操作。
   >
   > 3. 扩容操作：当需要扩容时，HashMap会创建一个新的两倍大小的数组，并将所有已有的键值对重新计算哈希码，并重新分配到新的数组位置上。
   >
   > 4. 迁移元素：在重新分配过程中，HashMap会遍历旧数组中的每个桶（bucket），将链表（或红黑树）中的键值对逐个迁移到新的数组中的对应位置上。
   >
   > 5. 红黑树转换：如果在旧数组中的某个桶中存在足够数量的键值对，使得链表的长度超过阈值（默认为8），HashMap会将该链表转换为红黑树，以提高查找效率。
   >
   > 6. 更新容量和阈值：扩容完成后，HashMap会更新内部的容量和阈值，以反映新的数组大小。
   >
   > 在扩容过程中，由于需要重新计算哈希码并重新分配元素，可能会产生一定的开销。因此，较大的初始容量和适当的负载因子可以减少扩容的频率，提高HashMap的性能。
   >
   > 需要注意的是，HashMap的扩容操作是在插入元素时自动进行的，对于使用者来说是透明的。扩容过程对于并发操作是线程安全的，多个线程可以同时进行读取操作，但在扩容过程中的写入操作仍需要进行同步处理。
   >
   > 总结起来，HashMap的扩容算法涉及创建新的数组、重新计算哈希码、迁移元素以及可能的链表转红黑树操作。通过扩容，HashMap能够动态调

7. ### map的get实现原理，如何确定找到了那个值

   > HashMap的`get()`方法实现原理如下：
   >
   > 1. 首先，根据传入的键对象，通过键的`hashCode()`方法计算哈希码（hash code）。
   >
   > 2. 使用哈希码通过哈希函数（Hash Function）计算出在内部数组中的索引位置。哈希函数通常使用取模运算，将哈希码映射到合法的索引范围内。
   >
   > 3. 在确定索引位置后，HashMap会检查该位置上的元素。如果该位置上没有元素，或者存在哈希碰撞（多个键映射到同一个索引位置），则表示未找到对应的值，`get()`方法返回null。
   >
   > 4. 如果该位置上存在元素，HashMap会进一步判断键的相等性。它首先比较键的哈希码，如果哈希码不相等，则表示未找到对应的值，`get()`方法返回null。
   >
   > 5. 如果键的哈希码相等，则继续比较键的相等性。HashMap会调用键对象的`equals()`方法与存储在该位置上的键进行比较。如果键相等，则表示找到了对应的值，`get()`方法返回该键对应的值。
   >
   > 6. 如果键不相等，则可能存在哈希碰撞，此时HashMap会遍历该索引位置上的链表或红黑树（Java 8及以后版本），依次比较链表或红黑树中每个键的相等性。如果找到了相等的键，则返回对应的值；如果遍历完整个链表或红黑树都没有找到相等的键，则表示未找到对应的值，`get()`方法返回null。
   >
   > 需要注意的是，为了正确比较键对象的相等性，键对象必须正确实现`hashCode()`和`equals()`方法。如果键对象的`hashCode()`方法返回不同的哈希码，即使它们的`equals()`方法返回true，HashMap也会认为它们是不同的键。
   >
   > 总结起来，HashMap的`get()`方法通过计算键的哈希码，定位到键在内部数组中的索引位置，并通过比较键的相等性来确定是否找到了指定的值。

8. ### concurrenthashmap怎么实现的，用的什么锁

   > ConcurrentHashMap是Java中的线程安全的哈希表实现，它使用了一些特殊的技术来支持高效的并发访问。下面是ConcurrentHashMap的实现原理：
   >
   > 1. 分段数组：ConcurrentHashMap内部使用了一种分段数组的数据结构。它将整个哈希表分为多个段（Segments），每个段都是一个独立的哈希表。每个段有自己的锁，不同的线程可以同时访问不同的段，从而实现了并发访问。
   >
   > 2. 锁粒度缩小：相比于使用单一的全局锁，ConcurrentHashMap采用了细粒度的锁，即每个段都有自己的锁。这样做的好处是，不同的线程可以同时操作不同的段，从而提高了并发性能。
   >
   > 3. 并发度控制：ConcurrentHashMap的并发度可以通过调整段的数量来控制。在创建ConcurrentHashMap实例时，可以指定段的数量，默认为16。较大的并发度可以提高并发访问的效率，但也会增加内存消耗。
   >
   > 4. 使用CAS操作：ConcurrentHashMap在执行插入、删除和更新操作时，使用了乐观锁的方式，即使用了CAS（Compare and Swap）操作来避免使用显式的锁。这样可以减少对锁的竞争，提高并发性能。
   >
   > 5. 线程安全的迭代器：ConcurrentHashMap提供了线程安全的迭代器，可以在遍历哈希表时防止并发修改。它使用了快照（Snapshot）技术，即在创建迭代器时，对每个段进行了快照，并在迭代期间保持不变。
   >
   > 总结起来，ConcurrentHashMap通过使用分段数组、锁粒度缩小、并发度控制和CAS操作等技术实现了线程安全和高效的并发访问。它的设计目标是提供高并发性能，并在保证线程安全的同时减少对锁的竞争。
   >
   > ---
   >
   > 在早期的 Java 版本（Java 7 及之前），ConcurrentHashMap 使用了分段锁（Segment-Level Locking）。每个段（Segment）都包含一个独立的锁，多个线程可以同时访问不同的段，从而提供了更好的并发性能。
   >
   > 然而，在 Java 8 中，ConcurrentHashMap 的内部实现发生了改变。它引入了一种更为高效的锁机制，称为 "锁分离"（Lock Striping）。锁分离的思想是将整个 ConcurrentHashMap 分成了许多小的锁单元，每个锁单元控制着一部分的桶（buckets）。
   >
   > 具体来说，Java 8 及之后的 ConcurrentHashMap 使用了一种称为 `StampedLock` 的锁机制。`StampedLock` 是一种基于乐观锁的机制，它允许多个线程同时访问共享资源，而不会阻塞读操作。只有在发生冲突的写操作时，读操作才会被阻塞。
   >
   > `StampedLock` 提供了三种模式的锁：读锁（Read Lock）、写锁（Write Lock）和乐观读锁（Optimistic Read Lock）。在 ConcurrentHashMap 的实现中，读操作使用了乐观读锁，写操作使用了写锁。
   >
   > 使用 `StampedLock` 的好处是，在读多写少的情况下，读操作不会被阻塞，可以实现更好的并发性能。同时，它也提供了一种乐观读锁的机制，避免了频繁的加锁和解锁操作。
   >
   > 需要注意的是，具体的实现可能会根据不同的 Java 版本和具体的 JVM 实现而有所不同。因此，这里提到的锁机制是基于 Java 8 的 ConcurrentHashMap 的一般实现原理。在未来的 Java 版本中，锁机制可能会有进一步的改进和优化。

9. ### 生产场景下什么时候用 ArrayList ，什么时候用 LinkedList 

   > 在生产场景下，选择使用ArrayList还是LinkedList取决于具体的需求和使用方式。
   >
   > ArrayList适用的场景：
   > 1. 需要高效的随机访问：ArrayList通过使用基于索引的访问方式，可以在常量时间内（O(1)）获取指定位置的元素。如果需要频繁地根据索引进行访问或更新操作，ArrayList通常比LinkedList更适合。
   >
   > 2. 需要高效的顺序遍历：ArrayList在顺序遍历时具有较好的性能，因为它通过连续的内存块存储元素，可以利用CPU缓存的局部性原理，提高访问效率。
   >
   > 3. 需要更少的内存消耗：ArrayList在存储元素时只需要连续的内存块，而LinkedList需要额外的内存来存储节点之间的链接关系。如果对内存消耗有限制，并且不需要频繁的插入和删除操作，ArrayList可能是更好的选择。
   >
   > LinkedList适用的场景：
   > 1. 频繁的插入和删除操作：由于LinkedList的元素之间使用节点进行链接，插入和删除操作只需要修改节点的引用，而不需要移动其他元素。因此，在需要频繁地插入和删除元素的场景下，LinkedList通常比ArrayList更适合。
   >
   > 2. 需要高效的插入和删除操作：LinkedList在头部和尾部的插入和删除操作具有较好的性能，因为只需要修改节点的引用。但是，在访问指定位置的元素时，LinkedList需要遍历到目标位置，因此访问效率较低。
   >
   > 3. 需要实现队列或栈等数据结构：由于LinkedList具有在头部和尾部进行高效插入和删除的特性，它常被用作实现队列、栈等数据结构。
   >
   > 需要注意的是，上述的适用场景是一般情况下的经验指导，实际选择应根据具体的业务需求和数据访问模式来评估和测试。有时使用ArrayList和LinkedList的混合结构，根据实际需求选择最优的数据结构，也可以获得更好的性能。

10. ### 链表扩容有什么问题

    > 链表在扩容时存在一些问题，主要涉及到内存分配和数据迁移的开销。
    >
    > 1. 额外的内存分配：在链表扩容时，通常需要为新的节点分配额外的内存空间。这可能会导致内存碎片化，并增加内存管理的复杂性。
    >
    > 2. 数据迁移的开销：链表扩容可能需要将现有的元素重新分配到新的内存空间中。这涉及到遍历链表并逐个复制元素的操作，对于大型链表来说，数据迁移的开销可能会很大。
    >
    > 3. 时间复杂度增加：链表扩容的时间复杂度是O(n)，其中n是链表的大小。相比于数组扩容的常数时间复杂度，链表扩容需要更多的时间。
    >
    > 4. 扩容频繁：由于链表的特性，每次插入操作都需要动态地分配新的节点，当插入操作频繁且链表长度不断增长时，链表可能会频繁地触发扩容操作，增加了时间和空间开销。
    >
    > 综上所述，链表扩容存在内存分配、数据迁移、时间复杂度增加和扩容频繁等问题。因此，在选择数据结构时，需要考虑到实际的使用场景和需求，权衡链表的优势和劣势。对于需要频繁的插入和删除操作，但对随机访问的性能要求不高的场景，链表可能仍然是一个合适的选择。然而，如果对内存占用和访问性能有较高要求，可能需要考虑其他的数据结构，如数组或平衡树结构。

11. ### Set 怎么去保证去重的

    > Set 是一种集合数据结构，它可以确保其中的元素不重复。Set 的去重特性是通过其内部的实现机制来实现的。
    >
    > Set 的去重机制主要依赖于两个方面：
    >
    > 1. 基于哈希（Hash-based）的实现：常见的 Set 实现类，如 HashSet，底层使用哈希表（Hash Table）来存储元素。哈希表使用哈希函数将元素映射到不同的存储位置，当插入新元素时，先计算其哈希值，然后根据哈希值找到对应的存储位置。在插入元素时，Set 会先判断该元素的哈希值是否已经存在在哈希表中，如果存在，则认为是重复元素，不进行插入。通过哈希函数和哈希表的机制，Set 可以高效地判断元素是否重复。
    >
    > 2. 基于比较（Comparable/Comparator-based）的实现：某些 Set 实现类，如 TreeSet，不仅使用哈希函数进行去重，还依赖于元素之间的比较机制。在插入元素时，Set 会根据元素的比较结果判断是否重复。如果元素已经存在于 Set 中，则不会进行插入。这要求元素类实现 Comparable 接口或提供 Comparator 对象，以便进行元素的比较。
    >
    > 需要注意的是，为了确保正确的去重，Set 中的元素类必须正确实现 hashCode() 和 equals() 方法。hashCode() 方法用于计算元素的哈希值，equals() 方法用于判断两个元素是否相等。这样才能保证 Set 能够正确地判断元素的重复性。
    >
    > 总结起来，Set 通过使用哈希表和比较机制来实现元素的去重。对于基于哈希的实现（如 HashSet），元素的去重依赖于哈希函数和哈希表的机制。对于基于比较的实现（如 TreeSet），元素的去重除了哈希机制，还需要元素之间的比较。无论是哪种实现，元素类都需要正确实现 hashCode() 和 equals() 方法来确保去重的正确性。

12. ### Set 里面，如果保存的是一个自定义的对象的话，那我这个对象应该是有什么动作吗？

    > 如果你想将自定义的对象作为元素存储在Set中，并希望确保去重的正确性，你的对象需要执行以下两个动作：
    >
    > 1. 重写 hashCode() 方法：为了让Set正确地使用哈希机制进行去重，你需要重写对象的 hashCode() 方法。hashCode() 方法应该根据对象的内容计算出一个整数值，且具有以下特性：
    >    - 如果两个对象相等（equals() 方法返回true），它们的hashCode() 方法应返回相同的值。
    >    - 如果两个对象不相等，hashCode() 方法返回的值可以不同。
    >    - 注意，如果你重写了 equals() 方法，就必须同时重写 hashCode() 方法，以保证 hashCode() 和 equals() 方法的一致性。
    >
    > 2. 重写 equals() 方法：equals() 方法用于比较两个对象的内容是否相等。你需要根据自定义对象的属性来实现 equals() 方法，以确保相同属性的对象被视为相等。equals() 方法应满足以下特性：
    >    - 自反性：对于任意非空对象x，x.equals(x) 应返回 true。
    >    - 对称性：对于任意非空对象x和y，如果 x.equals(y) 返回 true，则 y.equals(x) 也应返回 true。
    >    - 传递性：对于任意非空对象x、y和z，如果 x.equals(y) 返回 true，并且 y.equals(z) 返回 true，则 x.equals(z) 也应返回 true。
    >    - 一致性：对于任意非空对象x和y，在对象没有发生改变的情况下，多次调用 x.equals(y) 应该始终返回相同的结果。
    >    - 对于任意非空对象x，x.equals(null) 应返回 false。
    >
    > 通过重写 hashCode() 和 equals() 方法，你可以确保Set正确地判断对象的重复性，从而实现去重的功能。
    >
    > 需要注意的是，如果你的自定义对象被用作Set的元素时，它应该是不可变的（immutable），或者在添加到Set之后不会改变其内容，以避免破坏Set的内部机制和一致性。

## JVM

1. ### JMM

   > **Java 内存模型**（下文简称 **JMM**）就是在底层处理器内存模型的基础上，定义自己的多线程语义。它明确指定了一组排序规则，来保证线程间的可见性。
   >
   > 这一组规则被称为 **Happens-Before**, JMM 规定，要想保证 B 操作能够看到 A 操作的结果（无论它们是否在同一个线程），那么 A 和 B 之间必须满足 **Happens-Before 关系**：
   >
   > - **单线程规则**：一个线程中的每个动作都 happens-before 该线程中后续的每个动作
   > - **监视器锁定规则**：监听器的**解锁**动作 happens-before 后续对这个监听器的**锁定**动作
   > - **volatile 变量规则**：对 volatile 字段的写入动作 happens-before 后续对这个字段的每个读取动作
   > - **线程 start 规则**：线程 **start()** 方法的执行 happens-before 一个启动线程内的任意动作
   > - **线程 join 规则**：一个线程内的所有动作 happens-before 任意其他线程在该线程 **join()** 成功返回之前
   > - **传递性**：如果 A happens-before B, 且 B happens-before C, 那么 A happens-before C
   >
   > 怎么理解 happens-before 呢？如果按字面意思，比如第二个规则，线程（不管是不是同一个）的解锁动作发生在锁定之前？这明显不对。happens-before 也是为了保证可见性，比如那个解锁和加锁的动作，可以这样理解，线程1释放锁退出同步块，线程2加锁进入同步块，那么线程2就能看见线程1对共享对象修改的结果。
   >
   > ![img](http://blog-img.coolsen.cn/img/image-20210329222941923.png)
   >
   > Java 提供了几种语言结构，包括 *volatile*, *final* 和 *synchronized*, 它们旨在帮助程序员向**编译器**描述程序的并发要求，其中：
   >
   > - **volatile** - 保证**可见性**和**有序性**
   > - **synchronized** - 保证**可见性**和**有序性**; 通过**管程（Monitor）\**保证一组动作的**原子性**
   > - **final** - 通过禁止**在构造函数初始化**和**给 final 字段赋值**这两个动作的重排序，保证**可见性**（如果 **this 引用逃逸**就不好说可见性了）
   >
   > 编译器在遇到这些关键字时，会插入相应的内存屏障，保证语义的正确性。
   >
   > 有一点需要**注意**的是，**synchronized** **不保证**同步块内的代码禁止重排序，因为它通过锁保证同一时刻只有**一个线程**访问同步块（或临界区），也就是说同步块的代码只需满足 **as-if-serial** 语义 - 只要单线程的执行结果不改变，可以进行重排序。
   >
   > 所以说，Java 内存模型描述的是多线程对共享内存修改后彼此之间的可见性，另外，还确保正确同步的 Java 代码可以在不同体系结构的处理器上正确运行。

2. ### Java程序执行过程

   > 当你运行一个Java程序时，以下是Java程序执行的一般过程：
   >
   > 1. 编写源代码：首先，你需要使用文本编辑器编写Java源代码文件，使用.java作为文件扩展名。Java源代码是由类和方法组成的，它定义了程序的行为和逻辑。
   >
   > 2. 编译源代码：一旦你编写完源代码，你需要使用Java编译器（例如javac命令）将源代码编译成字节码文件。字节码是一种中间形式的代码，它可以在Java虚拟机（JVM）上执行。
   >
   > 3. 加载类文件：Java虚拟机负责加载编译后的字节码文件。当你运行Java程序时，JVM会根据需要加载程序中使用的类文件。
   >
   > 4. 验证字节码：在加载类文件之后，JVM会对字节码进行验证，以确保它符合Java语言规范和安全要求。这个过程可以防止恶意代码或非法操作。
   >
   > 5. 解释字节码或即时编译：一旦字节码通过验证，JVM可以选择将字节码解释执行或者使用即时编译器（Just-In-Time Compiler，JIT）将字节码转换为本地机器代码，以提高执行速度。
   >
   > 6. 执行程序：在执行过程中，JVM会按照字节码指令的顺序逐条执行程序。程序的执行包括调用方法、分配和操作内存、处理异常等。
   >
   > 7. 垃圾回收：Java具有自动内存管理机制，称为垃圾回收。在程序执行过程中，JVM会自动回收不再使用的对象和释放内存资源。
   >
   > 8. 程序结束：当程序的主线程完成执行或者调用`System.exit()`方法时，程序会正常结束。在程序结束时，JVM会释放所有的资源并终止。
   >
   > 需要注意的是，Java程序的执行是基于Java虚拟机（JVM）的。JVM是一个运行在操作系统上的虚拟机器，它负责解释和执行Java字节码。由于Java程序是在JVM上执行的，因此它具有跨平台的特性，可以在不同的操作系统上运行，只要这些操作系统上有对应的JVM实现。这种特性是通过将Java源代码编译成字节码，然后在不同平台上的JVM上执行字节码实现的。

3. ### volatile底层如何实现可见性

   > 在Java中，`volatile`关键字用于确保共享变量的可见性和禁止指令重排序。`volatile`的实现方式涉及到编译器和处理器的特性。
   >
   > 1. 编译器层面：编译器会在生成的字节码中插入特定的内存屏障（Memory Barrier）指令，以确保对`volatile`变量的读写操作按照预期顺序进行。这些内存屏障会阻止编译器对指令重排序的优化，保证`volatile`变量的读写操作不会被重排序到其他内存操作之前或之后。
   >
   > 2. 处理器层面：处理器也会对`volatile`变量的读写操作进行特殊处理，以保证其可见性。当一个线程写入一个`volatile`变量时，处理器会立即将该值刷新到主内存中，而不是仅仅缓存在处理器的缓存中。当另一个线程读取一个`volatile`变量时，处理器会从主内存中读取最新的值，而不是使用缓存中的旧值。
   >
   > 通过编译器和处理器的协同工作，`volatile`变量的读写操作能够保证可见性。当一个线程修改了`volatile`变量的值，其他线程立即能够看到最新的值，而不会使用过期的缓存值。
   >
   > 需要注意的是，虽然`volatile`关键字可以确保可见性，但它并不能解决所有的并发问题。例如，`volatile`不能保证原子性，无法解决复合操作的一致性问题。对于复杂的并发场景和操作，可能需要使用其他的同步机制，如锁（`synchronized`）或原子变量（`Atomic`）来保证线程安全性。

4. ### 为什么 volatile 能保证多线程可见

   > `volatile`关键字能够保证多线程可见性的原因主要有两个方面：
   >
   > 1. 禁止指令重排序：编译器和处理器在对`volatile`变量的读写操作上会插入内存屏障（Memory Barrier）指令，这些屏障指令会阻止编译器和处理器对指令进行重排序优化。这意味着在`volatile`变量的读写操作前后的其他指令都不能被重排序到其前后，从而保证了`volatile`变量的操作顺序按照程序的编写顺序执行。这样就避免了指令重排序可能导致的可见性问题。
   >
   > 2. 强制刷新到主内存：当一个线程对`volatile`变量进行写操作时，编译器会生成一条写屏障指令，该指令会将更新后的值立即刷新到主内存中。而其他线程在读取该`volatile`变量时，会从主内存中读取最新的值，而不是使用本地线程的缓存值。这样就保证了写操作对其他线程可见，即可见性的保证。
   >
   > 综上所述，`volatile`关键字通过禁止指令重排序和强制刷新到主内存的机制，确保了对`volatile`变量的读写操作在多线程中的可见性。这使得当一个线程修改了`volatile`变量的值时，其他线程能够立即看到最新的值，而不会使用过期的缓存值。但需要注意，`volatile`关键字只能保证可见性，并不能解决原子性和复合操作的一致性问题，对于这些问题需要使用其他的同步机制。

5. ### 关于volatile实现可见性底层中的主存和本地内存分别是物理机器中的什么

   > 在`volatile`实现可见性的底层机制中，涉及到两个概念：主内存（Main Memory）和本地内存（Local Memory）。
   >
   > 主内存是计算机系统中的一块物理内存区域，它可以被所有线程访问。主内存可以被看作是所有线程共享的内存存储区域，其中存储了所有的共享变量。在主内存中，共享变量的值可以被所有线程读取和修改。
   >
   > 本地内存是指每个线程独立拥有的内存区域，用于存储线程所需要的数据或者共享变量的副本。每个线程都有自己的本地内存，本地内存中的数据对其他线程是不可见的。当一个线程访问共享变量时，它首先会将共享变量的副本从主内存中读取到自己的本地内存中进行操作，然后再将修改后的值刷新回主内存。
   >
   > 当使用`volatile`修饰共享变量时，主内存中的共享变量的值在每次被线程访问时都会被强制刷新到线程的本地内存中，并且对共享变量的修改也会立即刷新回主内存。这样可以保证不同线程之间对共享变量的操作都是基于主内存中最新的值进行的，从而实现了可见性。
   >
   > 需要注意的是，主内存和本地内存的概念是逻辑上的概念，并不一定对应于物理机器中的具体存储区域。具体的实现可能会有不同的策略和机制来管理主内存和本地内存之间的数据交互，以满足`volatile`关键字的语义要求。

6. ### JVM内存模型，哪些会发生OOM

   > JVM（Java虚拟机）内存模型定义了Java程序在运行时所使用的内存结构和管理方式。JVM内存模型主要分为以下几个部分：
   >
   > 1. 堆（Heap）：堆是JVM管理的最大一块内存区域，用于存储对象实例。在堆中，被分配的对象可以通过自动内存管理机制（垃圾回收）进行分配和释放。堆内存可以分为新生代（Young Generation）和老年代（Old Generation）两部分。
   >
   > 2. 方法区（Method Area）：方法区用于存储类的元数据信息，包括类的信息、常量池、静态变量、即时编译器编译后的代码等。方法区也被称为永久代（Permanent Generation），但在JDK 8及以后的版本中，永久代被元空间（Metaspace）所取代。
   >
   > 3. 虚拟机栈（VM Stack）：每个线程在运行时都会创建一个对应的虚拟机栈，用于存储方法的局部变量、操作数栈、返回值等。每个方法的执行都会创建一个栈帧（Stack Frame），用于存储方法的信息。
   >
   > 4. 本地方法栈（Native Method Stack）：本地方法栈与虚拟机栈类似，但是用于执行本地方法（使用非Java语言编写的方法）。
   >
   > 5. 程序计数器（Program Counter）：程序计数器是一个较小的内存区域，用于存储当前线程正在执行的字节码指令的地址。程序计数器在线程切换时会被保存和恢复。
   >
   > 在JVM中，可能会发生OOM（OutOfMemoryError）错误的情况有以下几种：
   >
   > 1. 堆内存溢出：当堆中没有足够的空间来分配新的对象时，会发生堆内存溢出。这通常是因为程序创建了大量的对象，而垃圾回收无法及时回收这些对象，导致堆空间耗尽。
   >
   > 2. 方法区溢出：虽然在JDK 8及以后的版本中，方法区被元空间所取代，但仍然可能发生方法区溢出。当加载的类信息过多、常量池或静态变量占用过多内存时，都可能导致方法区溢出。
   >
   > 3. 虚拟机栈溢出：当线程请求的栈深度超过虚拟机栈允许的最大深度时，会发生虚拟机栈溢出。这通常是由于方法调用层级过深或者存在无限递归调用造成的。
   >
   > 4. 本地方法栈溢出：类似于虚拟机栈溢出，当本地方法请求的栈深度超过本地方法栈允许的最大深度时，会发生本地方法栈溢出。
   >
   > 需要注意的是，JVM还有一些其他的内存区域，如直接内存（Direct Memory）等，但它们不属于JVM规范中定义的内存模型，因此不在本次讨论范围内。

7. ### jvm组成，垃圾回收算法

   > JVM（Java虚拟机）是Java程序的运行环境，它由多个组件组成。以下是JVM的主要组成部分：
   >
   > 1. 类加载器（Class Loader）：类加载器负责将Java类的字节码加载到JVM中，并生成对应的Class对象。类加载器按照一定的委托关系进行类加载，可以从不同的来源加载类，如本地文件系统、网络等。
   >
   > 2. 执行引擎（Execution Engine）：执行引擎负责执行加载到内存中的字节码指令。它可以有解释执行和编译执行两种模式。解释执行逐条解释字节码指令执行，编译执行将字节码转换为机器码后执行，提高了执行效率。
   >
   > 3. 运行时数据区（Runtime Data Area）：运行时数据区是JVM在运行时分配和管理内存的区域。它包括堆、方法区、虚拟机栈、本地方法栈和程序计数器等。
   >
   > 4. 垃圾回收器（Garbage Collector）：垃圾回收器负责回收不再使用的内存资源，释放对象占据的内存空间。垃圾回收器会扫描内存中的对象，识别出不再被引用的对象，然后进行回收和内存整理。
   >
   > 垃圾回收算法是垃圾回收器的核心部分，它决定了如何判断对象是否存活以及如何进行垃圾回收。以下是几种常见的垃圾回收算法：
   >
   > 1. 标记-清除算法（Mark and Sweep）：该算法分为两个阶段，首先标记所有活动对象，然后清除未标记的对象。标记-清除算法会产生内存碎片，可能会导致内存分配效率降低。
   >
   > 2. 复制算法（Copying）：该算法将内存划分为两个区域，每次只使用其中的一半。当一半区域的对象存活时，将存活对象复制到另一半区域。复制算法解决了内存碎片的问题，但会浪费一部分内存。
   >
   > 3. 标记-压缩算法（Mark and Compact）：该算法首先标记活动对象，然后将存活对象压缩到内存的一端，清理未标记的对象。标记-压缩算法解决了内存碎片问题，但需要进行对象的移动，对于大对象或存活对象较多的情况可能效率较低。
   >
   > 4. 分代算法（Generational）：分代算法根据对象的存活周期将内存划分为不同的代（Generation），一般分为新生代（Young Generation）和老年代（Old Generation）。新生代中的对象生命周期较短，采用复制算法进行垃圾回收；老年代中的对象生命周期较长，采用标记-压缩算法进行垃圾回收。
   >
   > 以上只是一些常见的垃圾回收算法，实际上还有其他算法和优化技术，如增量式、并发和并行垃圾回收等。不同的垃圾回收器可以选择不同的算法来适应不同的应用场景和性能要求。

8. ### 可以作为GCRoots对象的有哪些？(直接懵逼)

   > 虚拟机栈(栈帧中的本地变量表)中引用的对象
   >
   > 本地方法栈(Native 方法)中引用的对象
   >
   > 方法区中类静态属性引用的对象
   >
   > 方法区中常量引用的对象
   >
   > 所有被同步锁持有的对象
   >
   > JNI（Java Native Interface）引用的对象

9. ### OOM的常见场景及你是如何检查这个问题的

   > OOM（OutOfMemoryError）是指Java程序在运行过程中无法分配更多的内存空间，导致内存耗尽的错误。以下是一些常见的导致OOM的场景：
   >
   > 1. 堆内存溢出：当程序创建了大量的对象，并且无法及时回收这些对象时，堆内存会耗尽，导致OOM。这可能是因为程序中存在内存泄漏、对象生命周期过长或者堆空间设置不合理等原因。
   >
   > 2. 方法区溢出：当加载的类信息过多、常量池或静态变量占用过多内存时，会导致方法区溢出。这通常发生在动态生成大量类的场景，如使用CGLIB等字节码增强技术。
   >
   > 3. 虚拟机栈溢出：当线程请求的栈深度超过虚拟机栈允许的最大深度时，会发生虚拟机栈溢出。这通常是由于方法调用层级过深或者存在无限递归调用造成的。
   >
   > 4. 本地方法栈溢出：类似于虚拟机栈溢出，当本地方法请求的栈深度超过本地方法栈允许的最大深度时，会发生本地方法栈溢出。
   >
   > 为了检查OOM问题，可以采取以下一些方法：
   >
   > 1. 查看错误日志：当程序发生OOM错误时，通常会在错误日志中输出相关信息，如错误类型（OutOfMemoryError）和错误堆栈跟踪。查看错误日志可以了解到发生OOM的位置和原因。
   >
   > 2. 分析堆转储文件（Heap Dump）：在发生OOM时，可以生成堆转储文件，记录了OOM发生时堆内存中的对象信息。通过分析堆转储文件，可以查看哪些对象占用了大量内存，进而定位到可能存在内存泄漏的代码。
   >
   > 3. 使用内存分析工具：有许多专门的内存分析工具可用于检测和分析OOM问题，如Eclipse Memory Analyzer（MAT）、VisualVM等。这些工具可以帮助查看堆内存的使用情况、对象引用关系以及内存泄漏等信息。
   >
   > 4. 检查代码逻辑：仔细检查程序的代码逻辑，尤其是涉及大量对象创建和使用的地方。确保及时释放不再使用的对象，并注意避免不必要的对象创建和持有。
   >
   > 5. 调整堆内存设置：如果确定程序需要更多的堆内存，可以通过调整JVM的启动参数来增加堆内存的大小，例如使用-Xmx参数来增大最大堆内存限制。
   >
   > 综合使用以上方法，可以更好地检查和定位OOM问题，并进行相应的优化和调整。

10. ### 方法区会出现什么问题

    > 方法区是JVM的一个重要组成部分，用于存储类的元数据信息、常量池、静态变量等。在某些情况下，方法区可能会遇到以下问题：
    >
    > 1. 方法区溢出（OutOfMemoryError）：当加载的类信息过多、常量池或静态变量占用过多内存时，会导致方法区溢出。这可能发生在动态生成大量类的场景，如使用CGLIB等字节码增强技术，或者在大型应用中存在大量的类加载和卸载操作。
    >
    > 2. 内存泄漏：方法区中存储的类信息是不会被垃圾回收器自动回收的，因此如果存在不再使用的类或类加载器的引用没有被正确释放，就会导致方法区的内存泄漏。长时间运行的应用程序可能会不断加载和卸载类，如果不正确处理类的引用，方法区的内存占用会越来越高。
    >
    > 3. 类信息无法卸载：由于方法区存储了类的元数据信息，使得类的卸载变得复杂。在某些情况下，如果类的加载器仍然存在强引用，即使类实例已经不再被引用，类信息也无法被卸载，导致方法区的内存无法释放。
    >
    > 4. 运行时常量池限制：方法区中的常量池用于存储类的常量，包括字符串字面量、符号引用等。如果程序中使用大量的字符串字面量或者动态生成大量的常量，可能会导致方法区中的常量池大小不够，从而抛出OutOfMemoryError。
    >
    > 为了解决方法区的问题，可以采取以下一些方法：
    >
    > 1. 调整方法区大小：可以通过调整JVM的启动参数，如-XX:MaxMetaspaceSize来增加方法区的大小限制。
    >
    > 2. 关闭动态类加载和卸载：在某些场景下，关闭动态类加载和卸载（如使用CGLIB等字节码增强技术）可以减少方法区的内存占用。
    >
    > 3. 清理无用的类和类加载器引用：及时释放不再使用的类和类加载器的引用，防止方法区内存泄漏。
    >
    > 4. 优化字符串和常量的使用：避免过多使用字符串字面量和大量动态生成的常量，可以减少方法区常量池的内存消耗。
    >
    > 总之，了解方法区的问题和针对性的优化措施可以帮助保证JVM的稳定性和性能。

11. ### G1 CMS ，深问了 G1后台线程维护回收区域和回收价值 优先级列表是怎么做的

    > G1（Garbage-First）是一种现代的垃圾回收器，它是在Java 7中引入的，旨在改进大堆内存下的垃圾回收性能和暂停时间。G1回收器使用了一种基于区域的垃圾回收方式，将整个堆内存划分为多个大小相等的区域，并以不同的方式管理这些区域。
    >
    > G1回收器的后台线程主要负责维护回收区域和执行垃圾回收操作。下面是G1后台线程的功能及其作用：
    >
    > 1. 并发标记线程（Concurrent Marking Thread）：并发标记线程是G1回收器最重要的线程之一。它负责在应用程序运行的同时，标记出所有存活对象。并发标记线程的工作是在整个堆内存中进行的，它会从根对象开始进行可达性分析，标记所有可达的对象。这个过程是并发进行的，不会停顿应用程序的执行。
    >
    > 2. 并发预备重标记线程（Concurrent Prepare Remark Thread）：这个线程在并发标记阶段之后启动，负责处理在并发标记期间发生的引用关系变化。它会标记那些在并发标记阶段发生变化的对象，并将这些对象放入回收集（Collection Set）中。
    >
    > 3. 并发清理线程（Concurrent Cleanup Thread）：并发清理线程负责执行垃圾回收过程中的一些清理操作。它会清理无效的对象引用、释放空闲的内存空间，并协助处理并发标记和并发预备重标记期间产生的垃圾。
    >
    > 4. 并发回收线程（Concurrent Evacuation Thread）：并发回收线程负责执行实际的垃圾回收操作。它会在后台并发地将垃圾对象从Eden区、存活对象区域（Survivor区）和回收区域（Collection Set）中清除，并将存活对象复制到空闲的区域中。
    >
    > G1回收器使用回收价值（Recycling Value）优先级列表来确定每个回收区域的回收顺序。回收价值是指回收区域中包含的垃圾对象的数量和回收收益的综合评估。G1回收器会根据回收价值优先级列表，优先选择具有更高回收价值的区域进行回收，以最大程度地提高垃圾回收效率。
    >
    > 回收价值优先级列表是通过一种基于采样的动态计算方式得出的。G1回收器会定期对各个回收区域进行采样，统计区域中的垃圾对象数量和回收收益，然后根据这些统计结果动态调整回收价值优先级列表。这样做的目的是使G1回收器能够根据实际情况动态地选择回收区域，以提高整体的垃圾回收效率。
    >
    > 总的来说，G1回收器通过并发标记线程和后台线程的协作，以及回收价值优先级列表的动态调整，实现了高效的垃圾回收和较低的暂停时间，适用于大堆内存下的应用程序。

12. ### 垃圾回收算法,g1 zgc

13. ### 垃圾回收中为什么要stop the world

    > 在垃圾回收（Garbage Collection）过程中，“Stop-the-World”是指应用程序的执行在某个时间点被暂停，以便垃圾回收器能够安全地执行必要的操作。在这段停顿时间内，所有的应用程序线程都被挂起，不再执行任何代码。
    >
    > Stop-the-World 暂停的目的是为了保证垃圾回收器能够在一个稳定的状态下执行，以避免与并发执行的应用程序线程发生冲突。以下是几个主要的原因，解释了为什么在垃圾回收中需要 Stop-the-World 停顿：
    >
    > 1. 标记阶段：在大多数垃圾回收算法中，标记阶段是必要的步骤，用于确定哪些对象是存活的。为了保证标记的准确性，必须停止应用程序线程，以便垃圾回收器可以遍历整个对象图，并进行标记。否则，在标记过程中，应用程序可能会继续创建、删除或修改对象，导致标记的不一致性。
    >
    > 2. 垃圾回收器的安全性：垃圾回收器在执行清除操作时，需要对堆内存进行修改，清除被标记为垃圾的对象。如果在清除过程中应用程序线程继续执行，可能会访问或引用已经被回收的对象，导致不可预测的错误。通过 Stop-the-World 停顿，可以保证垃圾回收器能够安全地执行清除操作，而不会影响应用程序的执行。
    >
    > 3. 堆一致性：在某些垃圾回收算法中，如复制算法或标记-整理算法，对象需要在堆内进行移动。如果在移动过程中应用程序线程继续执行，可能会引用已经移动的对象，导致错误的引用或数据不一致。通过 Stop-the-World 停顿，可以确保在对象移动期间应用程序线程不会访问或引用这些对象。
    >
    > 尽管 Stop-the-World 停顿会导致应用程序的暂停和延迟，但它是确保垃圾回收器的正确执行和堆一致性的关键机制。现代的垃圾回收器，如G1回收器，通过并发执行和增量标记等技术，尽量减少 Stop-the-World 停顿的时间，并在可行的情况下与应用程序线程并发执行，以减少对应用程序性能的影响。

14. ### 垃圾回收中safepoint的作用

    > 在垃圾回收中，"Safepoint"（安全点）是指应用程序中的一个特定位置，在这个位置上，垃圾回收器可以安全地暂停应用程序的执行并进行必要的操作，例如线程挂起、栈的检查和修改等。Safepoint 的作用是确保垃圾回收器和应用程序之间的协调和同步。
    >
    > 以下是 Safepoint 的几个主要作用：
    >
    > 1. 停顿应用程序：Safepoint 提供了一个机制，使得垃圾回收器可以在某个安全点暂停所有的应用程序线程。这样做的目的是为了在停顿期间，确保应用程序的状态不会发生变化，从而保证垃圾回收器的操作的正确性和一致性。
    >
    > 2. 栈的检查和修改：在 Safepoint 上，垃圾回收器可以安全地检查和修改应用程序线程的栈。这对于垃圾回收器来说是非常重要的，因为它需要遍历和标记栈上的对象引用，以确定哪些对象是存活的。Safepoint 提供了一个确定的时间点，在此期间垃圾回收器可以安全地访问和修改栈的状态。
    >
    > 3. 全局数据结构的一致性：在 Safepoint 上，垃圾回收器可以确保全局数据结构的一致性。这包括对象图、引用关系和其他与垃圾回收相关的数据结构。在 Safepoint 上，垃圾回收器可以暂停所有的应用程序线程，并在此期间对这些数据结构进行必要的修改和更新，以确保数据一致性。
    >
    > 4. 确定垃圾回收开始和结束的时间：在 Safepoint 上，垃圾回收器可以确定垃圾回收的开始和结束时间。当所有的应用程序线程都达到了 Safepoint，垃圾回收器可以开始执行垃圾回收操作。同时，在垃圾回收完成后，垃圾回收器会通知应用程序线程继续执行。这样可以确保垃圾回收操作的正确性和完整性。
    >
    > 总之，Safepoint 在垃圾回收中起着重要的作用。它提供了一个安全的点，垃圾回收器可以在此暂停应用程序的执行，并进行必要的操作，以确保垃圾回收的正确性、一致性和协调性。

15. ### java出现异常会打印异常信息吗

    > 是的，Java 在出现异常时通常会打印异常信息。当 Java 程序抛出异常时，它会生成一个异常对象，其中包含有关异常的详细信息，如异常类型、异常消息、异常发生的位置等。默认情况下，Java 会将这些异常信息打印到标准错误流（stderr）。
    >
    > 通常情况下，Java 运行时环境会提供一个默认的异常处理机制，当异常没有被显式地捕获和处理时，它会将异常信息输出到控制台。这些信息通常包括异常的类型、消息和堆栈跟踪信息，用于帮助开发人员定位和解决问题。
    >
    > 例如，以下是一个简单的 Java 代码示例，会触发一个空指针异常（NullPointerException）：
    >
    > ```java
    > public class ExceptionExample {
    >     public static void main(String[] args) {
    >         String str = null;
    >         System.out.println(str.length());
    >     }
    > }
    > ```
    >
    > 当执行该代码时，由于尝试在一个空对象上调用 `length()` 方法，会抛出空指针异常。Java 运行时会打印异常信息，类似于以下内容：
    >
    > ```
    > Exception in thread "main" java.lang.NullPointerException
    >     at ExceptionExample.main(ExceptionExample.java:4)
    > ```
    >
    > 这个异常信息指示了异常类型（NullPointerException），异常发生的位置（在 `ExceptionExample.java` 文件的第4行），帮助开发人员定位问题所在。
    >
    > 需要注意的是，异常信息的打印方式可以通过自定义异常处理机制进行修改，例如，可以使用 try-catch 块捕获异常并进行自定义处理，或者使用日志框架记录异常信息到日志文件中。但是在没有显式处理异常的情况下，默认行为是将异常信息打印到控制台。

## Java并发

线程池某个线程挂了会对线程池出现什么影响？出现异常怎么处理

LinkedList多线程并发，会出现环吗？怎么优化？头插和尾插区别？

公平锁和非公平锁区别

线程池的关键参数；

线程池整个流程，从大量任务进入到任务被执行；

如何理解线程安全，有哪些方式保证线程安全

线程池的使用，出现错误会有什么应对措施

乐观锁悲观锁说说你的理解

ABA问题

乐观锁应用场景

线程池怎么让一个线程停止，怎么阻塞

CAS的机制

CAS怎么解决ABA问题，jdk里用什么解决上一个问题

多线程扩容会有什么问题

进程、线程、协程的关系，Java中的锁，乐观锁悲观锁使用场景

如果中途一些线程离开或者失效，怎么办，怎么维护

wait和join哪个会释放锁

BlockingQueue有哪些实现类，基本原理，怎么实现阻塞的

synchronized和reentrantlock区别

synchronized实现原理，锁的机制

基于synchronized加锁，多线程去抢资源，底层会发生什么

创建线程的方式

在并发量特别高的情况下是使用 synchronized 还是 ReentrantLock

你说高并发下 ReentrantLock 性能比 synchronized 高，那为什么 ConcurrentHashMap 在 JDK 1.7 中要用 ReentrantLock，而 ConcurrentHashMap 在 JDK 1.8 要用 Synchronized

说一下 ConcurrentHashMap 中并发安全的实现

有哪些并发安全的实现方式

不用 ThreadLocal 你会想用什么方式存用户信息

Java多线程编程需要注意哪些地方

多线程间如何传值

有哪些方式做多线程开发，需要注意哪些点。说了很多注意点，可能没答到面试官想要的那个答案，一直问“还有吗”

线程用一次就创建一次，然后销毁，行吗，怎么解决

Java线程池刚启动时是多少个线程（是0还是核心线程数）？0

介绍下AQS

如何用AQS实现共享锁

CountDownLatch原理

AQS如何改变state的值（CAS）

ReentrantLock公平锁和非公平锁实现

## 数据库

### 慢SQL怎么排查，主要看explain执行计划的哪些内容

> 排查慢 SQL 的过程可以借助 `EXPLAIN` 执行计划来进行。`EXPLAIN` 是一个用于分析查询执行计划的关键字，它可以帮助我们理解数据库是如何执行查询的，并找出可能导致性能问题的地方。在执行 `EXPLAIN` 时，可以关注以下关键内容：
>
> 1. **id**：每个查询操作的唯一标识符。在一个查询中，id 的值会按照执行顺序递增。
>
> 2. **select_type**：描述查询的类型，常见的值包括 `SIMPLE`（简单查询）、`PRIMARY`（主查询）和 `SUBQUERY`（子查询）等。
>
> 3. **table**：查询操作涉及的表名。
>
> 4. **type**：表示查询使用的访问方法，常见的类型有 `ALL`（全表扫描）、`index`（使用索引扫描）和 `range`（使用索引范围扫描）等。一般来说，访问方法的优化顺序是 `system` < `const` < `eq_ref` < `ref` < `range` < `index` < `ALL`。
>
> 5. **possible_keys**：表示查询可能使用的索引。
>
> 6. **key**：实际使用的索引。
>
> 7. **rows**：表示查询需要扫描的行数或估计的行数。
>
> 8. **Extra**：包含其他额外信息，如 `Using where`（表示查询中使用了 WHERE 条件）和 `Using index`（表示查询使用了覆盖索引）等。
>
> 通过分析 `EXPLAIN` 的执行计划，可以关注以下方面来排查慢 SQL：
>
> - **索引使用情况**：检查是否使用了合适的索引，确保索引能够优化查询性能。如果没有使用索引或使用了不合适的索引，可能需要调整数据库表的索引设计。
>
> - **访问方法**：关注查询的访问方法，特别是是否存在全表扫描或范围扫描等低效的操作。根据情况，考虑是否需要优化查询条件或添加适当的索引。
>
> - **数据统计准确性**：执行计划中的行数估算（rows）是否准确，如果估算明显偏离实际，可能需要更新数据库的统计信息。
>
> - **连接类型**：对于涉及多个表的查询，检查连接类型（如 `JOIN`）是否合理，避免产生笛卡尔积或过多的连接操作。
>
> - **查询复杂度**：评估查询的复杂度，如果查询逻辑过于复杂，可能需要重新考虑查询的设计和优化。
>
> 此外，还可以结合其他性能分析工具和技术，如慢查询日志、

### 大数据量分页查询慢怎么解决

> 当面对大数据量的分页查询慢的情况时，可以考虑以下几种解决方法：
>
> 1. **使用索引优化**：确保查询的字段或排序字段上存在适当的索引。索引可以加快查询的速度，特别是在排序和筛选条件上。通过分析查询的执行计划，确认是否使用了合适的索引，并根据需要进行索引的创建或优化。
>
> 2. **使用游标或主键分页**：对于数据库支持游标或主键分页的情况，可以通过使用游标或主键来实现分页查询，而不是使用传统的 `LIMIT` 和 `OFFSET`。这种方式可以减少数据库在每次查询时扫描的数据量，提高查询效率。
>
> 3. **缓存查询结果**：如果查询结果变化不频繁，可以考虑将查询结果缓存在缓存中，以避免每次都进行数据库查询。这可以通过使用缓存系统如Redis或Memcached来实现。当需要分页查询时，首先从缓存中获取数据，如果缓存中不存在，则进行数据库查询，并将结果存入缓存中。
>
> 4. **数据分片**：如果数据量非常大，可以考虑对数据进行分片存储。将数据按照某个维度（如时间范围、地理位置等）进行分割，并存储在不同的数据表或数据库中。在进行分页查询时，只查询所需分片的数据，避免了对整个数据集的扫描，提高查询效率。
>
> 5. **预聚合或摘要数据**：如果分页查询的结果是聚合数据（如总数、平均值等），可以在数据更新时预先计算并存储聚合结果。这样，在进行分页查询时，可以直接获取预先计算好的聚合结果，避免了大量数据的扫描和计算。
>
> 6. **分布式存储和计算**：对于非常大的数据集，可以考虑使用分布式存储和计算系统，如Hadoop、Spark等。这些系统可以并行处理大规模数据，并提供高效的分页查询功能。
>
> 7. **优化查询语句和数据模型**：审查查询语句和数据模型，确保它们是合理的并且符合业务需求。优化查询语句，避免不必要的计算和扫描。优化数据模型，将数据存储和查询方式设计得更加高效。
>
> 综合考虑以上方法，可以根据具体情况选择合适的解决方案。通常情况下，综合使用索引优化、合理的数据分片和缓存等策略可以显著提高大数据量分页查询的性能。

### 数据库索引的结构

> 数据库索引是一种数据结构，用于提高数据库查询的效率。常见的数据库索引结构包括以下几种：
>
> 1. **B树索引**：B树（Balanced Tree）是一种平衡的多路搜索树，常用于数据库索引。B树索引按照排序顺序将数据存储在一个平衡树结构中，使得在最坏情况下，每个节点的查找时间复杂度为O(log N)。B树索引适用于范围查询和精确查找。
>
> 2. **B+树索引**：B+树（Balanced Plus Tree）是对B树的一种改进，常用于数据库索引。B+树索引与B树索引类似，但在B+树中，所有的数据都存储在叶子节点中，而非内部节点。这样可以减少磁盘I/O操作，提高范围查询的效率。
>
> 3. **哈希索引**：哈希索引使用哈希函数将索引键映射到存储位置，适用于等值查找。哈希索引的查询速度非常快，通常为O(1)的时间复杂度。然而，哈希索引不支持范围查询，也不适用于排序操作。
>
> 4. **全文索引**：全文索引用于对文本内容进行搜索。它可以在文本中建立索引，以便进行关键词搜索和模糊匹配。全文索引常用的数据结构包括倒排索引（Inverted Index）和Trie树（Prefix Tree）等。
>
> 5. **位图索引**：位图索引用于处理具有少量不同取值的列，如性别、状态等。它使用位图来表示每个不同取值的存在与否，使得位图索引在位运算上非常高效，适用于快速的位查询和位运算操作。
>
> 6. **组合索引**：组合索引是指在多个列上创建的索引，可以同时对多个列进行查询。组合索引可以提供更高效的范围查询、排序和筛选操作，但也会增加索引的维护和存储开销。
>
> 不同的数据库管理系统支持不同类型的索引结构，并且各种索引结构在不同的查询场景下有着不同的优势。选择合适的索引结构需要考虑数据库的特性、查询需求以及数据的分布情况等因素。

### acid是怎么实现的

> ACID（原子性、一致性、隔离性和持久性）是一组数据库事务的特性，用于确保数据库操作的正确性和可靠性。下面是对每个特性的简要说明：
>
> 1. **原子性（Atomicity）**：事务是一个原子操作单元，要么全部执行成功，要么全部回滚到事务开始前的状态。这意味着事务内的所有数据库操作要么都被执行，要么都不执行。数据库管理系统通过日志（或者其他机制）记录事务的操作，以便在故障发生时进行回滚。
>
> 2. **一致性（Consistency）**：事务开始前和结束后，数据库的状态必须保持一致。这意味着事务的执行不能破坏数据库中的完整性约束条件（如主键、外键、唯一性约束等）。如果事务执行失败或回滚，数据库必须返回到之前的一致状态。
>
> 3. **隔离性（Isolation）**：并发执行的多个事务之间应该相互隔离，使得每个事务都感觉不到其他事务的存在。隔离性确保事务在并发执行时不会相互干扰，防止数据的不一致性和并发问题（如脏读、不可重复读和幻读等）。
>
> 4. **持久性（Durability）**：一旦事务提交成功，其所做的修改将永久保存在数据库中，即使在系统发生崩溃或重启的情况下也不会丢失。数据库管理系统通过将事务的操作持久化到磁盘或其他可靠的存储介质上，确保数据的持久性。
>
> 数据库管理系统通过使用多种技术和机制来实现ACID特性，包括：
>
> - **事务日志**：记录事务的操作，以便在故障发生时进行恢复和回滚。
>
> - **锁机制**：使用锁来管理并发事务的访问和修改权限，确保隔离性和一致性。
>
> - **多版本并发控制（MVCC）**：通过为每个事务创建多个版本的数据来实现隔离性，从而避免了一些并发问题。
>
> - **写前日志（Write-Ahead Logging，WAL）**：在事务进行修改之前，将修改操作记录到日志中，确保持久性。
>
> - **缓冲管理**：使用缓冲区管理技术，将事务的修改暂时保存在内存中，以提高性能，并在适当的时候将修改持久化到磁盘上。
>
> 综合使用以上技术和机制，数据库管理系统能够实现ACID特性，确保事务的正确性、一致性和持久性。不同的数据库管理系统可能会使用不同的实现方式，但总体目标都是保证数据的可靠性和一致性。

### mysql什么情况下加表锁

> 在MySQL中，可以根据具体需求和场景决定是否需要使用表级锁。以下是一些常见情况下可能需要加表锁的情况：
>
> 1. **并发写操作**：当多个会话需要同时对同一张表进行写操作（如插入、更新、删除），并且可能存在数据冲突的情况下，可以考虑使用表级锁来保证数据的一致性。表级锁可以防止其他会话对整个表进行写操作。
>
> 2. **表维护**：在进行表结构变更、数据迁移、数据备份或其他表维护操作时，为了避免其他会话对表的干扰，可以加表锁来阻止对表的读写操作。
>
> 3. **数据一致性要求高**：在某些特殊情况下，对于数据一致性要求非常高的操作，可以使用表级锁来确保数据的完整性。例如，在进行复杂的数据处理逻辑、数据导入导出等操作时，为了保证数据的一致性，可以对相关表加表级锁。
>
> 需要注意的是，表级锁对并发性能有一定的影响，因为它会限制其他会话对表的访问。因此，在考虑加表锁时，需要综合评估并发需求、数据一致性要求和性能影响等因素。
>
> 此外，MySQL还提供了更细粒度的锁机制，如行级锁和页级锁，可以更灵活地控制并发访问。在实际应用中，根据具体的业务需求和性能要求，可以选择合适的锁级别来进行并发控制。

### 非唯一索引范围查询加锁方式

> 在非唯一索引的范围查询中，MySQL使用的是行级锁来实现并发控制。对于范围查询，可能涉及到多个索引键值范围内的行，因此需要加锁来保证查询的一致性和避免并发冲突。
>
> 在MySQL中，对于非唯一索引的范围查询，通常会使用共享锁（Shared Lock）来防止其他事务对查询结果进行修改，同时允许其他事务也进行读操作。这样可以保证并发读取的一致性。
>
> 具体的加锁方式取决于事务的隔离级别。在MySQL默认的可重复读隔离级别下，范围查询会使用Next-Key Locks，它是行级锁和间隙锁（Gap Lock）的组合。Next-Key Locks会在索引键上加共享锁，并在索引键之间的间隙上加间隙锁，以确保查询结果的一致性。
>
> 需要注意的是，范围查询的加锁行为可能会导致锁冲突和性能问题。如果范围查询的结果集较大或涉及到大量的行，可能会导致锁的竞争和等待，影响并发性能。因此，在设计数据库架构和查询逻辑时，需要综合考虑并发需求、数据一致性要求和性能影响，选择合适的索引、隔离级别和优化策略来提高性能和并发性。

### redis和数据库一致性怎么保证(面试官补充了说用binlog)

> 

### Innodb默认隔离级别；

> InnoDB是MySQL数据库中的一种存储引擎，它具有事务支持和高并发性能。在InnoDB中，默认的隔离级别是"可重复读"（Repeatable Read）。
>
> "可重复读"隔离级别是MySQL的默认隔离级别，并且是最常用的隔离级别之一。在可重复读隔离级别下，事务在执行过程中看到的数据是一致的，即使其他事务对数据进行了修改，也不会被当前事务读取到。
>
> 具体来说，当一个事务开始时，InnoDB会创建一个一致性读视图，这个视图会记录当前事务开始时数据库中的所有数据。然后，在事务执行期间，所有的读操作都会使用这个一致性读视图，保证事务读取的数据是一致的。
>
> 需要注意的是，"可重复读"

### 可重复读和串行化的区别；

> "可重复读"（Repeatable Read）和"串行化"（Serializable）是MySQL数据库中的两种不同的隔离级别，它们在并发控制和数据一致性方面有一些区别。
>
> 1. **可重复读（Repeatable Read）**：
> 在可重复读隔离级别下，事务在执行期间会看到一致性的快照数据，即使其他事务对数据进行了修改，当前事务读取的数据也不会受到其他事务的影响。可重复读隔离级别通过使用一致性读视图来实现这一点。
>
> 但是，需要注意的是，在可重复读隔离级别下，其他事务对数据的修改操作（如插入、更新、删除）会以行级锁的方式进行，以保证并发事务之间的数据一致性。因此，在高并发场景下，可能会产生较多的锁冲突，从而影响并发性能。
>
> 2. **串行化（Serializable）**：
> 串行化隔离级别是最严格的隔离级别，它确保每个事务的执行都像是在一个完全串行的环境中执行的。在串行化隔离级别下，所有事务的读写操作都是相互串行执行的，不会出现并发冲突。
>
> 串行化隔离级别通过强制使用行级锁和表级锁来实现数据的完全隔离，以避免并发事务之间的任何冲突。这样可以确保数据的完全一致性，但同时也会牺牲并发性能，因为事务需要等待其他事务释放锁才能继续执行。
>
> 总结来说，可重复读隔离级别提供了较好的并发性能和一致性保证，适用于大多数应用场景。而串行化隔离级别则提供了最高的数据一致性，但会牺牲并发性能，适用于对数据一致性要求非常高的特定场景。选择合适的隔离级别需要根据具体的业务需求和性能要求进行权衡。

### 介绍一下MVCC;

> MVCC（Multi-Version Concurrency Control）是一种并发控制机制，用于在数据库系统中实现并发事务的隔离性。MVCC通过在数据库中保存不同版本的数据来实现并发事务的读写操作的隔离，从而提供了高并发性能和较好的事务隔离级别。
>
> 在MVCC机制中，每个事务在开始时会创建一个独立的快照（Snapshot），这个快照包含了当前事务开始时数据库中的数据版本。当事务读取数据时，它只能看到在它开始之前已经存在的数据版本，而不会看到其他并发事务中未提交的修改。
>
> MVCC的实现通常依赖以下两个关键组件：
>
> 1. **版本号（Version Number）**：每个数据行都会有一个版本号，用于标识该数据行的版本。在每次更新操作（如插入、更新、删除）时，数据库会为该数据行创建一个新的版本，并将版本号递增。
>
> 2. **回滚段（Undo Log）**：当事务更新数据时，数据库会将旧版本的数据保存在回滚段中，以便在事务回滚或其他事务需要读取旧版本数据时使用。回滚段中的数据可以用于构建事务的一致性读取视图，并且在事务提交后可以被清理。
>
> 使用MVCC机制，读操作不会对写操作产生阻塞，因为每个事务都有自己的快照。当一个事务更新数据时，它只会修改自己的快照中的数据版本，并不会影响其他事务的读操作。这样可以提高并发性能，并减少锁冲突。
>
> 需要注意的是，MVCC并非绝对的隔离性，因为某些操作（如范围查询）可能会读取到其他事务未提交的数据。因此，在某些隔离级别下，仍然需要使用锁机制来提供更严格的隔离性。此外，MVCC机制还可以结合索引技术来提高查询性能，例如通过使用多版本的B+树索引。
>
> MVCC是许多数据库系统（如MySQL的InnoDB存储引擎）中常用的并发控制机制，它在提供高并发性能和较好的事务隔离级别方面发挥着重要作用。

### undo log、redo log、binlog的作用；

> 在数据库中，Undo Log、Redo Log和Binlog是三种不同的日志文件，它们在数据库的恢复、故障恢复、数据一致性和复制等方面发挥着重要作用。
>
> 1. **Undo Log（回滚日志）**：
>    - 作用：Undo Log用于实现事务的回滚操作。在事务执行期间，Undo Log会记录事务对数据库进行的修改操作，包括旧值和新值。当事务需要回滚时，可以通过Undo Log将数据恢复到事务开始之前的状态。
>    - 实现方式：Undo Log通过将事务的修改操作反向执行，将数据恢复到事务开始之前的状态。它通常以日志的形式存储在磁盘上，确保即使在数据库故障的情况下，也能够进行数据恢复。
>
> 2. **Redo Log（重做日志）**：
>    - 作用：Redo Log用于实现事务的持久性和故障恢复。它记录了事务对数据库所做的修改操作，以确保这些修改操作能够在数据库恢复时重新执行，从而将数据恢复到事务提交后的状态。
>    - 实现方式：Redo Log以日志的形式记录在磁盘上，它首先将事务的修改操作顺序写入日志文件，然后在后台异步地将这些修改操作应用到数据库中。这样即使在数据库崩溃的情况下，也能通过Redo Log重新执行修改操作，确保数据的持久性和一致性。
>
> 3. **Binlog（二进制日志）**：
>    - 作用：Binlog用于实现数据库的备份、数据恢复、数据同步和数据库复制等功能。它记录了数据库中所有的修改操作，包括数据的插入、更新和删除等，以便在需要时能够对数据库进行恢复或者将这些修改操作应用到其他数据库实例上。
>    - 实现方式：Binlog以二进制的形式记录在磁盘上，它包含了数据库中的每个修改操作的详细信息，以及相应的元数据。通过解析和处理Binlog，可以实现数据的备份和恢复，以及数据库的复制和同步等功能。
>
> 总结来说，Undo Log用于实现事务的回滚操作，Redo Log用于实现事务的持久性和故障恢复，Binlog用于实现数据库的备份、数据恢复和数据库复制等功能。这三种日志文件在数据库系统中密切配合工作，确保数据的一致性、持久性和高可用性。

### 索引失效的场景(少说了隐式转换）

> 索引在数据库中起到加快查询速度和提高数据检索效率的作用。然而，有些情况下索引可能会失效，导致查询性能下降。以下是一些常见的导致索引失效的场景：
>
> 1. **使用函数或表达式对索引列进行操作**：
>    - 当查询条件中对索引列进行函数操作（如使用函数进行计算、字符串处理等）时，索引可能无法被有效使用，导致索引失效。
>    - 例如，如果索引列是"WHERE YEAR(date_column) = 2021"，则索引可能无法被使用，因为需要对每一行进行函数计算才能匹配条件。
>
> 2. **使用不等于操作符（!= 或 <>）**：
>    - 不等于操作符通常无法有效使用索引，因为它需要对索引列进行全表扫描来找到不匹配的值。
>    - 如果查询条件中使用了不等于操作符，例如"WHERE column != value"，则索引可能会失效。
>
> 3. **模糊匹配和通配符开头的模式匹配**：
>    - 当查询条件中使用模糊匹配（如LIKE操作符）并且通配符在开头（如'%'开头）时，索引可能无法被使用。
>    - 索引可以有效支持通配符在结尾或中间的模糊匹配，但在开头的情况下，需要对索引列进行全表扫描。
>
> 4. **数据列类型不匹配**：
>    - 当查询条件中的数据类型与索引列的数据类型不匹配时，索引可能会失效。
>    - 例如，如果索引列是整数类型，但查询条件中使用了字符串类型进行匹配，索引可能无法被使用。
>
> 5. **索引列上的类型转换**：
>    - 当在查询条件中对索引列进行类型转换时，索引可能会失效。
>    - 例如，如果索引列是字符串类型，但在查询条件中将其转换为数字类型，索引可能无法被使用。
>
> 6. **低选择性的索引列**：
>    - 如果索引列的选择性（不同值的数量与总行数的比例）非常低，索引可能会失效。
>    - 当查询条件使用这样的索引列时，数据库可能会选择进行全表扫描而不是使用索引。
>
> 除了上述情况外，还有其他一些因素可能导致索引失效，如表的统计信息不准确、索引列上存在NULL值等。在设计和优化数据库查询时，需要考虑这些因素，以确保索引的有效使用，提高查询性能。

### mysql索引

### 索引自由发挥

### a b c 联合索引，a = 1 and b › 1 and c = 1 走哪几个索引，为什么

> 假设有一个名为 "abc_index" 的联合索引包含列 a、b 和 c。对于查询条件 "a = 1 and b > 1 and c = 1"，该查询可以利用联合索引进行优化。
>
> 在这种情况下，该查询将首先使用 a 列的索引进行匹配，过滤出 a 值等于 1 的行。然后，对于满足条件的行，将使用索引中的 b 列进行进一步的筛选，找出 b 值大于 1 的行。最后，对于满足前两个条件的行，使用索引中的 c 列进行最终的筛选，找出 c 值等于 1 的行。
>
> 因此，该查询将依次使用索引的 a、b 和 c 列，以加快查询速度和提高数据检索效率。这种联合索引的使用方式可以帮助数据库系统快速定位满足所有条件的数据行，而无需对整个表进行完全扫描。
>
> 需要注意的是，索引的选择和使用可能还受到数据库系统的优化器和查询执行计划的影响。优化器可能会根据索引的统计信息、数据分布和查询成本等因素来决定最佳的查询执行计划。因此，在实际应用中，除了索引的定义，还需要关注数据库配置和统计信息的更新，以确保查询的最佳执行计划。

### Redis和mysql数据一致性，为什么不更新完数据库后直接更新缓存。

> 对于数据一致性的考虑，更新完数据库后直接更新缓存可能会引发以下问题：
>
> 1. **并发写操作的竞争条件**：在高并发环境下，多个客户端同时更新数据库和缓存可能导致竞争条件。如果多个写操作同时更新缓存，可能会导致数据不一致的情况，因为最后一次写入的值可能会覆盖之前的写入。
>
> 2. **数据库更新失败的情况**：如果数据库更新失败（如由于网络问题、死锁等原因），而缓存已经被更新，那么数据库和缓存之间就会出现数据不一致的情况。
>
> 3. **缓存更新失败的情况**：如果在更新数据库后立即更新缓存时，缓存更新失败（如网络问题、缓存服务宕机等），那么数据库和缓存之间就会出现数据不一致的情况。
>
> 为了解决这些问题，常见的做法是采用缓存失效和延迟双写等策略来保持Redis和MySQL之间的数据一致性。
>
> 1. **缓存失效策略**：在更新数据库时，可以将缓存中对应的数据标记为失效，下次读取时从数据库中获取最新的数据并更新缓存。这样可以确保缓存中的数据在一定时间后与数据库保持一致。
>
> 2. **延迟双写策略**：在数据库更新后，将缓存更新操作延迟到后台异步任务或消息队列中执行。这样可以降低直接更新缓存带来的性能开销，并保证数据库和缓存的一致性。
>
> 通过采用这些策略，可以更好地处理并发写操作和异常情况，保证Redis和MySQL之间的数据一致性。需要根据具体的业务需求和系统性能要求来选择合适的策略。

### MySQL优化方法（发散思维，面试官说想到什么说什么，看你具体都了解哪些方面。我只回答了索引相关的，有没有人补充一下？）

> 优化MySQL数据库性能是提升应用程序效率和响应时间的关键方面。以下是一些常见的MySQL优化方法：
>
> 1. **索引优化**：通过正确设计和使用索引，可以加快数据检索速度。确保关键查询的列上有适当的索引，并避免过多或不必要的索引。使用`EXPLAIN`语句来分析查询执行计划，以确定是否使用了合适的索引。
>
> 2. **查询优化**：编写高效的查询语句是提高性能的关键。避免使用不必要的连接、子查询和全表扫描。优化复杂查询，考虑使用JOIN、UNION、GROUP BY和HAVING等语句来减少数据集的大小。
>
> 3. **适当的数据类型**：使用适当的数据类型可以减少存储空间和提高查询性能。选择合适的整数类型、字符类型和日期/时间类型，以确保最小存储需求和最大查询效率。
>
> 4. **适当的配置**：根据应用程序的需求和服务器的硬件配置，调整MySQL的配置参数。这包括调整缓冲区大小、连接数、并发性、日志设置等。使用MySQL的配置工具或编辑my.cnf文件进行配置。
>
> 5. **分区和分表**：对于大型表，可以考虑对其进行分区或分表，以减少单个表的数据量和查询开销。分区可以根据某个列的范围或哈希值进行，而分表可以根据特定的规则将数据分散到多个表中。
>
> 6. **缓存优化**：使用适当的缓存机制，如使用Redis或Memcached来缓存查询结果、页面片段或常用数据。这可以减少对数据库的访问次数，提高响应时间和吞吐量。
>
> 7. **定期维护和优化**：定期进行数据库维护操作，如数据备份、索引重建、表优化和统计信息更新等。这可以清理无用数据、恢复数据库性能，并确保数据库处于良好的状态。
>
> 8. **垂直和水平扩展**：根据业务需求，考虑垂直扩展（升级硬件、增加内存等）和水平扩展（使用主从复制、分布式架构等）来提高数据库的处理能力和可扩展性。
>
> 以上是一些常见的MySQL优化方法，但具体的优化策略需要根据应用程序的需求、数据量和访问模式等因素来确定。使用性能监控工具来识别瓶颈并进行性能优化测试也是一个好的实践。

### 雪花算法，缺点

> 雪花算法（Snowflake Algorithm）是一种用于生成唯一ID的分布式算法。它由Twitter开发，用于在分布式系统中生成全局唯一的ID。雪花算法的ID由以下三部分组成：
>
> 1. **时间戳**：使用毫秒级的时间戳来保证生成的ID是递增的，可以通过时间戳来获取生成ID的时间信息。
>
> 2. **机器ID**：用于标识不同的机器。在分布式环境中，每台机器都会被分配一个唯一的机器ID，以避免生成重复的ID。
>
> 3. **序列号**：在同一毫秒内，使用序列号来区分不同的ID。序列号可以保证在同一台机器上生成的ID是唯一递增的。
>
> 使用这三个部分的组合，雪花算法可以生成一个64位的唯一ID，具有足够的容量来支持大规模分布式系统中的ID生成需求。
>
> 然而，雪花算法也存在一些缺点：
>
> 1. **依赖于机器时钟**：雪花算法的正确性依赖于机器时钟的准确性。如果机器时钟发生回拨或不同步，可能会导致生成的ID不唯一或不按预期顺序递增。
>
> 2. **单点故障**：由于雪花算法中的机器ID是固定的，如果某台机器发生故障或无法使用，那么该机器ID对应的ID生成就会中断，可能导致ID重复或不连续。
>
> 3. **扩展性限制**：雪花算法在设计时为64位ID留下了一些位数的空间，用于支持扩展，如数据中心ID或工作节点ID。然而，这也限制了雪花算法在大规模系统中的扩展性。当需要支持更多的数据中心或机器时，可能需要重新设计ID结构或选择其他算法。
>
> 4. **全局唯一性无法保证**：由于雪花算法中的机器ID是分配的，并没有全局唯一性的保证。如果不同的分布式系统使用相同的机器ID范围，可能会导致生成的ID冲突。
>
> 因此，在使用雪花算法时，需要仔细考虑时钟同步、机器ID分配和扩展性等问题，以确保生成的ID在分布式系统中的唯一性和正确性。在一些特殊情况下，可能需要结合其他手段来解决这些问题，如使用全局唯一ID生成服务或其他分布式ID生成算法。

### 数据库的表级锁，表锁什么时候会出现

> 数据库的表级锁是指对整个表进行加锁，以控制对表的并发访问。当一个事务获取了表级锁后，其他事务需要等待锁释放才能访问该表。
>
> 表级锁会在以下情况下出现：
>
> 1. **写操作**：当一个事务对表进行写操作（如插入、更新、删除）时，数据库系统通常会自动为该表加上表级写锁。这样可以确保在该事务执行期间，其他事务无法修改该表的数据，保证数据的一致性和完整性。
>
> 2. **显式锁定**：开发人员可以显式地使用锁定语句（如`LOCK TABLES`）对特定表进行锁定操作。这种情况下，表级锁会被直接应用于被锁定的表，直到锁被显式释放。
>
> 3. **DDL操作**：某些数据库系统在执行某些数据定义语言（DDL）操作时，会对相关的表进行表级锁定。例如，当执行`ALTER TABLE`、`DROP TABLE`等DDL语句时，数据库会对相关表进行锁定，以防止其他事务对表结构进行修改。
>
> 表级锁相比于行级锁或其他粒度的锁有一些特点和限制：
>
> 1. **并发性低**：表级锁会限制对整个表的并发访问，可能会导致并发性下降，降低系统的性能和吞吐量。
>
> 2. **锁冲突概率高**：由于表级锁的粒度较大，如果多个事务同时访问同一个表，可能会导致锁冲突的概率增加，从而造成较多的等待和阻塞。
>
> 3. **锁粒度大**：表级锁是最粗粒度的锁，意味着在锁定表期间，其他事务无法对表中的任何行进行读或写操作。
>
> 4. **锁定范围广**：当一个事务获取了对某个表的表级锁后，该事务可以对表进行任意修改操作，而不仅仅限于特定的行或数据。
>
> 因此，在设计数据库系统时，需要根据具体的业务需求和并发访问模式，合理选择锁的粒度，避免过度使用表级锁，以提高并发性和系统性能。

### 可重复读的表级锁

> 在MySQL数据库中，可重复读（Repeatable Read）隔离级别下，会使用表级锁来实现事务的隔离性和一致性。可重复读是MySQL默认的隔离级别。
>
> 在可重复读隔离级别下，当一个事务开始时，它会为所有查询涉及的表自动获取一个读锁，并且这个锁会在整个事务期间保持不变。这意味着在同一个事务中，其他事务无法对这些表进行写操作，从而保证了数据的一致性和可重复读性。
>
> 表级锁在可重复读隔离级别中的应用有以下特点：
>
> 1. **读操作的共享锁**：当一个事务执行读操作时，会为涉及的表获取共享锁（也称为读锁）。共享锁允许多个事务同时获取并持有该锁，这样可以实现读操作的并发性。
>
> 2. **写操作的排他锁**：当一个事务执行写操作（如插入、更新、删除）时，会为涉及的表获取排他锁（也称为写锁）。排他锁会阻止其他事务对表进行读或写操作，确保数据的一致性。
>
> 3. **锁的自动释放**：在可重复读隔离级别下，表级锁会在事务提交或回滚时自动释放。这样可以确保其他事务在当前事务完成之前无法对表进行写操作。
>
> 需要注意的是，表级锁在可重复读隔离级别下的应用可能会导致以下问题：
>
> 1. **并发性降低**：由于表级锁的粒度较大，一个事务在锁定表期间，其他事务无法并发地对表进行读写操作，可能导致并发性降低。
>
> 2. **锁冲突概率增加**：由于表级锁的应用范围广，多个事务同时访问同一个表时，锁冲突的概率会增加，可能造成较多的等待和阻塞。
>
> 因此，在使用可重复读隔离级别时，需要权衡并发性和数据一致性之间的关系，确保合理地使用表级锁，以满足业务需求和性能要求。在高并发场景下，可以考虑使用更细粒度的锁（如行级锁）来减少锁冲突和提高并发性。

### 并发操作数据库需要注意什么

> 在并发操作数据库时，需要注意以下几个方面：
>
> 1. **事务隔离级别**：选择适当的事务隔离级别能够平衡并发性和数据一致性的需求。不同的隔离级别对并发操作的影响不同，例如可重复读隔离级别可以避免脏读和不可重复读，但会增加锁冲突和降低并发性。因此，根据具体业务需求选择合适的隔离级别。
>
> 2. **锁的粒度**：选择适当的锁粒度是提高并发性能的关键。过大的锁粒度（如表级锁）会导致锁冲突概率增加，并发性降低；而过小的锁粒度（如行级锁）可能导致锁开销增加。需要根据具体业务场景和数据访问模式，合理选择锁粒度。
>
> 3. **锁冲突和死锁**：并发操作时，锁冲突是常见的问题。锁冲突会导致事务等待和阻塞，降低并发性能。设计良好的数据库模型、合理的索引和查询优化可以减少锁冲突的概率。此外，需要注意避免死锁的发生，通过良好的事务管理和锁顺序规定来预防死锁。
>
> 4. **并发控制机制**：数据库提供了多种并发控制机制，如事务、锁、MVCC（多版本并发控制）等。熟悉并了解这些机制，选择适合的机制来确保数据的一致性和并发性。
>
> 5. **索引设计**：良好的索引设计可以提高并发操作的性能。根据查询的访问模式和频率，选择合适的索引策略。过多或不恰当的索引可能导致锁冲突增加，降低并发性能。
>
> 6. **连接池管理**：在并发操作中，数据库连接的管理也是重要的一环。使用连接池可以减少连接的创建和销毁开销，提高连接的复用率和并发性能。
>
> 7. **性能监控和调优**：在并发操作中，及时监控数据库的性能指标，如锁等待时间、响应时间、并发连接数等，可以帮助发现潜在的性能瓶颈和问题，并进行调优和优化。
>
> 总之，在并发操作数据库时，需要综合考虑事务隔离级别、锁粒度、锁冲突和死锁、并发控制机制、索引设计、连接池管理以及性能监控和调优等方面的因素，以提高数据库的并发性能和数据一致性。

### 有没有处理过慢查询，怎么排查索引失效

> 要排查索引失效导致的慢查询问题，可以按照以下步骤进行：
>
> 1. **确认慢查询**：首先，确定哪些查询被认为是慢查询，可以通过数据库的性能监控或日志来获取慢查询的相关信息，如执行时间、扫描行数等。
>
> 2. **执行计划分析**：对于慢查询，通过查看其执行计划可以了解查询的执行方式和涉及的表，以及是否使用了合适的索引。执行计划可以通过数据库的查询优化器或相关命令来获取。
>
> 3. **检查索引使用情况**：检查执行计划中的索引使用情况，确认是否使用了合适的索引。注意以下几个方面：
>    - 索引是否被正确选择和使用。
>    - 是否存在索引失效或无效的情况。
>    - 是否存在全表扫描或索引范围扫描等低效的访问方式。
>
> 4. **索引统计信息**：确保索引的统计信息是最新的，因为过期或不准确的统计信息可能导致优化器做出错误的执行计划选择。可以使用数据库提供的命令或工具，如ANALYZE TABLE或统计信息收集工具来更新索引统计信息。
>
> 5. **索引设计审查**：审查索引设计是否满足查询的需求。确认是否存在缺失的索引或不合理的索引设计。考虑调整或添加索引以更好地支持查询。
>
> 6. **查询重写**：有时，通过重写查询语句的结构或逻辑，可以改善索引的使用情况。尝试优化查询语句，使用更有效的方式进行查询，以便更好地利用索引。
>
> 7. **性能测试和监控**：对于进行了索引调整或查询重写的查询，进行性能测试和监控，以确认是否改善了查询性能。观察执行计划、响应时间和资源利用情况等指标，进行评估和优化迭代。
>
> 需要注意的是，索引失效可能有多种原因，如数据分布不均、索引列数据类型不匹配、查询条件不准确等。因此，排查索引失效问题可能需要综合使用上述方法，并结合具体的业务场景和数据库特性来进行分析和调整。

### MySQL 里面，然后事物的隔离级别都有哪些？

> 在MySQL中，存在以下四个事务隔离级别：
>
> 1. **读未提交（Read Uncommitted）**：最低级别的隔离级别，事务可以读取其他事务尚未提交的数据，可能导致脏读、不可重复读和幻读的问题。
>
> 2. **读已提交（Read Committed）**：事务只能读取其他已提交的事务所做的修改，能够避免脏读，但仍可能出现不可重复读和幻读的问题。
>
> 3. **可重复读（Repeatable Read）**：MySQL默认的隔离级别。事务开始时创建一个一致性视图，事务期间读取的数据都来自于这个一致性视图，避免了脏读和不可重复读，但仍可能出现幻读。
>
> 4. **串行化（Serializable）**：最高级别的隔离级别，事务串行执行，确保了最高的数据一致性。通过强制所有事务依次执行，避免了脏读、不可重复读和幻读的问题。然而，串行化隔离级别的性能较低，因为它会限制并发性。
>
> 可以使用以下语句来设置事务的隔离级别：
>
> ```sql
> SET TRANSACTION ISOLATION LEVEL <隔离级别>;
> ```
>
> 其中，`<隔离级别>`可以是`READ UNCOMMITTED`、`READ COMMITTED`、`REPEATABLE READ`或`SERIALIZABLE`。
>
> 需要注意的是，不同的数据库管理系统可能具有不同的隔离级别名称和行为。上述隔离级别是MySQL的实现方式，其他数据库系统可能存在略微不同的命名和行为。

### 提交读和可重复读有什么区别？

> 在MySQL中，提交读（Read Committed）和可重复读（Repeatable Read）是两个事务隔离级别，它们之间有以下区别：
>
> 1. **读取数据的一致性**：提交读级别下，每个SQL语句在执行时都会读取其他已提交事务所做的修改，即读取的是最新提交的数据。而可重复读级别下，事务开始时创建一个一致性视图，事务期间读取的数据都来自于这个一致性视图，不受其他事务的修改影响。
>
> 2. **幻读问题**：幻读指在同一个事务中，两次相同的查询语句返回不同的结果行数。在提交读级别下，由于其他事务可能在事务执行过程中插入或删除数据，因此可能出现幻读问题。而在可重复读级别下，一致性视图锁定了查询范围内的数据，确保了数据的一致性，避免了幻读问题。
>
> 3. **锁定范围**：在提交读级别下，MySQL使用瞬时共享锁（S锁）来保护读取的数据，这种锁只在读取过程中存在，读取完成后立即释放。而在可重复读级别下，MySQL使用共享锁（S锁）锁定查询范围内的数据，直到事务提交或回滚才释放锁。
>
> 4. **并发性能**：可重复读级别下的锁定范围更广，因此可能导致更多的锁冲突，对并发性能有一定的影响。而提交读级别下锁定的范围较小，可以更好地支持并发操作。
>
> 需要注意的是，提交读级别和可重复读级别的选择应根据具体的业务需求和数据一致性要求来决定。如果需要更高的数据一致性，并且能够容忍一定的锁冲突和并发性能损失，可重复读级别是一个较好的选择。如果对数据一致性要求相对较低，并且追求更好的并发性能，提交读级别可能更合适。

### MySQL如何实现事务的

> MySQL实现事务的方式主要涉及以下几个方面：
>
> 1. **事务的隔离级别**：MySQL支持多个事务隔离级别，如读未提交（Read Uncommitted）、读已提交（Read Committed）、可重复读（Repeatable Read）和串行化（Serializable）。可以使用`SET TRANSACTION ISOLATION LEVEL`语句来设置事务的隔离级别。
>
> 2. **事务的开启和提交**：可以使用`START TRANSACTION`、`BEGIN`或`SET autocommit=0`语句来显式地开启一个事务。事务中的SQL语句执行后，可以通过`COMMIT`语句来提交事务，将事务中的修改永久保存到数据库。
>
> 3. **事务的回滚**：可以使用`ROLLBACK`语句来回滚事务，将事务中的修改撤销，恢复到事务开始前的状态。
>
> 4. **事务的管理**：MySQL提供了事务的保存点（Savepoint）机制，可以在事务中设置保存点，通过`SAVEPOINT`和`ROLLBACK TO SAVEPOINT`语句来实现在事务内的部分回滚。
>
> 5. **事务的自动提交**：在默认的自动提交模式下（即`autocommit`参数设置为1），每条SQL语句都被视为一个单独的事务，执行后立即提交。可以通过设置`autocommit`参数为0来将自动提交模式关闭，从而将多个语句组织在一个事务中。
>
> 6. **事务的并发控制**：MySQL使用锁机制来实现事务的并发控制，包括行级锁和表级锁。常用的存储引擎InnoDB支持行级锁，可以提高并发性能和事务隔离性。
>
> 需要注意的是，事务的使用需要满足以下条件：数据库引擎必须支持事务，表的存储引擎必须是支持事务的引擎，如InnoDB。此外，事务的正确使用还需要考虑并发访问的情况，避免死锁等并发问题。

### redolog和binlgo区别，如何保证它们的数据一致

> Redo log（重做日志）和Binlog（二进制日志）是MySQL中用于数据恢复和复制的两种日志文件，它们有以下区别：
>
> 1. **功能和用途不同**：
>    - Redo log是InnoDB存储引擎特有的日志，用于崩溃恢复和事务的持久性。它记录了每个事务对数据库所做的修改操作，包括插入、更新和删除等，并在事务提交之前将这些修改写入磁盘上的重做日志文件。当数据库发生崩溃或重新启动时，通过重做日志可以将未持久化的修改重新应用到数据库，以保证数据的一致性。
>    - Binlog是MySQL服务器层产生的日志，用于实现数据复制和恢复。它记录了所有对数据库的更改操作，包括数据的增删改以及数据库结构的变更，以二进制的形式保存在磁盘上。Binlog主要用于实现主从复制、数据备份和恢复以及数据审计等功能。
>
> 2. **格式和存储位置不同**：
>    - Redo log以循环写入的方式组织，通常有多个预设的固定大小的日志文件组成。它以顺序追加的方式记录日志，写入磁盘的速度非常快。
>    - Binlog以追加写入的方式组织，也可以有多个日志文件。Binlog文件是逻辑日志，记录了对数据库的逻辑操作，以SQL语句的形式保存。Binlog的写入速度相对较慢，因为它需要将SQL语句写入磁盘。
>
> 3. **数据一致性保证**：
>    - Redo log保证了事务的持久性和数据的一致性。在事务提交之前，相关的修改操作被写入重做日志，即使在发生崩溃时，数据库可以通过重做日志将未持久化的修改恢复到磁盘，保证数据的一致性。
>    - Binlog用于复制和恢复，它记录了所有对数据库的更改操作。通过读取Binlog，可以将这些操作应用到其他MySQL实例上，实现主从复制，从而保持数据一致性。
>
> 为了保证Redo log和Binlog的数据一致性，MySQL采用了以下机制：
>
> 1. **事务的持久性**：在事务提交之前，相关的修改操作必须被写入Redo log，确保事务的持久性。只有当Redo log中的日志已经成功写入磁盘，事务才被认为是提交的。
>
> 2. **Redo log与数据文件的同步**：在事务提交时，MySQL使用一种称为"write-ahead logging"（先写日志，再写磁盘）的机制。这意味着在将修改操作应用到数据文件之前，相关的修改操作必须先写入Redo log，以确保数据文件和Redo log的一致性。
>
> 3. **Binlog的同步和复制**：在主从复制中，MySQL确保主服务器上的Binlog已经写入磁盘并同步到从服务器，从服务器才被认为是数据一致的。主服务器将Binlog写入磁盘后，通知从服务器进行复制操作，从服务器读取Binlog并将其中的操作应用到本地数据库，从而保持主从数据的一致性。
>
> 综上所述，通过Redo log和Binlog的机制，MySQL可以保证数据在崩溃恢复和复制过程中的一致性。

### 为什么要加索引？什么情况加？

> 加索引的主要目的是提高数据库的查询性能和响应时间。索引可以加快数据的检索速度，减少数据访问量和磁盘IO，提供有序访问和查询优化的选择空间。以下是一些情况下适合加索引的场景：
>
> 1. **频繁的查询操作**：如果某个表经常被查询，特别是对其中的某个列进行查询或根据特定条件过滤数据，那么在该列上创建索引可以显著提高查询性能。常见的查询操作包括等值查询、范围查询和排序操作。
>
> 2. **连接操作**：当多个表进行连接操作时，加上连接列的索引可以加速连接过程，减少连接操作的时间。连接操作涉及到多个表之间的匹配和数据组合，索引可以提供更快的数据定位和访问。
>
> 3. **排序和分组操作**：如果需要对某个列进行排序或分组操作，创建索引可以大大减少排序或分组的时间。索引可以按照指定的列的顺序进行有序访问，从而避免使用临时表或排序算法，提高排序和分组的效率。
>
> 4. **频繁的更新操作**：虽然索引会对插入、更新和删除等操作带来一定的开销，但在某些情况下仍然值得加索引。如果表的读取远远多于写入，并且更新操作的开销可以接受，那么在适当的列上加索引可以大大提高读取操作的性能。
>
> 需要注意的是，不是所有的列都适合创建索引。通常情况下，对于经常进行查询、过滤、排序和连接操作的列，以及作为主键或外键的列，加索引的效果最好。另外，索引的创建和维护也会占用存储空间和一定的开销，因此需要权衡查询性能和更新性能。在设计索引时，需要根据具体的应用需求和数据库访问模式进行评估和测试，选择合适的列进行索引。

### 加索引为什么快？

> 索引在数据库中起到了加快数据检索速度的重要作用，主要有以下几个原因：
>
> 1. **减少数据访问量**：索引是根据特定的列或字段值创建的数据结构，它存储了某种数据的引用或指针。通过使用索引，数据库可以直接定位到符合条件的数据行，而不需要扫描整个表。这样可以大大减少数据访问的数量，提高查询效率。
>
> 2. **提供数据的有序访问**：索引通常是按照键值的顺序排列的，这使得数据库可以利用索引的有序性进行顺序访问。有序访问对于范围查询、排序操作以及连接操作等都非常高效，因为它可以将相关的数据行紧凑地存储在一起，减少磁盘IO和CPU的消耗。
>
> 3. **减少磁盘IO**：索引结构通常比数据表的存储结构更小，因为它只存储了索引键和指向数据行的引用。当数据库需要从磁盘加载数据时，索引可以减少需要读取的磁盘块数量，从而减少磁盘IO的开销。
>
> 4. **提供查询优化器的选择空间**：索引提供了多个查询路径供查询优化器选择，使得优化器可以根据查询的条件和索引的统计信息选择最优的查询计划。通过选择合适的索引路径，可以避免全表扫描和临时表的生成，提高查询性能。
>
> 需要注意的是，索引的创建和维护也会带来一定的开销，包括插入、更新和删除数据时的索引更新操作。因此，在设计索引时需要权衡查询性能和更新性能，并避免创建过多或不必要的索引。正确地使用索引可以显著提高数据库的查询性能和响应时间。

### innodb底层B+树结构

> InnoDB存储引擎的底层索引结构是一个多层的B+树。具体来说，InnoDB使用了以下几个层次的B+树结构：
>
> 1. **聚簇索引（Clustered Index）**：InnoDB的主键索引就是一个特殊的B+树，也被称为聚簇索引。聚簇索引决定了表中数据的物理存储顺序。如果表没有显式定义主键，则会使用一个唯一的隐藏列作为聚簇索引。聚簇索引的叶子节点存储了完整的数据行，而非叶子节点存储了指向数据页的指针。
>
> 2. **辅助索引（Secondary Index）**：除了聚簇索引外，InnoDB还支持创建多个辅助索引，也是基于B+树实现的。辅助索引的叶子节点存储了索引键和指向聚簇索引中对应数据行的物理指针。非叶子节点存储了索引键和指向下一级节点的指针，可以通过B+树的搜索过程快速定位到目标数据。
>
> 3. **自适应哈希索引（Adaptive Hash Index）**：InnoDB还引入了自适应哈希索引，它是一个内存中的数据结构，用于加速热点数据的访问。自适应哈希索引并不是B+树结构，而是使用哈希表实现。它存储了最频繁访问的索引键和对应数据行的指针。
>
> 总结起来，InnoDB的底层索引结构主要包括聚簇索引（主键索引）和辅助索引（Secondary Index），它们都是基于B+树实现的。聚簇索引决定了数据的物理存储顺序，辅助索引加速了对数据的检索。另外，InnoDB还使用自适应哈希索引来优化热点数据的访问。这些索引结构的组合使得InnoDB能够提供高效的数据查询和存储性能。

### B+树叶子节点和非叶子节点存储的分别是什么？

> 在B+树中，叶子节点和非叶子节点存储的内容有所区别。
>
> 1. **叶子节点**：叶子节点存储实际的数据项（或键值对）。每个叶子节点包含一个键和对应的值。在数据库中，通常将叶子节点的键作为索引的一部分，而值则是对应的数据记录的指针或存储位置。叶子节点按照键的顺序进行排序，形成一个有序链表。这使得范围查询和顺序访问非常高效。
>
> 2. **非叶子节点**：非叶子节点存储的是索引信息，用于指导搜索过程。它们包含一组键，并且每个键都指向下一级节点（可以是叶子节点或其他非叶子节点）。非叶子节点按照键的顺序进行排序，使得查找过程可以通过二分搜索快速定位到目标节点。非叶子节点的数量通常比叶子节点的数量少得多，因为它们的作用是提供索引结构而不存储实际的数据。
>
> 总结起来，B+树的叶子节点存储实际的数据项，而非叶子节点存储索引信息。这种分层结构使得B+树能够高效地支持范围查询、顺序访问和快速定位。叶子节点形成有序链表，而非叶子节点提供索引结构以加速搜索过程。

### 为什么innodb用B+树而不用B树（为什么B+树快）？

> InnoDB存储引擎选择使用B+树而不是B树作为其索引结构，是为了提供更好的性能和支持更多的功能。以下是一些原因：
>
> 1. **磁盘访问优化**：B+树的叶子节点形成了一个有序链表，可以通过顺序IO进行高效的范围查询和顺序访问。相比之下，B树的所有节点都存储了键和值，范围查询时可能需要进行随机IO，效率较低。
>
> 2. **减少IO次数**：B+树的内部节点只存储索引信息，而不存储实际的数据，使得每个节点可以存储更多的键和指针。这样可以减少B+树的层数，从而减少磁盘IO的次数，提高查询性能。
>
> 3. **更好的缓存利用**：由于B+树的内部节点和叶子节点有明确的分离，叶子节点存储了完整的数据项，可以更好地利用缓存来提高查询的命中率。而B树的节点既存储了索引信息又存储了数据，导致缓存中存储的是冗余的数据，降低了缓存的效率。
>
> 4. **支持更多功能**：B+树的有序链表结构和范围查询特性使得它更适合支持索引的范围扫描、排序和分组等操作。此外，B+树的叶子节点形成了一个聚簇索引，可以提供更好的主键顺序访问性能。
>
> 综上所述，InnoDB选择使用B+树而不是B树作为索引结构，主要是为了提供更好的性能、减少IO次数、更好地利用缓存以及支持更多的功能。B+树的有序链表结构和范围查询特性使得它成为大部分关系型数据库引擎的首选索引结构。

### 索引失效情况

### MYSQL存储引擎知道哪些？有什么区别？

> MySQL支持多种存储引擎（也称为表类型），每个存储引擎具有不同的特点和适用场景。以下是MySQL常见的存储引擎及其主要区别：
>
> 1. **InnoDB**：
>    - InnoDB是MySQL的默认存储引擎，也是最常用的存储引擎之一。
>    - 支持事务（ACID特性），具有较好的并发性能和数据完整性保护。
>    - 提供行级锁定和多版本并发控制（MVCC）机制，适用于高并发读写场景。
>    - 支持外键约束和崩溃恢复，具有较好的可靠性和数据一致性。
>    - 支持自动增长列和热备份等功能。
>
> 2. **MyISAM**：
>    - MyISAM是MySQL的另一个常见存储引擎。
>    - 不支持事务和行级锁定，适用于读操作较多、写操作较少的场景。
>    - 具有较高的插入和查询性能，适合用于数据仓库、日志记录等应用。
>    - 不支持外键约束和崩溃恢复，对数据完整性和可靠性的支持较弱。
>
> 3. **MEMORY**（也称为HEAP）：
>    - MEMORY存储引擎将数据存储在内存中，速度非常快，但数据在服务器重启时会丢失。
>    - 适用于临时表、缓存表和计算结果表等对速度要求较高、对数据持久性要求较低的场景。
>    - 不支持事务、行级锁定、外键约束和崩溃恢复。
>
> 4. **NDB Cluster**（也称为MySQL Cluster）：
>    - NDB Cluster存储引擎适用于分布式环境和高可用性要求的应用。
>    - 支持水平扩展和自动分区，可以将数据分布在多个节点上，实现数据的并行处理和高可用性。
>    - 具有较好的性能和容错能力，支持事务、行级锁定、外键约束和崩溃恢复。
>
> 除了以上几种存储引擎，MySQL还支持其他存储引擎如ARCHIVE、CSV、BLACKHOLE等，它们各自具有特定的用途和特点。选择合适的存储引擎需要考虑应用的读写比例、并发性能要求、事务支持、数据完整性和可靠性等因素。在MySQL的每个表创建时，可以通过指定存储引擎来选择要使用的存储引擎类型。

### 事务的一致性是如何实现的

> 事务的一致性是通过数据库管理系统（DBMS）的事务处理机制来实现的。下面是事务一致性的实现原理：
>
> 1. **原子性（Atomicity）**：原子性确保事务中的操作要么全部执行成功，要么全部回滚到事务开始之前的状态。如果事务中的任何操作失败，系统会自动回滚事务中的所有修改，以保持数据的一致性。
>
> 2. **一致性（Consistency）**：一致性要求事务的执行不会破坏数据库的完整性约束和业务规则。在事务开始和结束之间，数据库处于一致的状态。如果一个事务违反了完整性约束或业务规则，DBMS将回滚该事务并恢复到事务开始之前的状态。
>
> 3. **隔离性（Isolation）**：隔离性确保并发执行的事务之间互不干扰，每个事务都感知不到其他事务的存在。通过隔离级别（如读未提交、读已提交、可重复读和串行化）来控制事务之间的可见性和并发操作的影响。
>
> 4. **持久性（Durability）**：持久性确保一旦事务提交，其结果将永久保存在数据库中，即使在系统崩溃或重新启动后也是如此。DBMS使用日志（如重做日志）来记录事务的操作，以便在崩溃恢复时重新应用这些操作，以恢复到事务提交后的状态。
>
> 通过以上机制，DBMS实现了事务的一致性。事务的原子性和一致性是通过回滚机制和完整性约束的检查来保证的。事务的隔离性通过并发控制机制（如锁、多版本并发控制）来实现。事务的持久性通过日志和崩溃恢复机制来保证。
>
> 需要注意的是，事务的一致性不仅仅依赖于DBMS的机制，还取决于应用程序的正确实现和合理设计。应用程序需要在事务中进行适当的数据验证和处理，以确保业务规则的一致性和完整性。

### MVCC工作原理

> MVCC（Multi-Version Concurrency Control）是一种并发控制机制，用于在数据库系统中实现并发事务的隔离性。MVCC的工作原理如下：
>
> 1. **版本号**：每个数据行都有一个版本号，用于标识该数据行的历史版本。版本号通常是一个递增的时间戳或事务ID。
>
> 2. **读操作**：当一个事务开始执行读操作时，系统会为该事务创建一个独立的视图。这个视图基于事务开始时的数据库状态，并记录了每个数据行的版本号。
>
> 3. **写操作**：当一个事务开始执行写操作时，系统会为该事务创建一个新版本的数据行，并分配一个新的版本号。原始数据行的版本号保持不变，仍然可供其他事务读取。
>
> 4. **读取规则**：在MVCC中，读取操作的规则如下：
>    - 事务只能读取在其开始之前已经存在的数据行版本。
>    - 事务不能读取其他事务正在修改的数据行版本，以避免读取到未提交的数据。
>
> 5. **写入规则**：在MVCC中，写入操作的规则如下：
>    - 事务只能修改自己创建的数据行版本。
>    - 当一个事务提交时，它的修改会成为新的数据行版本，并更新对应的版本号。
>
> 通过使用版本号和读取规则，MVCC实现了并发事务的隔离性，避免了脏读（Dirty Read）、不可重复读（Non-repeatable Read）和幻读（Phantom Read）等问题。每个事务都有自己的视图，通过读取自己开始之前已经存在的数据行版本，保证了事务之间的隔离性。
>
> 需要注意的是，MVCC并非适用于所有存储引擎，它通常与支持多版本并发控制的存储引擎（如InnoDB）结合使用。不同的存储引擎可能在MVCC的实现细节上有所差异，但其核心思想是相似的。

### 分布式事务的解决方案

> 分布式事务是指跨多个独立的数据库或服务的事务操作。由于分布式环境的复杂性和网络延迟等因素，保证分布式事务的一致性和隔离性是一个挑战。以下是一些常见的解决方案：
>
> 1. **两阶段提交（Two-Phase Commit，2PC）**：2PC是一种经典的分布式事务协议，它通过协调者（Coordinator）和参与者（Participant）之间的消息交换来确保所有参与者要么都提交事务，要么都回滚事务。它的工作原理包括准备阶段和提交阶段，但2PC存在锁定资源时间长、单点故障和阻塞等问题。
>
> 2. **补偿事务（Compensating Transaction）**：补偿事务模式通过在事务中引入补偿操作，来处理分布式环境下的事务回滚。当一个参与者失败或回滚时，系统会执行相应的补偿操作来撤销已经执行的操作。这种方法的关键是定义和实现补偿逻辑，以确保系统的一致性。
>
> 3. **事务日志（Transaction Log）**：利用事务日志来记录事务的操作和状态变化，以便在发生故障时进行恢复和回滚。通过将事务操作和状态变化记录到持久化的日志中，可以在故障恢复过程中重新执行或回滚未完成的事务，以保持数据的一致性。
>
> 4. **Saga模式**：Saga是一种分布式事务的解决方案，它将一个大的事务拆分为多个小的局部事务，并通过一系列的补偿操作来保证事务的一致性。Saga模式适用于长时间运行的事务和高可用性要求的系统，并且可以容忍部分失败。
>
> 5. **分布式数据库**：使用支持分布式事务的数据库系统，如Google Spanner、CockroachDB和TiDB等。这些数据库系统提供了内置的分布式事务支持，可以跨多个节点和数据中心执行事务操作，确保数据的一致性和隔离性。
>
> 需要根据具体的应用场景和需求选择适合的分布式事务解决方案。每种解决方案都有其优缺点和适用性，需要权衡各种因素来做出决策。同时，了解分布式系统的性能、可用性和复杂性等方面的影响也是非常重要的。

### Seata下AT模式，发生事务超时怎么解决

> 在 Seata 的 AT（Automatic Transaction）模式中，如果发生事务超时，可以采取以下步骤来解决：
>
> 1. **确认事务超时原因**：首先，需要确认事务超时的具体原因。事务超时可能是由于网络延迟、资源繁忙或其他不可预测的问题导致的。通过检查日志和监控数据，可以确定超时问题的根本原因。
>
> 2. **调整事务超时设置**：根据超时原因和业务需求，可以适当调整事务超时的设置。在 Seata 中，事务超时时间可以通过配置文件进行设置。通过增加或减少事务超时时间，可以使事务在合理的时间范围内完成，避免过早或过晚超时。
>
> 3. **增加资源的处理能力**：如果事务超时是由于资源繁忙导致的，可以考虑增加资源的处理能力。例如，增加数据库的连接池大小、提高网络带宽、优化慢查询等，以增加资源的吞吐能力和响应速度，从而减少事务超时的可能性。
>
> 4. **检查业务逻辑和数据库操作**：有时，事务超时可能是由于业务逻辑或数据库操作造成的。检查代码中的业务逻辑是否存在性能瓶颈或死锁等问题，并优化相关的逻辑和操作，以提高事务的执行效率和稳定性。
>
> 5. **分布式事务的最终一致性处理**：如果事务超时后无法回滚或提交所有参与者的事务，那么需要考虑分布式事务的最终一致性处理。可以通过定时任务或其他机制来检查未提交或未回滚的事务，并根据业务需求进行相应的处理，以保证数据的一致性。
>
> 需要注意的是，解决事务超时问题需要综合考虑系统架构、业务需求和性能要求等因素。在设计和实现分布式事务时，可以采取合适的超时设置、资源优化和最终一致性处理策略，以确保事务的可靠性和性能。

## SpringBoot

1. ### spring框架主要用了哪些设计模式

   > Spring框架是一个广泛使用的Java企业应用开发框架，它采用了多种设计模式来实现不同的功能和层次。以下是Spring框架主要使用的几种设计模式：
   >
   > 1. **依赖注入（Dependency Injection）**：Spring框架的核心特性之一就是依赖注入。它通过使用控制反转（Inversion of Control，IoC）实现，即对象的创建和依赖关系的管理由框架负责。这种设计模式降低了组件之间的耦合度，使得代码更加灵活、可维护和可测试。
   >
   > 2. **单例模式（Singleton）**：Spring容器默认使用单例模式管理Bean（组件）。这意味着在整个应用程序中，每个Bean的实例只有一个，可以通过配置使其在需要时延迟实例化或立即实例化。
   >
   > 3. **工厂模式（Factory）**：Spring框架使用工厂模式来创建和管理对象。它提供了多种工厂类，如BeanFactory和ApplicationContext，用于创建和获取应用程序中的各种对象。
   >
   > 4. **观察者模式（Observer）**：Spring框架的事件驱动机制使用了观察者模式。当特定事件发生时，触发事件的对象会通知所有注册的观察者，观察者可以对事件做出响应。
   >
   > 5. **模板方法模式（Template Method）**：Spring框架的JdbcTemplate和HibernateTemplate等模块使用了模板方法模式。这种模式定义了一个抽象的操作流程骨架，具体实现由子类提供。
   >
   > 6. **适配器模式（Adapter）**：Spring框架中的适配器模式用于将旧的或不兼容的接口转换为新的接口。例如，Spring MVC框架中的控制器适配器将不同类型的控制器转换为统一的处理方式。
   >
   > 除了上述设计模式，Spring框架还涉及其他模式，如策略模式、装饰器模式、代理模式等，用于实现不同的功能和提供灵活性。设计模式在Spring框架中的应用使得框架更具可扩展性、可定制性和可维护性，同时提供了一致的编程模型。

2. ### spring事务，a普通方法，b方法上加了事务注解，同一个类中，a方法内部调用了b方法，事务会生效吗

   > 在Spring框架中，事务的生效与方法的调用方式有关。如果是在同一个类中的方法之间进行调用，且事务注解被应用在方法上，那么被调用的方法不会经过Spring的代理，因此事务注解不会生效。
   >
   > 具体来说，当在同一个类中的方法A内部调用方法B时，如果方法A没有通过Spring代理进行调用（即方法A是在同一个类的实例上直接调用方法B），那么事务注解对方法B不会生效。这是因为事务注解是通过Spring的代理机制来实现的，它需要在方法调用时经过代理对象才能触发事务的切面行为。
   >
   > 如果确保事务注解生效，可以通过以下两种方式实现：
   >
   > 1. 将方法B抽取到另一个Bean中：将方法B提取到另一个Spring管理的Bean中，并通过依赖注入的方式在方法A中调用该Bean的方法B。这样，方法B会经过Spring的代理，事务注解将生效。
   >
   > ```java
   > @Service
   > public class MyService {
   >     @Autowired
   >     private AnotherService anotherService;
   > 
   >     @Transactional
   >     public void methodA() {
   >         // 调用另一个Bean的方法B
   >         anotherService.methodB();
   >     }
   > }
   > 
   > @Service
   > public class AnotherService {
   >     @Transactional
   >     public void methodB() {
   >         // 方法B的逻辑
   >     }
   > }
   > ```
   >
   > 2. 通过AOP代理强制方法调用：可以通过Spring的AOP功能，使用代理对象来调用方法B，以确保事务注解生效。这可以通过ApplicationContext.getBean()方法获取代理对象，而不是直接调用同一个类的方法。
   >
   > ```java
   > @Service
   > public class MyService {
   >     @Autowired
   >     private ApplicationContext applicationContext;
   > 
   >     @Transactional
   >     public void methodA() {
   >         // 获取代理对象
   >         MyService proxy = applicationContext.getBean(MyService.class);
   >         // 调用代理对象的方法B
   >         proxy.methodB();
   >     }
   > 
   >     @Transactional
   >     public void methodB() {
   >         // 方法B的逻辑
   >     }
   > }
   > ```
   >
   > 在这种方法中，通过获取代理对象并调用方法B，方法B会经过Spring的代理，事务注解将生效。
   >
   > 总结起来，如果在同一个类中的方法A内部直接调用方法B，事务注解不会生效。为了确保事务注解生效，需要将方法B抽取到另一个Bean中，或者通过AOP代理强制方法调用。

3. ### spring主要用到的设计模式

4. ### SpringBoot自动配置原理

   > Spring Boot 的自动配置是通过条件化配置（Conditional Configuration）和自动扫描机制实现的。它的原理可以概括如下：
   >
   > 1. **条件化配置（Conditional Configuration）**：Spring Boot 中的自动配置使用了条件化配置的概念，即根据特定条件来决定是否应用某个配置。条件化配置是通过 `@Conditional` 注解和条件类（Condition）实现的。配置类上的 `@Conditional` 注解会根据条件类的评估结果来决定是否加载该配置类。
   >
   > 2. **自动扫描机制**：Spring Boot 使用了自动扫描机制来发现和加载自动配置类。自动配置类通常位于 `spring-boot-autoconfigure` 模块中，并使用 `@Configuration` 注解进行标记。Spring Boot 在启动时会自动扫描类路径下的 `META-INF/spring.factories` 文件，该文件列出了所有自动配置类的全限定名。
   >
   > 3. **条件注解**：Spring Boot 提供了多个条件注解，用于根据特定条件来决定是否应用某个自动配置。这些条件注解包括 `@ConditionalOnClass`、`@ConditionalOnBean`、`@ConditionalOnProperty` 等。通过在自动配置类的方法上使用这些条件注解，可以根据类的存在性、Bean 的存在性或配置属性的值来决定是否应用该自动配置。
   >
   > 4. **自动配置优先级**：Spring Boot 的自动配置类具有优先级。优先级高的自动配置类会覆盖优先级低的自动配置类中的配置。这样可以确保在多个自动配置类中存在冲突时，可以按照优先级顺序进行覆盖和选择。
   >
   > 5. **配置文件覆盖**：Spring Boot 的自动配置可以通过外部配置文件（如 application.properties 或 application.yml）进行覆盖和调整。通过在配置文件中设置特定的属性值，可以启用或禁用特定的自动配置，并修改默认的配置参数。
   >
   > 总的来说，Spring Boot 的自动配置通过条件化配置和自动扫描机制，根据特定的条件和优先级来自动加载和应用配置类。这样可以大大简化了应用程序的配置工作，使得开发者可以更专注于业务逻辑的实现。

5. ### Spring中使用了哪些设计模式，工厂模式使用场景

   > 在Spring框架中，工厂模式可以用于以下场景：
   >
   > 1. **对象创建的解耦**：工厂模式可以将对象的创建逻辑与使用逻辑解耦，使得客户端代码不直接依赖于具体的对象创建过程。通过工厂类来创建对象，客户端只需要与工厂接口进行交互，而不需要了解具体的对象创建细节。这样可以提高代码的灵活性和可维护性。
   >
   > 2. **隐藏对象创建细节**：工厂模式可以将对象的创建细节隐藏在工厂类中，对客户端代码进行封装。客户端只需要调用工厂类的方法来获取所需的对象，而不需要知道对象是如何创建的。这样可以降低客户端代码的复杂性，并提供更好的抽象和封装。
   >
   > 3. **根据条件创建不同对象**：工厂模式可以根据不同的条件来创建不同的对象。通过在工厂类中根据条件判断，选择合适的创建逻辑，可以实现根据需求动态创建不同类型的对象。这样可以提供更大的灵活性和可配置性。
   >
   > 4. **集中管理对象的生命周期**：工厂模式可以集中管理对象的生命周期，确保对象的创建、初始化和销毁等过程得到统一的管理。通过工厂类来管理对象的生命周期，可以实现对象的单例模式、对象池等高级功能，优化资源的利用和系统的性能。
   >
   > 在Spring框架中，工厂模式的应用非常广泛。例如，Spring中的BeanFactory和ApplicationContext就是工厂模式的实现，用于创建和管理应用程序中的各种Bean对象。通过配置文件或注解，可以定义不同的Bean定义和创建方式，实现对象的依赖注入和生命周期管理。此外，Spring中还有许多其他的工厂类，如JdbcTemplate和RestTemplate等，用于创建和管理数据库连接、HTTP请求等对象。
   >
   > 总而言之，工厂模式在Spring框架中的使用场景包括对象创建的解耦、隐藏对象创建细节、根据条件创建不同对象以及集中管理对象的生命周期等。通过工厂模式，Spring框架提供了更灵活、可配置和可扩展的对象创建和管理机制。

6. ### 了解过哪些设计模式

7. ### Spring中用到了哪些设计模式？你怎么理解模版模式，工厂模式

   > **模板模式（Template Pattern）**是一种行为设计模式，它定义了一个操作中的算法框架，将一些步骤的实现延迟到子类中。在模板模式中，基类提供了一个模板方法，该方法定义了算法的骨架，而具体的步骤实现则由子类来完成。模板模式的核心思想是将不变的部分封装在父类中，将可变的部分留给子类来实现，从而实现代码的复用和灵活性。
   >
   > 模板模式的主要特点包括：
   > - 定义了一个抽象类或接口作为模板，其中包含一个或多个抽象方法和一个模板方法。
   > - 模板方法定义了算法的骨架，包含了一系列的步骤，其中某些步骤可以由子类来实现。
   > - 子类通过继承模板类或实现模板接口，来提供具体步骤的实现。
   > - 模板方法在父类中被声明为 final 或不可覆盖，确保算法的结构不会被子类修改。
   >
   > 模板模式的经典应用场景是在算法框架中，当有多个相关的算法实现时，可以使用模板模式将公共的算法步骤抽象到父类，而将特定的实现留给子类。
   >
   > **工厂模式（Factory Pattern）**是一种创建型设计模式，它提供了一种创建对象的接口，但由子类决定具体实例化的类是哪一个。工厂模式将对象的实例化过程封装在工厂类中，客户端通过调用工厂类的方法来获取所需的对象，而不需要直接调用具体类的构造函数。
   >
   > 工厂模式的主要特点包括：
   > - 定义了一个工厂接口或抽象类，用于创建对象的方法。
   > - 具体的对象创建由工厂的子类来实现，每个子类对应一种具体的对象类型。
   > - 客户端通过工厂类接口或方法来获取对象，而不需要直接调用对象的构造函数。
   >
   > 工厂模式的目的是将对象的创建和使用解耦，使客户端代码依赖于工厂接口而不是具体的类。这样可以提高代码的灵活性和可维护性，同时也方便了代码的扩展和修改。
   >
   > 工厂模式在Spring框架中被广泛应用，例如通过BeanFactory或ApplicationContext来创建和管理对象。Spring框架提供了多种工厂实现，如XML配置工厂、注解扫描工厂等，以满足不同的对象创建需求。
   >
   > 总的来说，模板模式和工厂模式都是常见的设计模式，模板模式用于定义算法的骨架，将具体步骤的实现交给子类，而工厂模式用于封装对象的实例化过程，将具体对象的创建交给工厂类。这两种设计模式都可以提高代码的可复用性、可扩展性和灵活性。

8. ### 讲讲学过的、印象深刻的设计模式，追问了观察者模式

   > 观察者模式（Observer Pattern）是一种行为设计模式，用于在对象之间建立一种一对多的依赖关系，当一个对象的状态发生变化时，它的所有依赖者（观察者）都会收到通知并自动更新。
   >
   > 观察者模式的主要角色包括：
   >
   > 1. **主题（Subject）**：也称为被观察者或可观察对象，它维护一组观察者对象，并提供添加、删除和通知观察者的方法。主题对象通常包含一个状态，当状态发生变化时，会通知所有注册的观察者。
   >
   > 2. **观察者（Observer）**：观察者是主题的依赖者，它定义了一个更新方法，当主题的状态发生变化时，观察者会接收到通知并执行相应的更新操作。
   >
   > 观察者模式的实现步骤如下：
   >
   > 1. 定义主题接口（Subject），其中包含添加观察者、删除观察者和通知观察者的方法。
   >
   > 2. 定义观察者接口（Observer），其中包含一个更新方法，用于接收主题的通知并执行相应的操作。
   >
   > 3. 实现具体的主题类（具体的被观察者），维护一个观察者列表，并在状态变化时通知观察者。
   >
   > 4. 实现具体的观察者类，实现更新方法以响应主题的通知。
   >
   > 观察者模式的优点包括：
   > - 松耦合：主题和观察者之间是松耦合的关系，它们可以独立变化而互不影响。
   > - 可扩展性：可以很方便地增加新的观察者或主题，而不需要修改现有的代码。
   > - 一致性：观察者模式可以确保依赖于同一主题的多个观察者对象之间保持一致性。
   >
   > 观察者模式在实际应用中非常常见，例如：
   > - GUI事件处理：在图形界面中，按钮、菜单等控件的点击事件可以通过观察者模式来处理。
   > - 消息通知机制：在订阅-发布（Pub-Sub）模型中，订阅者（观察者）可以订阅主题（发布者）的消息，并在消息发布时接收

9. ### SpringBoot框架中的IOC和DI解释，为什么要用DI

   > 在Spring Boot框架中，IOC（Inversion of Control，控制反转）和DI（Dependency Injection，依赖注入）是两个核心概念。
   >
   > **IOC（控制反转）**是一种设计原则，它将对象的创建和管理权从应用程序代码转移到容器中。在传统的编程模型中，对象的创建通常由应用程序代码直接实例化，而在IOC中，对象的创建和管理由容器负责。容器负责创建对象并维护它们之间的依赖关系，应用程序代码只需要使用这些对象而不需要关心对象的创建过程。
   >
   > **DI（依赖注入）**是实现IOC的一种方式。它是指通过将依赖关系注入到对象中，而不是在对象内部自己创建依赖对象。依赖注入可以通过构造函数注入、属性注入或方法注入的方式实现。通过DI，对象的依赖关系由外部容器在创建对象时注入，从而实现了对象之间的解耦和灵活性。
   >
   > 为什么要使用DI呢？以下是使用DI的几个好处：
   >
   > 1. **解耦和灵活性**：使用DI可以将对象之间的依赖关系从代码中解耦，使得对象之间的耦合度降低。这样，如果需要更改或替换依赖对象，只需要调整容器的配置而不需要修改代码，提供了更大的灵活性和可维护性。
   >
   > 2. **可测试性**：由于DI使得对象的依赖关系明确且可配置，可以更方便地进行单元测试。在测试环境中，可以使用模拟对象或者测试替身来替代真实的依赖对象，从而对被测试对象进行独立测试。
   >
   > 3. **代码复用**：通过将依赖关系注入到对象中，可以实现依赖的共享和复用。同一个依赖对象可以在多个对象中共享使用，减少了代码的重复性。
   >
   > 4. **可扩展性**：DI可以使得系统更容易进行扩展和修改。当需要添加新的依赖对象时，只需要创建新的对象并配置到容器中，而不需要修改现有的代码。这样，系统的扩展性得到了提高。
   >
   > 总结起来，DI是实现IOC的一种方式，它通过将对象的依赖关系注入到对象中，实现了对象之间的解耦和灵活性。使用DI可以提高代码的可测试性、可维护性和可扩展性，减少了代码的耦合度和重复性。在Spring Boot框架中，IOC和DI是核心特性，通过容器管理对象的创建和依赖关系，使开发者可以更专注于业务逻辑的实现而不需要关注对象的创建和管理过程。

10. ### IOC和DI介绍一下

11. ### IOC的好处

    > IOC（Inversion of Control，控制反转）是一种软件设计原则，它带来了许多好处和优点，下面是一些常见的IOC的好处：
    >
    > 1. **松耦合**：IOC通过将对象的创建和依赖关系的管理从应用程序代码中转移到容器中，实现了对象之间的松耦合。对象不再负责自己的依赖对象的创建和管理，它们只需要通过接口或抽象类与依赖进行交互，而不需要直接与具体的实现类耦合在一起。这样，当需要更改或替换依赖对象时，只需调整容器的配置，而不需要修改对象的代码。松耦合提高了代码的可维护性和可扩展性。
    >
    > 2. **可测试性**：由于IOC使得对象的依赖关系明确且可配置，可以更方便地进行单元测试。在测试环境中，可以使用模拟对象或测试替身来替代真实的依赖对象，从而对被测试对象进行独立测试。通过注入特定的依赖对象，可以轻松模拟各种场景和条件，提高了测试的灵活性和可靠性。
    >
    > 3. **可扩展性**：IOC使得系统更容易进行扩展和修改。当需要添加新的依赖对象时，只需要创建新的对象并配置到容器中，而不需要修改现有的代码。这样，系统的扩展性得到了提高。在大型项目中，使用IOC可以支持模块化开发和组件重用，减少了代码的耦合度，提高了系统的可维护性和可扩展性。
    >
    > 4. **代码复用**：通过IOC容器管理对象的创建和依赖关系，可以实现依赖的共享和复用。同一个依赖对象可以在多个对象中共享使用，减少了代码的重复性。通过配置依赖关系，可以将通用的依赖对象注入到多个对象中，提高了代码的复用性和可维护性。
    >
    > 5. **集中管理和配置**：使用IOC容器可以集中管理和配置对象的创建和依赖关系。通过配置文件或注解，可以定义对象之间的依赖关系，容器会根据配置自动创建对象并解决依赖关系。这样，开发者可以将注意力集中在业务逻辑的实现上，而不需要关心对象的创建和管理过程，提高了开发效率。
    >
    > 总结起来，IOC带来了许多好处，包括松耦合、可测试性、可扩展性、代码复用和集中管理和配置。它通过将对象的创建和依赖关系的管理交给容器，实现了对象之间的解耦和灵活性，提高了代码的可维护性和可扩展性。

12. ### SpringBootStarter 制作流程

    > 制作一个Spring Boot Starter（以下简称Starter）的流程如下：
    >
    > 1. **确定功能和依赖项**：首先确定Starter所要提供的功能和特性，并确定其所依赖的其他库或框架。这可以是任何你想要封装并提供给其他Spring Boot应用程序使用的功能，例如数据库访问、消息队列、缓存等。
    >
    > 2. **创建新的Maven或Gradle项目**：使用构建工具创建一个新的项目，作为Starter的基础。选择适当的构建工具和项目类型（Maven或Gradle）。
    >
    > 3. **添加Starter依赖项**：在Starter项目的构建配置文件中，添加Spring Boot的依赖项，以及任何其他需要的依赖项。这些依赖项将提供Starter所需的核心功能。
    >
    > 4. **编写自动配置**：创建自动配置类，用于配置Starter所提供的功能。自动配置类应该使用Spring Boot提供的注解（如`@Configuration`、`@ConditionalOnClass`、`@ConditionalOnProperty`等）来根据条件加载或配置所需的Bean和属性。
    >
    > 5. **处理Starter属性**：如果Starter需要一些自定义属性来配置其行为，可以创建一个属性配置类，并使用Spring Boot的特性来加载和解析这些属性。
    >
    > 6. **编写必要的代码和逻辑**：根据Starter的功能，编写必要的代码和逻辑来实现所需的功能。这可能包括创建必要的类、接口、配置、工具等。
    >
    > 7. **编写文档和示例**：为了方便其他开发者使用Starter，编写详细的文档和示例代码，解释如何使用Starter以及如何配置和定制Starter的行为。
    >
    > 8. **构建和发布Starter**：使用构建工具将Starter打包成可发布的形式，如JAR文件。可以将Starter发布到本地Maven或Gradle仓库，或者上传到公共的Maven仓库，以便其他开发者可以方便地使用。
    >
    > 9. **测试和验证**：编写单元测试和集成测试来验证Starter的功能和行为是否正确。确保Starter在各种使用场景下都能正常工作。
    >
    > 10. **持续维护和更新**：继续维护和更新Starter，跟进相关的依赖项和最新的Spring Boot版本，修复bug并添加新功能。
    >
    > 总结起来，制作一个Spring Boot Starter的流程包括确定功能和依赖项、创建项目、编写自动配置、处理属性、编写必要的代码和逻辑、编写文档和示例、构建和发布Starter，以及进行测试和持续维护。通过制作Starter，可以将特定功能的配置和依赖项封装起来，使其他开发者可以方便地引入和使用这些功能，提高了代码的复用性和可维护性

## Redis

1. ### 数据库和缓存双写一致性，项目里哪些内容用到了缓存

   > 在应用程序中同时使用数据库和缓存时，确保数据库和缓存之间的双写一致性是一项重要的任务。以下是一些常见的方法来保证数据库和缓存的双写一致性：
   >
   > 1. **使用事务**：在涉及到数据库和缓存的写操作时，使用数据库的事务来确保数据的一致性。在事务中，先更新数据库，然后再更新缓存。如果任何一个操作失败，整个事务将回滚，保证数据库和缓存的数据一致性。这要求数据库和缓存都支持事务操作。
   >
   > 2. **使用缓存刷新策略**：在更新数据库后，立即刷新缓存中相应的数据。这可以通过发送缓存刷新事件或直接删除缓存中的数据来实现。在读取数据时，如果缓存中不存在，则从数据库中读取，并将读取到的数据放入缓存。这样可以保证缓存中的数据与数据库中的数据保持一致。
   >
   > 3. **订阅/发布模式**：使用发布-订阅模式，将数据库的更新操作发布到消息队列或事件总线中。缓存订阅这些更新事件，并相应地更新缓存。这样可以确保数据库和缓存之间的更新操作是有序的，避免数据不一致的情况。
   >
   > 4. **使用缓存失效策略**：在更新数据库时，立即使缓存中相应的数据失效。当下一次读取数据时，发现缓存失效，从数据库中读取最新数据并放入缓存。这样可以保证缓存中的数据与数据库中的数据保持一致，但可能会导致读取操作时的一定延迟。
   >
   > 5. **定期同步或异步同步**：定期或异步地将数据库中的数据同步到缓存中。可以设置定时任务或使用消息队列等机制，将数据库中的数据同步到缓存中，保持数据的一致性。
   >
   > 无论使用哪种方法，都需要仔细考虑应用程序的需求和性能要求。在某些情况下，可以接受稍微的数据不一致性，而在其他情况下，需要严格的一致性。根据具体的应用场景和需求，选择适合的方法来确保数据库和缓存的双写一致性。

2. ### lua脚本怎么保证redis操作的原子性

   > 在Redis中，可以使用Lua脚本来保证一组Redis操作的原子性。Redis提供了`EVAL`和`EVALSHA`命令，可以执行Lua脚本。下面是使用Lua脚本来保证Redis操作的原子性的方法：
   >
   > 1. **编写Lua脚本**：首先，编写一个包含一组Redis操作的Lua脚本。这些操作可以是读取、写入、删除等Redis命令。确保这组操作在执行时是原子性的，即要么全部执行成功，要么全部不执行。
   >
   > 2. **使用EVAL或EVALSHA命令**：将编写好的Lua脚本作为参数传递给Redis的`EVAL`或`EVALSHA`命令。`EVAL`命令可以直接执行Lua脚本，而`EVALSHA`命令使用已经保存在Redis中的脚本的SHA1校验和来执行脚本。
   >
   > 3. **传递参数**：如果Lua脚本需要接收参数，可以在`EVAL`或`EVALSHA`命令中传递参数。在Lua脚本中，可以使用`ARGV`全局变量来获取传递的参数。
   >
   > 4. **执行Lua脚本**：通过调用`EVAL`或`EVALSHA`命令，执行Lua脚本。Redis会将脚本作为一个整体来执行，保证脚本中的操作是原子性的。
   >
   > 在执行Lua脚本时，Redis会将脚本作为一个事务来执行，确保其中的操作是原子性的。如果在执行脚本期间出现错误，Redis会回滚所有已执行的命令，保持数据的一致性。
   >
   > 需要注意的是，为了避免每次执行脚本都发送脚本内容，可以使用`SCRIPT LOAD`命令将脚本加载到Redis中，并获取其SHA1校验和。然后，可以使用`EVALSHA`命令来执行已加载的脚本，提高执行效率。
   >
   > 使用Lua脚本可以将多个Redis操作封装为原子性操作，确保操作的一致性和可靠性。

3. ### Redis 在项目中的应用，具体说说

4. ### Redis Zset 的底层实现

   > 在Redis中，有序集合（Sorted Set）使用的是一种叫做跳跃表（Skip List）的数据结构作为底层实现。
   >
   > 跳跃表是一种有序数据结构，类似于平衡二叉树，但相比于平衡二叉树，跳跃表具有更好的平均性能和更简单的实现。跳跃表通过使用多层链表来加速查找操作，每一层链表都是原始链表的一部分，包含一些节点的指针。每个节点都包含一个分值（score）和一个成员（member）。
   >
   > 在跳跃表中，每个节点都有多个指针，可以横向和纵向移动。横向指针用于在同一层级上移动，纵向指针用于从较高的层级快速跳转到较低的层级。通过这种方式，跳跃表可以在平均情况下实现快速的有序集合操作，如插入、删除和查找。
   >
   > 在有序集合中，每个成员都有一个唯一的分值，根据分值对成员进行排序。通过跳跃表的层级结构，可以快速定位到指定分值范围内的成员，并支持快速插入和删除操作。
   >
   > 除了跳跃表之外，Redis还使用了散列表（Hash Table）来实现有序集合的索引，通过散列表来维护成员与其在跳跃表中的位置之间的映射关系，以提高成员的查找效率。
   >
   > 通过跳跃表和散列表的结合使用，Redis可以高效地实现有序集合（Sorted Set），并提供插入、删除、查找等操作的较好性能。跳跃表的简单结构和高效性能使得Redis在实现有序集合功能时具有优势。

5. ### Redis 的 Hyperloglog底层以及其针对解决的问题

   > Redis中的HyperLogLog（HLL）是一种概率性数据结构，用于估计数据集的基数（不重复元素的数量），即集合中不重复元素的近似计数。
   >
   > HyperLogLog的底层实现使用了基数估计算法。它通过使用哈希函数和位数组来实现，具体如下：
   >
   > 1. **哈希函数**：HyperLogLog使用哈希函数将输入元素映射到一个固定长度的二进制字符串（通常是64位）。这些哈希函数能够将不同的输入均匀地映射到二进制空间，以减少冲突。
   >
   > 2. **位数组**：HyperLogLog使用一个位数组来存储哈希函数的结果。位数组的长度由算法参数决定，一般是2^b（b是参数值）。初始时，位数组的所有位都被初始化为0。
   >
   > 3. **计数估计**：对于输入的每个元素，通过哈希函数计算出一个哈希值，并找到哈希值的最左边连续的0的位数（即二进制表示中最高位连续0的长度）。将这个长度与位数组中对应位置的值进行比较，如果更大，则将位数组中的该位置更新为这个长度。最终，位数组中最大的长度即为基数的估计值。
   >
   > HyperLogLog使用概率性算法来估计基数，因此具有一定的误差范围。但在实际应用中，HyperLogLog的误差通常在0.81%左右，可以提供较为准确的基数估计。
   >
   > HyperLogLog主要解决的问题是在大规模数据集中对基数进行近似计数。相较于传统的精确计数方法，如使用位图或散列表，HyperLogLog的内存占用更少，可以在保证较小内存占用的同时提供较为准确的基数估计。因此，HyperLogLog适用于需要统计大规模数据集的基数信息，如网站的独立访客数、广告点击数、社交媒体的活跃用户数等场景。

6. ### 缓存之间配合使用的一致性问题的解决

   > 在使用多个Redis缓存实例时，确保缓存之间的一致性是一个重要的问题。以下是一些解决方案：
   >
   > 1. **缓存更新策略**：在更新数据时，确保所有相关的缓存实例都被更新。可以采用先更新数据库，然后再更新缓存的方式，确保数据的一致性。或者使用发布/订阅功能，当数据更新时，发布一个消息给所有缓存实例，通知它们进行相应的更新。
   >
   > 2. **缓存失效策略**：使用合适的缓存失效策略，以避免脏数据。可以设置合适的过期时间，或者使用LRU（最近最少使用）等缓存淘汰算法，以保持缓存的新鲜度。
   >
   > 3. **分布式锁**：使用分布式锁来保证多个缓存实例之间的互斥性。在更新数据时，通过获取分布式锁来确保只有一个实例能够进行数据更新操作，其他实例需要等待锁释放。常用的分布式锁实现有Redis的`SETNX`和`SET`命令结合使用，或者使用基于Redis的分布式锁库，如Redlock、Redisson等。
   >
   > 4. **读写分离**：将读写操作分离到不同的Redis实例上，减少对同一实例的并发写操作，从而降低冲突和一致性问题的风险。
   >
   > 5. **数据同步机制**：使用数据同步机制来保持多个缓存实例之间的数据一致性。可以使用Redis的复制功能，将一个实例作为主节点，其他实例作为从节点进行数据复制。当主节点更新数据时，从节点会自动同步更新。另外，可以使用Redis的持久化功能，将数据定期写入磁盘，保证数据的持久性和一致性。
   >
   > 需要根据具体的应用场景和需求选择合适的解决方案，以确保多个Redis缓存实例之间的数据一致性和可靠性。

7. ### Redis常用数据结构和他们的底层实现

   > Redis是一个基于内存的键值存储系统，提供了多种数据结构来满足不同的需求。下面是Redis中常用的数据结构及其底层实现：
   >
   > 1. **字符串（String）**：字符串是Redis最基本的数据结构，用于存储和操作二进制数据。字符串在Redis中的底层实现是简单的动态字符串（Dynamic String），它是一个可变长度的字符数组，可以高效地执行字符串操作。
   >
   > 2. **哈希表（Hash）**：哈希表用于存储键值对集合，类似于其他编程语言中的字典或映射。Redis的哈希表底层使用散列表（Hash Table）实现，通过哈希函数将键映射到桶（Bucket）中，每个桶中存储了一个链表或跳跃表，用于解决哈希冲突。
   >
   > 3. **列表（List）**：列表是一个有序的字符串元素集合，可以在列表的两端进行插入、删除和查询操作。Redis的列表底层使用双向链表（Doubly Linked List）实现，每个节点包含前驱指针和后继指针。
   >
   > 4. **集合（Set）**：集合是一个无序的、不重复的字符串元素集合，支持添加、删除和查询操作。Redis的集合底层使用哈希表实现，每个元素作为哈希表的键，值为空。
   >
   > 5. **有序集合（Sorted Set）**：有序集合是一个有序的、不重复的字符串元素集合，每个元素关联一个分值，用于排序。Redis的有序集合底层使用跳跃表（Skip List）和散列表（Hash Table）相结合的方式实现，跳跃表用于元素的有序存储，散列表用于支持快速的成员查找。
   >
   > 除了以上常用的数据结构外，Redis还提供了一些特殊的数据结构，如位图（Bitmap）、地理空间索引（Geospatial Index）等，它们都有各自的底层实现方式。
   >
   > 了解Redis的数据结构及其底层实现可以帮助我们更好地理解Redis的性能和使用场景，并在实际应用中选择合适的数据结构来满足需求。

8. ### redis数据结构，跳表，跳表怎么删除元素

   > 在Redis中，有序集合（Sorted Set）的底层实现中使用了跳表（Skip List）。跳表是一种有序的数据结构，它通过添加多级索引来提高元素的查找效率。
   >
   > 要删除跳表中的元素，需要执行以下步骤：
   >
   > 1. **定位元素**：首先，需要通过遍历跳表的索引层，从顶层索引开始，逐级向下查找，找到包含待删除元素的节点。这一步是为了定位到包含待删除元素的最底层索引节点。
   >
   > 2. **更新索引节点指针**：一旦找到包含待删除元素的节点，需要更新相邻节点的指针，将它们连接起来。即将待删除节点的前驱节点与后继节点相连，跳过待删除节点。
   >
   > 3. **删除底层节点**：在最底层索引节点中删除待删除元素的节点。这一步是直接的节点删除操作。
   >
   > 4. **更新上层索引**：如果待删除节点在高层索引中也存在，需要将高层索引中对应的节点删除。这一步是为了保持跳表的结构一致性。
   >
   > 需要注意的是，删除操作可能会导致跳表的高度发生变化。如果删除元素后导致某一层索引没有节点，可以将该层索引删除，减少跳表的高度，以节省空间。
   >
   > 跳表的删除操作时间复杂度为O(log n)，其中n是跳表中的元素数量。与查找操作类似，删除操作也利用了跳表的索引结构，使得删除操作具有较高的效率。
   >
   > 需要注意的是，以上是简化的描述，实际的跳表实现可能还包括其他细节和优化。Redis的跳表实现是一种经过优化的跳表结构，用于实现有序集合，以提供高效的有序集合操作。

9. ### Redis数据结构，使用的场景

   > Redis支持多种数据结构，每种数据结构都有自己的特点和适用场景。以下是Redis中常用的数据结构及其使用场景：
   >
   > 1. **字符串（String）**：字符串是Redis最基本的数据结构，用于存储和操作二进制数据。它可以用于缓存、计数器、消息队列等场景。
   >
   > 2. **哈希表（Hash）**：哈希表用于存储键值对集合，类似于其他编程语言中的字典或映射。哈希表适用于存储对象、缓存数据、存储用户属性等场景。
   >
   > 3. **列表（List）**：列表是一个有序的字符串元素集合，可以在列表的两端进行插入、删除和查询操作。列表适用于消息队列、发布/订阅、最新消息列表等场景。
   >
   > 4. **集合（Set）**：集合是一个无序的、不重复的字符串元素集合，支持添加、删除和查询操作。集合适用于社交网络关系、标签系统、唯一值存储等场景。
   >
   > 5. **有序集合（Sorted Set）**：有序集合是一个有序的、不重复的字符串元素集合，每个元素关联一个分值，用于排序。有序集合适用于排行榜、热门数据统计、范围查询等场景。
   >
   > 6. **位图（Bitmap）**：位图是一个由二进制位组成的数据结构，可以进行高效的位操作。位图适用于布隆过滤器、在线状态跟踪、标记系统等场景。
   >
   > 7. **地理空间索引（Geospatial Index）**：地理空间索引用于存储地理位置信息，并支持地理位置的查询和计算。地理空间索引适用于位置服务、附近的人功能等场景。
   >
   > Redis的数据结构灵活且高效，可以根据不同的场景选择合适的数据结构来存储和操作数据。同时，Redis还提供了丰富的命令和功能，使得这些数据结构更易于使用和管理。

10. ### redis有了解吗？为什么要学redis？

    > 是的，我了解Redis。Redis（Remote Dictionary Server）是一个开源的高性能键值存储系统，以内存为主要存储介质，提供了丰富的数据结构和功能。它具有以下特点：
    >
    > 1. **高性能**：Redis将数据存储在内存中，因此具有快速的读写速度。同时，Redis采用了单线程的事件驱动模型，避免了线程切换和锁竞争的开销，进一步提高了性能。
    >
    > 2. **丰富的数据结构**：Redis支持多种数据结构，如字符串、哈希表、列表、集合、有序集合等，每种数据结构都有自己的特点和适用场景，使得Redis可以满足不同的数据存储和操作需求。
    >
    > 3. **持久化支持**：Redis支持将数据持久化到磁盘，以保证数据的可靠性和持久性。它提供了两种持久化方式：快照（snapshotting）和日志（append-only file，AOF），可以根据实际需求选择合适的持久化方式。
    >
    > 4. **分布式支持**：Redis提供了分布式功能，可以将数据分布在多个节点上，实现数据的高可用和扩展性。它支持主从复制（master-slave replication）、哨兵（sentinel）和集群（cluster）等分布式模式。
    >
    > 为什么要学习Redis呢？以下是一些学习Redis的理由：
    >
    > 1. **高性能缓存**：Redis作为一个内存数据库，被广泛应用于缓存场景。学习Redis可以帮助你了解如何使用Redis作为高性能缓存，提升应用程序的性能和响应速度。
    >
    > 2. **数据存储和处理**：Redis提供了丰富的数据结构和功能，可以用于存储和处理各种类型的数据。学习Redis可以帮助你掌握如何使用不同的数据结构和命令来操作数据，满足各种业务需求。
    >
    > 3. **分布式系统**：Redis提供了分布式功能，可以将数据分布在多个节点上，实现高可用和扩展性。学习Redis的分布式特性可以帮助你构建和管理分布式系统，提高系统的可靠性和性能。
    >
    > 4. **应用场景广泛**：Redis的应用场景非常广泛，可以应用于缓存、队列、计数器、实时排行榜、发布/订阅等各种场景。学习Redis可以帮助你了解这些应用场景，并掌握如何使用Redis来解决实际问题。
    >
    > 总之，学习Redis可以帮助你掌握一种高性能、灵活的数据存储和处理工具，提升你在开发和架构设计中的能力，同时也为你打开了更多的职业机会和发展空间。

11. ### redis为什么快？背后的数据模型，线程模型？

    > Redis之所以快速，主要有以下几个原因：
    >
    > 1. **数据存储在内存中**：Redis将数据存储在内存中，而不是磁盘上。相比于磁盘存储，内存读写速度更快，可以提供非常高的吞吐量和低延迟。
    >
    > 2. **单线程模型**：Redis采用单线程的事件驱动模型，通过事件循环（event loop）处理客户端请求。这样避免了线程切换和锁竞争的开销，提高了处理请求的效率。
    >
    > 3. **非阻塞式 I/O**：Redis使用非阻塞式 I/O 操作，通过异步地处理网络请求，避免了等待 I/O 操作完成的阻塞情况。这使得 Redis 能够同时处理大量的并发请求。
    >
    > 4. **优化的数据结构和算法**：Redis内部实现了一些优化的数据结构和算法，例如使用哈希表和跳表来实现键值对的存储和查找，以及使用压缩列表来优化列表和集合的存储。这些优化可以提高数据操作的效率。
    >
    > 关于背后的数据模型和线程模型：
    >
    > 1. **数据模型**：Redis采用键值对（key-value）的数据模型。每个键都与一个值关联，而值可以是字符串、哈希表、列表、集合、有序集合等不同的数据结构。这种简单而灵活的数据模型使得Redis可以适用于各种不同的应用场景。
    >
    > 2. **线程模型**：Redis采用单线程的事件驱动模型。在主线程中，通过事件循环（event loop）来处理客户端的请求。当一个请求到达时，Redis会将其放入队列中，并通过事件循环逐个处理请求。这种单线程模型避免了多线程之间的竞争和锁的使用，简化了并发编程的复杂性，并且能够提供良好的响应性能。
    >
    > 需要注意的是，尽管Redis采用单线程模型，但在处理某些耗时的操作（如持久化过程）时，会使用额外的线程来执行这些操作，以避免阻塞主线程。
    >
    > 总之，Redis通过将数据存储在内存中、采用单线程的事件驱动模型以及优化的数据结构和算法等方式，实现了高性能和低延迟的特性。这使得Redis成为一个快速、可靠的键值存储系统，并得到广泛的应用。

12. ### 为什么redis读写是单线程的就快呢？

    > Redis之所以在读写操作上单线程模型下能够实现快速的原因如下：
    >
    > 1. **避免线程切换开销**：线程切换会引入上下文切换的开销，包括保存当前线程的上下文、加载新线程的上下文等操作。在高并发场景下，线程切换开销会显著影响系统的性能。Redis采用单线程模型，避免了线程切换的开销，使得处理请求的过程更加高效。
    >
    > 2. **避免锁竞争**：多线程环境下，如果多个线程同时访问共享数据，就会引发锁竞争的问题。而锁的获取和释放会引入额外的开销。Redis的单线程模型避免了多线程下的锁竞争问题，从而提高了系统的并发性能。
    >
    > 3. **充分利用 CPU 缓存**：在多线程环境下，不同线程访问不同的数据，可能导致缓存行无效（cache line invalidation）的情况发生，从而导致缓存竞争和性能下降。而单线程模型下，Redis可以更好地利用 CPU 缓存，减少缓存行无效的情况，提高了数据访问的效率。
    >
    > 4. **优化的异步非阻塞 I/O**：Redis采用了异步非阻塞的 I/O 操作，通过事件循环（event loop）处理客户端请求。这种方式下，即使是单线程，Redis也能够同时处理多个请求，从而提高了并发能力和系统的响应性能。
    >
    > 需要注意的是，Redis的单线程模型适用于大多数场景下的读密集型操作。对于写密集型操作，由于单线程模型会阻塞其他请求的处理，可能会影响系统的吞吐量。但是，通过合理的配置和优化，结合持久化策略和主从复制等技术，可以在一定程度上缓解写入压力。
    >
    > 总的来说，Redis采用单线程模型的优点在于避免了线程切换和锁竞争的开销，充分利用 CPU 缓存，以及通过异步非阻塞 I/O 实现高效的请求处理。这些因素使得Redis在读密集型场景下能够实现快速的读写操作。

13. ### redis如果有大量的读写操作过来了，其中有一个瞎搞就把后面的全都卡住了，如果是单线程是不是会有一个人坏了一锅粥的情况？

    > 是的，由于Redis采用单线程模型，如果有一个读写操作出现阻塞或耗时过长的情况，那么后续的请求也会被阻塞，可能导致整个系统的响应性能下降。这种情况下可以被称为"一个人坏了一锅粥"的情况。
    >
    > 当某个操作阻塞时，Redis无法同时处理其他请求，因为它是按照顺序处理请求的。这种情况下，如果有大量的请求涌入，而其中一个请求出现阻塞，那么后续的请求都会被排队等待处理，导致系统的整体吞吐量和响应时间受到影响。
    >
    > 为了避免这种情况，需要注意以下几点：
    >
    > 1. **优化阻塞操作**：尽量避免在Redis中执行阻塞或耗时较长的操作，例如避免执行复杂的计算、网络请求或其他可能导致阻塞的操作。可以将这些操作放到异步任务或其他线程中处理，以减少对主线程的阻塞。
    >
    > 2. **合理设置超时时间**：对于可能会阻塞的操作，设置适当的超时时间，以避免长时间的阻塞。超时时间可以根据实际需求和操作的预期执行时间来设定。
    >
    > 3. **使用多个Redis实例**：可以通过使用多个Redis实例来分摊请求负载，使得一个实例的阻塞操作不会影响其他实例的正常运行。可以使用主从复制、哨兵模式或Redis集群等机制来实现多个Redis实例的部署和协调。
    >
    > 4. **合理的性能优化和资源配置**：进行性能优化和资源配置，包括合理设置Redis的最大连接数、调整操作的并发度、配置合适的硬件资源等，以提高系统的并发处理能力和响应性能。
    >
    > 需要注意的是，尽管Redis采用单线程模型会有这样的限制，但在大多数情况下，Redis的单线程模型仍然能够提供很高的吞吐量和响应性能。合理的架构设计和性能优化可以帮助克服这些限制，并确保系统的高可用性和性能。

14. ### Redis分布式锁底层实现

    > Redis分布式锁的底层实现可以通过以下几种方式：
    >
    > 1. **基于SETNX命令（SET if Not eXists）**：SETNX命令是Redis提供的原子操作，用于设置一个键的值，仅当该键不存在时才设置成功。分布式锁的实现可以利用SETNX命令，在一个共享的键上进行加锁操作。如果SETNX返回1，表示加锁成功；如果返回0，表示锁已经存在，加锁失败。解锁时可以通过DEL命令删除键来释放锁。
    >
    > 2. **基于SET命令和过期时间**：在加锁操作中，可以使用SET命令设置一个键的值，并通过设置过期时间来实现锁的自动释放。如果某个客户端成功地设置了该键，并设置了适当的过期时间，那么它获得了锁。其他客户端在尝试加锁时可能会失败，因为键已经存在。解锁时可以使用DEL命令删除键来释放锁。
    >
    > 3. **基于Redlock算法**：Redlock是Redis作者提出的一种分布式锁算法。它使用多个独立的Redis实例来实现锁的可靠性。具体实现包括在多个Redis实例上尝试加锁、设置合适的超时时间、检查大多数实例是否成功加锁等。Redlock算法考虑了网络分区和故障的情况，提供了更高的可靠性和安全性。
    >
    > 需要注意的是，使用Redis实现分布式锁时，需要考虑到以下问题：
    >
    > - **锁竞争**：多个客户端同时尝试获取锁时可能会发生竞争条件。需要确保加锁操作是原子的，以避免多个客户端同时获得锁。使用Redis提供的原子操作或乐观锁等技术可以解决这个问题。
    >
    > - **锁的超时处理**：在分布式环境中，如果一个客户端获取到锁后发生故障，导致无法正常释放锁，会引发死锁问题。可以通过设置适当的锁超时时间，并在超时后自动释放锁来解决这个问题。
    >
    > - **时钟的同步性**：分布式系统中，不同节点的时钟可能存在偏差。如果使用基于过期时间的锁实现，需要确保各个节点的时钟是相对同步的，以避免锁的过期时间计算错误。
    >
    > 以上是一些常见的Redis分布式锁底层实现方式，具体的选择取决于应用场景和需求。为了确保分布式锁的正确性和可靠性，建议使用经过验证的分布式锁实现库或参考可靠的实现方案。

15. ### 服务集群部署，怎么保证平均访问每个节点（轮询），有哪些访问分配的方式

    > 在服务集群部署中，可以采用以下几种方式来实现平均访问每个节点的负载均衡（Load Balancing）：
    >
    > 1. **轮询（Round Robin）**：将请求依次分发到每个节点，实现简单的轮询方式。每个请求按顺序分配给下一个节点，确保每个节点接收到的请求数大致相同。轮询算法适用于节点性能相近且请求处理时间相对稳定的情况。
    >
    > 2. **加权轮询（Weighted Round Robin）**：为每个节点分配一个权重值，根据权重值决定每个节点接收到的请求量。节点权重越高，接收到的请求量就越多。这种方式可以根据节点的性能或资源配置分配不同的权重，使得高性能节点或资源强大的节点能够处理更多的请求。
    >
    > 3. **随机（Random）**：随机选择一个节点来处理每个请求。这种方式简单快速，适用于节点性能相近的场景，但不能保证每个节点接收到的请求数完全均衡。
    >
    > 4. **最少连接（Least Connections）**：基于当前连接数选择连接数最少的节点来处理请求。通过监控每个节点的连接数，将请求分发给连接数最少的节点，以实现负载均衡。这种方式适用于请求处理时间较长或连接数占据较多资源的情况。
    >
    > 5. **IP哈希（IP Hash）**：根据客户端的IP地址将请求分发到特定节点。通过对客户端IP地址进行哈希运算，将同一客户端的请求分配给相同的节点，实现会话保持（Session Affinity）。这种方式适用于需要保持会话状态的应用，确保同一客户端的请求都发送到同一节点。
    >
    > 上述方式是常见的负载均衡算法，实现方式可以在负载均衡器（如Nginx、HAProxy）或应用层代码中进行。根据实际需求，可以选择合适的负载均衡策略来平衡集群中每个节点的负载，并提高系统的性能和可靠性。

## 微服务

1. ### 有哪些注册中心，zookeeper有什么功能

   > 以下是一些常见的注册中心：
   >
   > 1. **ZooKeeper**：ZooKeeper是一个开源的分布式协调服务，可以用作注册中心。它提供了高可用性、一致性和可靠性，并支持分布式锁、配置管理、命名服务等功能。ZooKeeper通过维护一个树形结构的数据模型（称为ZooKeeper的ZNode），用于存储和管理各个服务的元数据信息。
   >
   > 2. **Consul**：Consul是一个开源的服务网格和服务发现系统，也可用作注册中心。它提供了服务注册和发现、健康检查、KV存储、分布式一致性等功能。Consul支持多数据中心部署，并提供了HTTP和DNS接口来进行服务发现和路由。
   >
   > 3. **Eureka**：Eureka是Netflix开源的服务注册与发现组件，用于构建高可用的、分布式的系统架构。Eureka使用基于RESTful的API来实现服务注册和发现，并提供了客户端负载均衡和故障转移的功能。
   >
   > 4. **etcd**：etcd是一个分布式的键值存储系统，也可用作注册中心。它提供了高可用性、一致性和持久性，并支持分布式锁、配置管理、服务发现等功能。etcd使用Raft一致性算法来维护分布式的数据存储。
   >
   > 5. **Nacos**：Nacos是阿里巴巴开源的一个动态服务发现、配置管理和服务管理平台。它提供了服务注册和发现、配置管理、动态DNS、服务路由等功能，支持多种注册和发现方式，包括基于HTTP和DNS的发现。
   >
   > 这些注册中心都提供了服务注册和发现的功能，使得分布式系统中的各个服务能够方便地进行通信和协作。它们还提供了一些附加功能，如健康检查、负载均衡、故障转移等，以提高系统的可靠性和性能。
   >
   > 至于ZooKeeper的功能，它主要用于协调分布式系统中的各个节点，提供以下功能：
   >
   > 1. **命名服务**：ZooKeeper提供了一个树形结构的命名空间，可以用于注册和查找服务，类似于文件系统的目录结构。
   >
   > 2. **分布式锁**：ZooKeeper提供了分布式锁的实现，用于协调多个节点对共享资源的访问，避免竞争条件和数据不一致问题。
   >
   > 3. **配置管理**：ZooKeeper可以用于存储和管理配置信息，各个节点可以通过监听ZooKeeper上的配置节点来获取最新的配置。
   >
   > 4. **集群管理**：ZooKeeper可以用于监控和管理分布式系统的集群状态，包括节点的上线和下线、故障检测和恢复等。
   >
   > 5. **领导者选举**：ZooKeeper提供了一种基于ZAB（ZooKeeper Atomic Broadcast）协议的领导者选举机制，用于选举出一个节点作为系统的领导者，确保系统的高可用性和容错性。
   >
   > 总而言之，ZooKeeper是一个功能强大的分布式协调服务，可以用于实现注册中心、分布式锁、配置管理等多种分布式系统的核心功能。

2. ### 注册中心有哪些中间件

## 中间价

1. ### kafka在高性能方面有哪些优化；

   > Kafka是一个高性能的分布式消息队列系统，以下是一些Kafka在高性能方面的优化技巧：
   >
   > 1. **适当的副本因子（Replication Factor）**：Kafka使用副本机制来提高数据的可靠性和可用性。选择适当的副本因子可以在性能和可靠性之间进行权衡。较低的副本因子可以提高写入性能，但可能降低可用性；较高的副本因子可以提高可靠性，但可能增加写入延迟。
   >
   > 2. **合理的分区数量和大小**：Kafka的主题（Topic）被划分为多个分区（Partition），并分布在不同的Broker上。合理设置分区数量和大小可以提高并行性和负载均衡。通常，过多的小分区会增加管理开销，而过少的大分区可能导致负载不均衡。根据实际需求和集群规模选择合适的分区设置。
   >
   > 3. **优化Producer的批量发送**：Kafka的Producer可以通过批量发送消息来提高性能。调整批量发送的参数（如`batch.size`和`linger.ms`）可以控制每次发送的消息数量和等待时间。适当增加批量大小和延迟时间可以减少网络开销和提高吞吐量，但要注意不要增加过多的延迟。
   >
   > 4. **合理配置消息压缩**：Kafka支持消息压缩，可以通过配置合适的压缩算法（如GZIP、Snappy或LZ4）来减少网络传输和存储开销。选择合适的压缩算法可以在性能和压缩率之间进行权衡，提高传输效率和节约存储空间。
   >
   > 5. **合理调整Consumer的参数**：Consumer的参数设置也会对性能产生影响。例如，合理设置`fetch.min.bytes`和`fetch.max.wait.ms`参数可以控制每次拉取消息的大小和等待时间，以减少网络开销和提高吞吐量。另外，使用多个Consumer实例并行消费分区可以提高消费速度。
   >
   > 6. **合理设置Kafka的日志清理策略**：Kafka的日志以分段（Segment）的方式存储，可以根据时间或大小进行清理。合理设置日志清理策略可以控制磁盘空间的使用和读写性能。根据业务需求和存储资源情况选择适当的清理策略。
   >
   > 7. **合理调整操作系统和硬件参数**：在部署Kafka时，还可以针对操作系统和硬件进行优化。例如，调整文件描述符限制、TCP参数、磁盘调度策略等，以提高Kafka的性能和稳定性。
   >
   > 需要注意的是，性能优化是一个综合考虑各个因素的过程，需要根据具体的应用场景和需求进行调优。在进行优化之前，建议先进行性能测试和基准测试，了解系统的瓶颈和性能瓶颈所在，然后有针对性地采取优化措施。

2. ### kafka怎么保证幂等性；

   > 在Kafka中，可以通过以下方法来保证消息的幂等性：
   >
   > 1. **消息唯一标识符（Message Unique Identifier）**：Producer在发送消息时，为每条消息附加一个唯一的标识符（例如消息ID）。这个标识符可以是全局唯一的，可以通过UUID或其他唯一生成算法生成。在消息处理过程中，Consumer可以使用这个标识符来判断消息是否已经处理过，从而实现幂等性。
   >
   > 2. **幂等性写入操作**：Producer在发送消息时，可以使用幂等性写入操作来确保消息的幂等性。Kafka提供了一种幂等性的Producer方法（`producer.send()`），它会在发送消息时自动进行幂等性检查。如果使用幂等性写入操作发送相同的消息多次，Kafka只会将消息写入一次，并确保消息在分区中的顺序。
   >
   > 3. **幂等性消费逻辑**：Consumer在处理消息时，需要实现幂等性的消费逻辑。消费者可以通过维护一个消费状态或消费记录来判断消息是否已经处理过。例如，可以使用数据库或缓存来记录已处理的消息的状态，消费者在处理消息时先查询状态，如果已经处理过则跳过，否则执行处理逻辑。
   >
   > 4. **保证端到端的幂等性**：在分布式系统中，保证端到端的幂等性是至关重要的。除了Producer和Consumer的幂等性处理，还需要确保系统中的其他组件也能保持幂等性。例如，如果消息被转发到其他系统进行处理，那么这些系统也需要实现幂等性的处理逻辑。
   >
   > 需要注意的是，幂等性是相对于同一条消息而言的，而不是针对不同的消息。即使消息的内容相同，但如果它们具有不同的唯一标识符，Kafka仍然会将它们视为不同的消息。因此，在实现幂等性时，需要保证消息的唯一性和一致性。
   >
   > 另外，值得注意的是，Kafka本身并不提供幂等性的绝对保证，而是通过合理的设计和实现来尽可能地保证幂等性。在实际应用中，需要综合考虑系统的设计、消息处理逻辑和数据一致性等方面来确保幂等性的正确性和可靠性。

3. ### netty的线程模型(没学netty没答上来)

   > Netty是一个基于事件驱动的网络应用框架，它采用了一种高效的线程模型来处理并发连接和网络I/O操作。Netty的线程模型基于两个核心组件：Boss Group和Worker Group。
   >
   > 1. **Boss Group**：Boss Group是用于接受客户端连接的线程组。它绑定在服务端的ServerSocket上，监听并接受传入的连接请求。当有新的连接请求到达时，Boss Group会将连接注册到Worker Group中的某个线程上进行处理。Boss Group通常只有一个线程。
   >
   > 2. **Worker Group**：Worker Group是用于处理I/O操作的线程组。它负责处理已经被接受的连接的I/O事件，例如读取数据、写入数据和处理业务逻辑。Worker Group通常包含多个线程，可以根据系统的负载情况进行配置。
   >
   > 在Netty的线程模型中，Boss Group和Worker Group之间存在两种不同的线程池模式：**单线程模式**和**多线程模式**。
   >
   > - **单线程模式**：Boss Group和Worker Group都使用单线程处理连接和I/O操作。这种模式适用于连接数较少且业务逻辑处理较轻的场景，可以确保所有的连接和I/O操作都在同一个线程上执行，避免了线程切换和锁竞争的开销。
   >
   > - **多线程模式**：Boss Group使用单线程处理连接请求，而Worker Group使用多个线程处理I/O操作。这种模式适用于连接数较多或者有较重的业务逻辑处理的场景，可以充分利用多核处理器的性能优势，提高并发处理能力。
   >
   > 无论是单线程模式还是多线程模式，Netty的线程模型都采用了事件循环（Event Loop）机制来处理I/O事件。每个线程都拥有一个事件循环，它负责监听和处理注册在其上的I/O事件。事件循环使用了高效的事件驱动机制，通过非阻塞的方式处理事件，避免了线程阻塞和资源浪费。
   >
   > 总的来说，Netty的线程模型采用了一种灵活而高效的设计，能够适应不同规模和性能要求的网络应用。通过合理的配置和调优，可以提高系统的并发处理能力和响应性能。

4. ### 有哪些缓存用的中间件

   > 有许多常用的缓存中间件可用于构建高性能的缓存系统。以下是一些常见的缓存中间件：
   >
   > 1. **Redis**：Redis是一个开源的高性能内存数据结构存储系统，常被用作缓存中间件。它支持多种数据结构（如字符串、哈希、列表、集合、有序集合等），提供了丰富的操作命令和功能，并具有高速的读写性能、持久化和复制特性。
   >
   > 2. **Memcached**：Memcached是一个简单而高效的分布式内存对象缓存系统。它通过在内存中存储键值对来实现数据缓存，具有高速的读写性能和良好的可扩展性。Memcached适用于缓存小型对象，并且不提供持久化功能。
   >
   > 3. **Ehcache**：Ehcache是一个Java开发的开源缓存框架，可以作为本地缓存或分布式缓存中间件使用。它提供了丰富的特性，包括内存管理、缓存策略、数据过期和持久化等功能。Ehcache可以与各种Java应用程序和框架集成。
   >
   > 4. **Caffeine**：Caffeine是一个基于Java的高性能内存缓存库，它提供了快速的缓存实现和低延迟的访问。Caffeine支持各种缓存特性，如过期、最大大小、定时刷新和异步加载等，并提供了高效的缓存数据结构和并发控制。
   >
   > 5. **Hazelcast**：Hazelcast是一个开源的内存数据网格（In-Memory Data Grid）系统，提供了分布式缓存和计算功能。它可以将数据存储在内存中，并通过分布式方式在多个节点之间共享和访问。Hazelcast支持多种数据结构和高级功能，如持久化、复制、分片和事务等。
   >
   > 这些缓存中间件都有各自的特点和适用场景，选择合适的缓存中间件应根据具体的需求和系统架构来决定。在进行选择时，需要考虑性能要求、数据一致性、持久化需求、可扩展性以及与应用程序的集成和使用成本等因素。

5. ### 你知道哪些消息队列，消息队列的作用是什么

   > 以下是一些常见的消息队列（Message Queue）系统：
   >
   > 1. **Apache Kafka**：Kafka是一个高吞吐量、分布式的发布-订阅消息系统。它可以处理大规模的实时数据流，并提供持久化、容错和数据副本等功能。
   >
   > 2. **RabbitMQ**：RabbitMQ是一个开源的、基于AMQP（高级消息队列协议）的消息中间件。它支持多种消息传递模式，如点对点、发布-订阅和请求-响应，并提供了高度灵活的路由和消息过滤功能。
   >
   > 3. **ActiveMQ**：ActiveMQ是一个开源的、基于Java的消息队列系统，支持JMS（Java消息服务）规范。它提供了可靠的消息传递、消息持久化、事务支持和集群部署等特性。
   >
   > 4. **Amazon Simple Queue Service（SQS）**：SQS是亚马逊云服务（AWS）提供的托管消息队列服务。它具有高可用性、可伸缩性和可靠性，并支持分布式系统和云原生应用程序的消息通信。
   >
   > 5. **Apache Pulsar**：Pulsar是一个分布式的、可扩展的事件流和消息系统。它可以处理高吞吐量的数据流，并提供多租户、持久化、故障转移和灵活的消息传递模型。
   >
   > 消息队列的作用是在分布式系统中实现异步通信和解耦。它们允许应用程序将消息发送到队列中，而不需要立即处理它们。其他应用程序（消费者）可以从队列中订阅消息，并以自己的速度异步地消费消息。这种解耦的方式带来了以下几个重要的好处：
   >
   > 1. **削峰填谷**：消息队列可以平衡生产者和消费者之间的流量差异，通过将消息缓冲在队列中，使得消费者可以以自己的速度处理消息，而不会因为生产速度过快而导致系统崩溃或性能下降。
   >
   > 2. **异步通信**：生产者和消费者之间的消息传递是异步的，生产者无需等待消费者的响应即可继续执行。这样可以提高系统的响应性能和吞吐量，并允许不同组件之间的解耦。
   >
   > 3. **可靠性和持久化**：消息队列通常提供消息的持久化机制，即使在消息发送或消费过程中出现故障，消息也不会丢失。这确保了消息的可靠传递和系统的可靠性。
   >
   > 4. **系统解耦**：通过引入消息队列，不同的应用程序和组件可以通过消息进行通信，而不需要直接依赖彼此。这种解耦性使得系统更加灵活、可扩展和易于维护。
   >
   > 消息队列在许多场景下都得到了广泛的应用，如异步任务处理、事件驱动架构、日志收集、实时推送、微服务通信等。它们为构建可靠、高性能和可扩展的分布式系统提供了重要的基础设施。

6. ### 消息队列作用

   > 

7. ### 消息队列有哪些中间件，消息队列的作用

   > 

8. ### kafka架构有什么，发送消息流程

   > Kafka是一个分布式的发布-订阅消息系统，具有高吞吐量和可扩展性。它的架构由以下几个核心组件组成：
   >
   > 1. **Producer**：Producer负责将消息发布到Kafka集群。它将消息发送到指定的主题（Topic），并可以选择性地指定消息的分区（Partition）。Producer可以以异步或同步方式发送消息。
   >
   > 2. **Broker**：Broker是Kafka集群中的消息代理服务器。它负责接收、存储和分发消息。每个Broker都是一个独立的服务实例，可以部署在不同的机器上，构成一个分布式的消息存储和处理系统。
   >
   > 3. **Topic**：Topic是消息的逻辑分类，它是消息的发布和订阅的主题。每个主题可以划分为多个分区，每个分区可以在不同的Broker上进行复制和存储。
   >
   > 4. **Partition**：Partition是Topic的一个子分片。每个分区都是一个有序、不可变的消息序列，可以水平扩展和并行处理。每个分区在存储层面上都是一个日志文件。
   >
   > 5. **Consumer**：Consumer是从Kafka集群中订阅消息的客户端。它可以以消费者组（Consumer Group）的形式进行组织，每个消费者组可以有多个消费者。每个分区只能由同一个消费者组内的一个消费者进行消费，这保证了消息在消费者组内的负载均衡。
   >
   > 发送消息的流程如下：
   >
   > 1. Producer将消息发送到指定的主题和分区。
   >
   > 2. Producer根据指定的分区策略（如轮询、随机等），将消息发送到对应的Broker。
   >
   > 3. Broker接收到消息后，将其持久化到磁盘，并返回一个确认消息的响应给Producer。
   >
   > 4. Consumer订阅感兴趣的主题，并从Broker拉取消息。
   >
   > 5. Consumer根据消费者组的协调机制，确定每个消费者负责消费的分区。
   >
   > 6. Consumer从指定的Broker中拉取分区内的消息。
   >
   > 7. Consumer处理消费得到的消息，并进行相应的业务逻辑处理。
   >
   > 8. 重复步骤6和7，直到消费者消费完所有的消息。
   >
   > Kafka的消息发送流程是基于异步的、批量的方式进行的，这使得Kafka能够达到高吞吐量和低延迟的特性。同时，Kafka通过分区和消费者组的机制，实现了高可靠性和可扩展性，适用于大规模的数据处理和流式处理场景。

9. ### kafka如何保证消费者消费消息幂等性

   > 

10. ### kafka延迟队列是怎么实现的

    > Kafka本身并没有内置的延迟队列功能，但可以通过一些技术手段来实现延迟队列的功能。下面是一种常见的实现方式：
    >
    > 1. **使用消息的时间戳（Timestamp）**：在消息的元数据中添加一个表示消息的延迟时间戳。Producer发送消息时，可以设置消息的时间戳为当前时间加上延迟时间。这样，消息将会在延迟时间过去后才可被消费。
    >
    > 2. **设置消息的过期时间（TTL）**：Producer在发送消息时，可以设置消息的过期时间（Time to Live）。Broker在接收到消息后，会检查消息的过期时间，如果消息已经过期，则不会立即发送给消费者，而是被丢弃或进入死信队列。
    >
    > 3. **使用专门的延迟队列消费者**：创建一个专门的消费者组，用于消费延迟队列中的消息。这个消费者组只消费具有延迟时间的消息，并在延迟时间到达后进行相应的处理。可以使用Kafka的消费者API来编写延迟队列的消费者逻辑。
    >
    > 4. **定时器和轮询**：另一种实现方式是使用定时器和轮询机制。在Producer发送消息时，同时设置一个定时器，当定时器触发时，再将消息发送到Kafka集群。消费者则通过轮询的方式检查是否有延迟时间到达的消息需要处理。
    >
    > 需要注意的是，这些方式都是基于Kafka的现有功能进行的延伸，但并不是Kafka本身提供的原生延迟队列解决方案。它们可能会在一定程度上降低Kafka的吞吐量和性能，并且对于非常精确的延迟要求可能不够准确。在实际应用中，可以根据具体的需求和场景选择合适的延迟队列实现方式或考虑使用专门的延迟队列系统。

11. ### 缓存有哪些中间件

12. ### 数据存储有哪些中间件，说了mysql、Oracle、MongoDB、缓存数据库、ClickHouse，面试官也是一直问“还有吗”。最后说想听到es这个答案

    > 数据存储中间件是用于存储和管理数据的软件层或服务，它们提供了各种功能和接口，以方便数据的读写、查询和管理。以下是一些常见的数据存储中间件：
    >
    > 1. **关系型数据库管理系统（RDBMS）**：包括MySQL、Oracle、Microsoft SQL Server、PostgreSQL等。它们使用结构化数据模型，支持SQL查询语言，并提供事务处理、数据完整性、索引和复杂查询等功能。
    >
    > 2. **NoSQL数据库**：NoSQL（Not Only SQL）数据库是一类非关系型数据库，适用于大规模分布式系统和非结构化数据存储。常见的NoSQL数据库包括MongoDB、Cassandra、Redis、Couchbase等。它们具有高可扩展性、灵活的数据模型和低延迟的特点。
    >
    > 3. **列式数据库**：列式数据库，如Apache HBase、Apache Cassandra、Google Bigtable等，以列为存储和查询的基本单位，适用于大规模分布式系统和需要快速查询特定列的场景。
    >
    > 4. **内存数据库**：内存数据库将数据存储在内存中，以提供极高的读写性能。常见的内存数据库有Redis、Memcached、Apache Ignite等，它们适用于缓存、会话管理、实时数据处理等场景。
    >
    > 5. **文档数据库**：文档数据库（Document Database）以文档为单位存储数据，常用的文档数据库有MongoDB、CouchDB等。它们适合存储和查询半结构化数据（如JSON、XML等）。
    >
    > 6. **图数据库**：图数据库（Graph Database）用于存储和处理图结构数据，如社交网络、知识图谱等。常见的图数据库有Neo4j、JanusGraph等，它们提供了高效的图遍历和图查询功能。
    >
    > 7. **搜索引擎**：搜索引擎，如Elasticsearch、Apache Solr等，专注于全文搜索和实时数据分析。它们支持复杂的搜索和聚合操作，适用于日志分析、推荐系统等应用。
    >
    > 8. **分布式文件系统**：分布式文件系统，如Hadoop HDFS、Ceph、GlusterFS等，用于存储和管理大规模分布式系统中的文件数据。
    >
    > 这些中间件提供了各种不同的数据存储模型、查询语言和特性，可以根据应用的需求选择合适的存储中间件。在实际应用中，也可以组合使用多个存储中间件来满足不同的数据存储和处理需求。
    >
    > ---
    >
    > ES是指Elasticsearch，它是一个开源的分布式搜索和分析引擎，构建在Apache Lucene库的基础上。Elasticsearch被广泛用于全文搜索、日志分析、实时数据分析和大规模数据存储等场景。
    >
    > 以下是一些关键特点和功能：
    >
    > 1. **分布式和可扩展性**：Elasticsearch以分布式的方式存储数据，可以跨多个节点进行水平扩展。它支持自动数据分片和副本机制，提供高可用性和容错性。
    >
    > 2. **全文搜索**：Elasticsearch具有强大的全文搜索功能，可以对大规模的文本数据进行高效的搜索和匹配。它支持复杂的查询语法、近实时的搜索响应和相关性排名。
    >
    > 3. **实时数据分析**：Elasticsearch支持实时索引和分析，可以在数据写入后几乎立即对数据进行搜索和聚合。它提供了丰富的聚合功能，如统计、分组、排序和数据可视化等。
    >
    > 4. **多种数据类型支持**：Elasticsearch可以处理多种数据类型，包括结构化数据、半结构化数据和非结构化数据。它支持JSON格式的文档存储，并提供动态映射和索引模板来适应不同类型的数据。
    >
    > 5. **分布式检索和聚合**：Elasticsearch可以将搜索和聚合操作分布在多个节点上并并行执行，以提高性能和吞吐量。它利用倒排索引和分布式缓存来加速搜索和聚合操作。
    >
    > 6. **多租户支持**：Elasticsearch支持多租户架构，可以在同一个集群上隔离和管理多个独立的索引和数据集。每个租户可以拥有自己的访问控制和配置设置。
    >
    > 7. **插件生态系统**：Elasticsearch拥有丰富的插件生态系统，可以扩展其功能。它支持各种插件，如数据备份和恢复、安全认证、监控和集成等。
    >
    > 由于其强大的搜索和分析功能、可扩展性和开放的生态系统，Elasticsearch被广泛应用于许多领域，包括企业搜索、日志分析、电子商务、智能推荐和安全分析等。

13. ### 介绍下Kafka整体架构

    > Kafka是一个分布式流处理平台，具有高吞吐量、可扩展性和持久性的特点。下面是Kafka的整体架构和关键组件的介绍：
    >
    > 1. **Producer（生产者）**：Producer负责向Kafka集群发送消息。它将消息发布到一个或多个主题（topic）。生产者可以选择将消息发送到特定的分区，也可以让Kafka自动选择分区。
    >
    > 2. **Broker（服务节点）**：Broker是Kafka集群中的一个节点，负责存储和处理消息。每个Broker都是一个独立的Kafka服务器实例，可以承载多个分区。每个分区在集群中都有多个副本，其中一个副本被选为领导者（Leader），其他副本作为追随者（Follower）。
    >
    > 3. **Topic（主题）**：Topic是消息的逻辑容器，用于对消息进行分类和组织。生产者将消息发布到特定的主题，消费者可以订阅一个或多个主题来接收消息。每个主题可以划分为多个分区。
    >
    > 4. **Partition（分区）**：每个主题可以划分为多个分区。分区是消息存储和传输的基本单位，每个分区在多个Broker之间进行分布式存储和复制。分区内的消息按照顺序写入，并保持有序。
    >
    > 5. **Consumer Group（消费者组）**：消费者组是一组消费者的集合，它们共同消费一个或多个主题的消息。每个分区只能被一个消费者组中的一个消费者消费。消费者组可以水平扩展，以提高消费能力和容错性。
    >
    > 6. **Consumer（消费者）**：消费者从Broker订阅一个或多个主题，并消费分区中的消息。消费者可以以不同的速率消费消息，并可以自定义消费的偏移量（Offset）。消费者可以使用轮询方式从分区中获取消息。
    >
    > 7. **Zookeeper**：Zookeeper是Kafka集群的协调服务，用于管理和维护集群的元数据和状态。它保存了分区分配的信息、消费者组的偏移量等。Kafka通过与Zookeeper进行交互来进行集群的管理和协调。
    >
    > 整体而言，Kafka的架构基于分布式、分区和副本的设计，具有高可用性和可扩展性。它通过分布式存储和复制来确保消息的持久性和可靠性，并通过分区和消费者组的概念来实现消息的并行处理和消费能力的横向扩展。

14. ### Kafka如何保证可靠性

    > Kafka通过以下方式来确保消息的可靠性和持久性：
    >
    > 1. **分区和副本机制**：Kafka将每个主题划分为多个分区，并将每个分区的数据复制到多个Broker上形成副本集。每个分区有一个领导者（Leader）和多个追随者（Follower）。生产者将消息写入分区的领导者，而追随者负责复制领导者的数据。如果领导者发生故障，其中一个追随者会自动成为新的领导者，保证数据的可用性和持久性。
    >
    > 2. **持久化**：Kafka使用持久化日志来存储消息。消息在写入分区时，首先被追加到日志文件（称为分段），然后异步刷新到磁盘上的文件系统。这种方式保证了消息的持久性，即使在写入到磁盘之前发生故障，数据也不会丢失。
    >
    > 3. **复制和同步**：Kafka的副本机制确保了数据的冗余和可靠性。每个分区的数据被复制到多个Broker上的副本集中。当生产者将消息写入领导者时，消息会异步地被复制到追随者。只有当所有的追随者都确认接收到消息后，生产者才会认为消息写入成功。这种同步复制机制可以保证数据在多个副本之间的一致性和可靠性。
    >
    > 4. **消息确认**：生产者可以选择使用不同级别的消息确认机制来确保消息的可靠性。生产者可以选择在消息写入到领导者后立即接收确认，或者等待消息在所有副本中都复制完成后再接收确认。这样可以根据应用的需求在消息传递的可靠性和延迟之间进行权衡。
    >
    > 5. **偏移量（Offset）**：Kafka使用偏移量来标识每个消息在分区中的位置。消费者可以自定义消费的偏移量，以便从指定位置开始消费消息。Kafka会将消费者的偏移量保存在Zookeeper或内置的__consumer_offsets主题中，确保消费者可以在发生故障或重新启动后继续消费未处理的消息。
    >
    > 通过分区和副本机制、持久化日志、复制和同步、消息确认和偏移量管理，Kafka提供了可靠性保证，确保消息在分布式环境中的可靠传递和持久存储。

## 手撕

求 n! （n的阶乘）暴力、递归

螺旋数组1

一个大小为n的无序数组，找出第k小的元素。

有效括号

中序遍历

最小栈

第n个的丑数

## 场景题

1. ### 你在家里用浏览器访问www.baidu.com的过程是怎么样的

   > 当您在家里使用浏览器访问www.baidu.com时，大致经历以下过程：
   >
   > 1. **解析域名**：浏览器首先将URL中的域名"www.baidu.com"解析成对应的IP地址。它会首先检查浏览器缓存中是否有该域名的解析结果，如果没有或已过期，浏览器会发起DNS查询请求。
   >
   > 2. **DNS查询**：浏览器向本地网络中的DNS解析器（通常由您的网络服务提供商提供）发送域名解析请求。如果解析器具有缓存记录，则直接返回对应的IP地址。否则，它将向其他DNS服务器发送请求，逐级查询，直到找到对应的IP地址。
   >
   > 3. **建立TCP连接**：一旦浏览器获得目标服务器的IP地址，它将使用HTTP协议的默认端口（通常是80）与服务器建立TCP连接。这个过程涉及三次握手，用于确保客户端和服务器之间的可靠通信。
   >
   > 4. **发送HTTP请求**：一旦TCP连接建立成功，浏览器会构建HTTP请求报文，其中包含请求的方法（通常是GET）、路径（/）和其他相关信息。然后，浏览器将该请求发送给目标服务器。
   >
   > 5. **服务器处理请求**：目标服务器接收到浏览器发送的HTTP请求后，将根据请求的内容进行处理。对于访问百度首页，服务器会检索相关的HTML、CSS、JavaScript文件等，并生成相应的HTTP响应。
   >
   > 6. **接收和渲染响应**：一旦浏览器接收到服务器发送的HTTP响应，它会解析响应报文，并根据其中的内容进行渲染。这包括解析HTML、加载和执行JavaScript、渲染页面元素等。
   >
   > 7. **显示网页**：最终，浏览器将解析和渲染后的网页内容显示给用户。您将看到百度的主页在浏览器中展示出来。
   >
   > 总的来说，访问www.baidu.com的过程包括解析域名、建立TCP连接、发送HTTP请求、服务器处理请求、接收和渲染响应等步骤，最终将页面内容显示

2. ### 如何设计一个排队系统（商场饭店的那种排队取号系统）

   > 设计一个排队系统（如商场或饭店的排队取号系统）可以遵循以下步骤：
   >
   > 1. **需求分析**：了解业务需求和系统要解决的问题。确定排队系统的目标、功能和约束条件。例如，确定是否需要支持多个服务窗口、预约功能、叫号方式等。
   >
   > 2. **系统组成**：确定系统的主要组成部分。通常包括以下几个核心组件：
   >    - **取号机**：提供取号功能，为每个客户分配一个唯一的号码，并将客户的相关信息存储到系统中。
   >    - **显示屏**：在适当的位置显示当前叫号信息，使客户能够知道自己的位置和等待时间。
   >    - **叫号器**：根据业务逻辑从等待队列中选择下一个客户，并发出声音或显示叫号信息，通知客户到达服务窗口。
   >    - **服务窗口**：提供服务的实际执行地点。每个窗口可以处理一个或多个客户，根据业务需求选择合适的窗口数量。
   >    - **后台管理系统**：用于管理排队系统的配置、监控和报告等功能。
   >    - **数据库**：存储客户信息、等待队列和服务窗口状态等数据。
   >
   > 3. **排队逻辑**：定义排队逻辑和业务规则。例如，确定优先级规则（如老年人、残障人士优先）、服务时间限制、预约逻辑等。
   >
   > 4. **系统交互**：确定系统各组件之间的交互方式。例如，取号机与显示屏的信息同步，叫号器与显示屏的信息传递，服务窗口与叫号器的状态同步等。
   >
   > 5. **用户界面**：设计用户界面，使用户可以轻松地取号、查看当前状态和等待时间等信息。可以考虑使用自助终端、手机应用程序或网页等形式。
   >
   > 6. **数据存储和管理**：选择合适的数据库系统来存储客户信息、等待队列和服务窗口状态等数据。确保数据的安全性和一致性。
   >
   > 7. **系统扩展和容错**：考虑系统的可扩展性和容错性。例如，可以通过水平扩展增加服务窗口数量、使用负载均衡来均衡请求负载，以及备份和冗余机制来提高系统的可靠性。
   >
   > 8. **性能优化**：对系统进行性能优化，以确保快速响应和高吞吐量。例如，使用缓存技术、异步处理和并发控制等手段来提高系统效率。
   >
   > 9. **安全性考虑**：确保系统的安全性和数据保护。采取适当的安全措施，如身份验证、访问控制和数据加密等，以保护用户和系统的安全。
   >
   > 10. **测试和部署**：进行系统测试，包括功能测试、性能测试和安全测试等。在正式部署之前，进行适当的准生产环境测试和用户培训。
   >
   > 以上是设计排队系统的一般步骤和考虑因素。具体的设计会根据实际需求和场景的不同而有所变化。

3. ### 如果数据要刷盘的话要怎么设计表结构，就假设我们用MySQL好了

   > 当数据需要刷盘到磁盘（硬盘）时，以下是一些在MySQL中设计表结构时应考虑的要点：
   >
   > 1. **选择合适的表引擎**：MySQL支持多种表引擎，如InnoDB、MyISAM等。对于需要强调数据可靠性和事务支持的场景，推荐使用InnoDB引擎，因为它提供了ACID事务支持和崩溃恢复功能。
   >
   > 2. **定义适当的数据类型**：选择适当的数据类型来存储数据，以节省空间并提高性能。避免使用过大的数据类型，可以根据实际需求选择合适的整数、字符串和日期/时间类型。
   >
   > 3. **添加合适的索引**：根据查询需求和数据访问模式，为表添加合适的索引。索引可以提高查询性能，但也会增加写操作的开销，因此需要权衡和选择适当的索引策略。
   >
   > 4. **合理划分表和分区**：如果数据量非常大，可以考虑在表级别或分区级别上进行水平分割，以提高查询性能和管理数据。根据业务需求和查询模式，选择合适的分割策略，如按日期、地理位置或其他业务维度进行分割。
   >
   > 5. **优化表结构和范式化**：设计表结构时，遵循数据库范式化原则，减少数据冗余，并优化查询性能。根据具体业务需求，进行适度的冗余设计，以提高查询性能和简化复杂的查询操作。
   >
   > 6. **使用合理的数据存储格式**：根据数据类型和查询需求，选择合适的数据存储格式。例如，对于大文本字段，可以考虑使用TEXT或BLOB类型。对于数值计算密集型的场景，可以选择合适的数值类型，如DECIMAL或FLOAT。
   >
   > 7. **合理设置表的存储参数**：根据具体的硬件和性能需求，设置适当的存储参数，如表空间、页大小、缓冲池大小等。这些参数可以影响数据的刷盘和读取性能。
   >
   > 8. **定期备份和恢复策略**：设计合理的数据备份和恢复策略，以确保数据的安全性和可靠性。定期进行全量备份，并根据业务需求和数据变化情况，选择增量备份或日志备份等。同时，测试和验证备份的可恢复性。
   >
   > 9. **监控和优化性能**：使用MySQL提供的性能监控工具和日志，对表的刷盘性能进行监控和优化。例如，通过分析慢查询日志，识别潜在的性能瓶颈，并进行适当的调整和优化。
   >
   > 10. **容灾和高可用性考虑**：设计容灾和高可用性策略，以确保数据的可靠性和服务的连续性。可以考虑使用MySQL的主从复制、主备切换、集群和分布式架构等技术，实现数据的备份和故障转移。
   >
   > 以上是在设计MySQL表结构时考虑数据刷盘的一些要点。具体的设计和优化策略会根据实际需求和系统的规模、负载情况等因素而有所不同。

4. ### 有千万级数据，如何判断一个整数是否存在

   > 对于一个千万级数据集，如果要快速判断一个整数是否存在，可以考虑以下几种方法：
   >
   > 1. **使用哈希表**：将整数作为键存储在哈希表中。哈希表可以提供快速的查找时间复杂度，通常为O(1)。您可以使用现有的哈希表数据结构，如Python中的字典（dictionary）或Java中的HashMap。
   >
   > 2. **使用布隆过滤器**：布隆过滤器是一种空间效率很高的概率型数据结构，用于判断一个元素是否存在于集合中。它可以在极低的误判率下快速判断元素是否存在，但无法提供确定性的结果。您可以将整数添加到布隆过滤器中，并使用它来快速过滤掉不存在的整数。
   >
   > 3. **使用数据库索引**：如果数据存储在关系型数据库中，可以在整数字段上创建索引。数据库索引可以加快查找速度，使查询时间复杂度为O(log n)或更低。您可以使用适当的查询语句，如SELECT语句，根据整数值查询数据，并根据索引进行快速查找。
   >
   > 4. **使用二分查找**：如果数据已经有序排列，您可以使用二分查找算法。通过在数据集中进行二分查找，可以在O(log n)的时间复杂度内确定整数是否存在。这种方法要求数据事先按照顺序排列，并且适用于静态数据集。
   >
   > 请注意，以上方法的适用性取决于具体的场景和数据特征。在选择合适的方法之前，您需要考虑数据集的大小、更新频率、内存限制以及对准确性和查询效率的要求。

5. ### 如何理解：布隆过滤器说某个元素存在，则大概率在。布隆过滤器说某个元素不在，则一定不在

   > 布隆过滤器是一种概率型数据结构，它用于判断一个元素是否存在于集合中。它通过使用一定数量的哈希函数和位数组来实现。
   >
   > 当布隆过滤器判断某个元素存在时，它可能会给出"存在"的结果，但并不能百分之百确定该元素确实存在于集合中。这是因为布隆过滤器使用多个哈希函数将元素映射到位数组的多个位置上，并将这些位置的对应位设置为1。当检查一个元素时，如果所有对应位置的位都为1，则布隆过滤器会返回"存在"的结果。
   >
   > 然而，由于哈希函数的散列冲突和位数组的有限大小，可能会出现不同元素映射到相同的位上（称为哈希碰撞）。这可能导致误判，即布隆过滤器判断某个元素存在，但实际上该元素并不存在于集合中。
   >
   > 因此，当布隆过滤器判断某个元素不存在时，它可以给出"不存在"的确定结果。这是因为如果一个元素的任何一个哈希函数的映射位置上的位为0，那么该元素一定不存在于集合中。这是因为如果元素存在于集合中，那么所有哈希函数的映射位置上的位都应该为1。
   >
   > 综上所述，布隆过滤器的判断结果有两种情况：
   > - 当布隆过滤器判断某个元素存在时，它可能存在于集合中，但也可能是误判。
   > - 当布隆过滤器判断某个元素不存在时，它一定不存在于集合中。
   >
   > 因此，在使用布隆过滤器时，需要注意潜在的误判风险，并根据具体的应用场景和需求来权衡使用布隆过滤器带来的概率性和确定性。

6. ### 千万级数据用布隆过滤器初始化的时候 redis 太慢了，有没有什么好方法

   > 如果在Redis中使用布隆过滤器初始化千万级数据时遇到性能问题，可以考虑以下几种方法来改善性能：
   >
   > 1. **增加Redis的内存限制**：布隆过滤器需要占用一定的内存空间来存储位数组和哈希函数，因此，确保Redis具有足够的内存来容纳大规模的布隆过滤器。您可以通过调整Redis的maxmemory配置参数来增加内存限制。
   >
   > 2. **使用批量操作**：将要添加到布隆过滤器的数据分批处理，而不是逐个添加。使用Redis的管道（pipeline）功能或批量操作命令（如MSETBIT）可以减少网络延迟和通信开销，从而提高性能。
   >
   > 3. **考虑使用分布式布隆过滤器**：如果单个Redis实例无法满足性能需求，可以考虑使用分布式布隆过滤器。将数据分散到多个Redis节点上，每个节点负责一部分数据。这样可以将负载分摊到多个节点上，提高并发处理能力。
   >
   > 4. **选择更高性能的布隆过滤器实现**：考虑使用其他高性能的布隆过滤器实现，如RedisBloom或Cuckoo Filter。这些实现可以提供更高的性能和更低的内存占用，以满足千万级数据的需求。
   >
   > 5. **预分配位数组空间**：在初始化布隆过滤器之前，可以预先为位数组分配足够的空间。通过提前计算所需的位数组大小，并在Redis中创建相应大小的位数组，可以避免在运行时动态调整位数组大小的开销。
   >
   > 6. **使用多线程或并行处理**：如果您的应用程序支持多线程或并行处理，可以将数据初始化任务分配给多个线程或进程并行执行。这样可以充分利用多核处理器的性能，加速布隆过滤器的初始化过程。
   >
   > 7. **评估是否需要布隆过滤器**：如果以上方法仍无法满足性能需求，您可以重新评估是否真正需要布隆过滤器，并考虑其他数据结构或算法来解决问题。根据具体的应用场景和需求，可能存在替代方案。
   >
   > 以上方法可以根据具体情况选择适用的方案，并进行性能测试和调优，以找到最佳的性能与资源平衡。

7. ### 如何设计登陆黑名单

   > 设计登录黑名单时，以下是一些常见的设计要点和建议：
   >
   > 1. **定义触发条件**：确定何时将用户添加到黑名单。例如，可以设置登录失败次数的阈值，当用户连续多次登录失败时触发添加到黑名单的操作。
   >
   > 2. **选择存储方式**：选择适合的存储方式来保存黑名单列表。可以使用数据库、缓存（如Redis）或内存数据结构（如哈希表）来存储黑名单用户的信息。
   >
   > 3. **设置过期时间**：针对黑名单中的每个用户，可以设置一个过期时间。一旦用户被添加到黑名单，系统会记录添加时间，并在过期时间后自动将用户从黑名单中移除，以允许用户再次尝试登录。
   >
   > 4. **记录登录失败次数**：在用户登录失败时，记录其失败次数。可以将失败次数存储在用户的个人资料中，或使用额外的计数器来保存失败次数。
   >
   > 5. **添加到黑名单**：当用户达到触发条件（如登录失败次数超过阈值）时，将其添加到黑名单中。可以将用户标记为黑名单用户，或将相关信息存储在黑名单列表中。
   >
   > 6. **验证登录状态**：在用户尝试登录时，检查其登录状态。如果用户在黑名单中，阻止其登录，并给予相应的提示或错误消息。
   >
   > 7. **监控和审计**：建立监控和审计机制，以便跟踪黑名单的使用情况。记录添加到黑名单和移除黑名单的操作，并监控黑名单用户的行为。
   >
   > 8. **黑名单管理**：提供管理接口或界面，允许管理员手动添加或移除用户到黑名单。这样可以对特定用户进行临时或永久的封禁操作。
   >
   > 9. **自动解封机制**：在黑名单中的用户达到一定条件（如等待一段时间或通过其他验证手段）后，可以自动解封用户并将其从黑名单中移除。
   >
   > 10. **安全性考虑**：确保黑名单设计具备一定的安全性，以防止恶意攻击或绕过黑名单。例如，可以采用安全哈希函数对黑名单列表进行加密存储，限制访问黑名单的权限等。
   >
   > 以上是设计登录黑名单的一些基本原则和建议，具体实现方式可以根据系统需求和安全要求进行调整和优化。

8. ### 比如说我们做在这个支付，就是你去转账的时候，就比如说然后你是个订单，然后你去支付的时候，这时候可能是说支付宝那边可能遇到就是会遇到网络超时，对你来说有可能是说然后要成功，对吧？有可能失败，但是你在当场前情况下你是不知道这个结果的。那这个时候，然后比如说是你作为一个订单的设计，比如说我怎么去防止这种情况？从前端上开始请求，或者是就是用户请求，然后到订单服务，然后再到一个支付服务，对不对？这三个服务，那如果比如说订单去支付的时候，然后这时候出现了一个超时，对吧？你订单的时候肯定会有个状态的流转，对不对？那这个流转你到底推还是不推？或者说你有什么办法去保证？然后订单的状态和你的支付状态是一致的？对，你不知道，因为它是超时的，就是你不知道支付他们那边的结果。

   > 在支付场景中，确保订单状态与支付状态一致是非常重要的。在面对网络超时等情况时，可以采取以下设计和策略来处理：
   >
   > 1. **请求超时设置**：在前端或后端发起支付请求时，设置适当的请求超时时间。如果在规定的时间内未收到支付结果，可以认为支付请求失败。
   >
   > 2. **幂等性设计**：为了防止重复支付或多次请求导致的问题，需要在设计支付接口时考虑幂等性。通过在请求中使用唯一标识符（如订单号）并在支付服务端进行幂等性检查，确保同一支付请求只能处理一次。
   >
   > 3. **支付结果回调**：支付服务可以提供支付结果的异步回调机制，即支付完成后，支付平台会将支付结果通知到订单服务。订单服务接收到支付结果后，更新订单状态以与支付状态保持一致。
   >
   > 4. **订单状态流转管理**：订单状态管理是确保一致性的关键。订单服务应该具备状态流转管理的能力，即在不同阶段更新订单状态，并记录每个状态变更的时间戳。这样即使支付超时，订单状态也能够反映当前的进展。
   >
   > 5. **定时查询支付状态**：如果支付结果回调机制不可靠或延迟，可以在订单服务端启用定时查询支付状态的机制。通过定时向支付服务发送查询请求，获取最新的支付结果，并根据结果更新订单状态。
   >
   > 6. **异常处理和补偿机制**：针对支付超时等异常情况，需要有相应的异常处理和补偿机制。例如，可以记录支付超时的情况并触发后续操作，如自动关闭订单或向用户发送通知。
   >
   > 7. **日志和监控**：对支付过程进行全面的日志记录和监控，包括请求、回调、状态变更等关键事件。这样可以帮助追踪问题、排查故障，并及时发现异常情况。
   >
   > 综上所述，通过设置适当的超时时间、处理回调机制、管理订单状态流转、定时查询支付状态等策略，可以尽量保证订单状态和支付状态的一致性，以提供更可靠的支付服务。

9. ### 介绍项目功能，遇到的问题，和你做了哪些优化
